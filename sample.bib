@article{Panwalkar:1977fw,
abstract = {In the past two decades researchers in the field of sequencing and scheduling have analyzed several priority dispatching rules through simulation techniques. This paper presents a summary of over 100 such rules, a list of many references that analyze them, and a classification scheme.},
author = {Panwalkar, S. S. and Iskander, Wafik},
doi = {10.1287/opre.25.1.45},
isbn = {0030-364X},
issn = {0030-364X},
journal = {Operations Research},
number = {1},
pages = {45--61},
title = {{A Survey of Scheduling Rules}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/opre.25.1.45},
volume = {25},
year = {1977}
}

@book{Engelbrecht:dm,
author = {Xing, Bo and Gao, Wen-Jing},
doi = {10.1007/978-3-319-03404-1_1},
mendeley-groups = {proposal},
pages = {3--17},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Introduction to Computational Intelligence}},
url = {http://link.springer.com/10.1007/978-3-319-03404-1{\_}1},
year = {2014}
}

@article{Ferdaus:2014ep,
abstract = {In this paper, we propose the AVVMC VM consolidation scheme that focuses on balanced resource utilization of servers across different computing resources (CPU, memory, and network I/O) with the goal of minimizing power consumption and resource wastage. Since the VM consolidation problem is strictly NP-hard and computationally infeasible for large data centers, we propose adaptation and integration of the Ant Colony Optimization (ACO) metaheuristic with balanced usage of computing resources based on vector algebra. Our simulation results show that AVVMC outperforms existing methods and achieves improve-ment in both energy consumption and resource wastage reduction.},
author = {Ferdaus, Hasanul and Murshed, Manzur Rodrigo N. Calheiros Rajkumar Buyya},
doi = {10.1007/978-3-319-09873-9_26},
isbn = {9781450327855},
issn = {0302-9743},
journal = {Euro-Par 2014 Parallel Processing},
number = {Chapter 26},
pages = {306--317},
title = {{Virtual Machine Consolidation in Cloud Data}},
volume = {8632},
year = {2014}
}
@article{Viswanathan:2012ej,
abstract = {Cloud computing has emerged as an exciting hosting paradigm to drive up server utilization and reduce data center operational costs. Even though clouds present a single unified homogeneous resource pool view to end users, the underlying server landscape may differ in terms of functionality and reconfiguration capabilities (e.g., support for shared processors, live migration). In a private cloud setting where information on the resources as well as workloads are available, the placement of applications on clouds can leverage it to achieve better consolidation with performance guarantees. In this work, we present the design and implementation of CloudMap, a provisioning system for private clouds. Given an application's resource usage patterns, we match it with a server cluster with the appropriate level of reconfiguration capability. In this cluster, we place the application on a server that has existing workloads with complementary resource usage profile. CloudMap is implemented using a hybrid architecture with a global server cluster selection module and local cluster-specific server selection modules. Using production traces from live data centers, we demonstrate the effectiveness of CloudMap over existing placement methodologies.},
author = {Viswanathan, Balaji and Verma, Akshat and Dutta, Sourav},
doi = {10.1109/NOMS.2012.6211877},
isbn = {9781467302685},
issn = {1542-1201},
journal = {Proceedings of the 2012 IEEE Network Operations and Management Symposium, NOMS 2012},
pages = {9--16},
title = {{CloudMap: Workload-aware placement in private heterogeneous clouds}},
year = {2012}
}
@article{2010arXiv1006.0308B,
abstract = {The rapid growth in demand for computational power driven by modern service applications combined with the shift to the Cloud computing model have led to the establishment of large-scale virtualized data centers. Such data centers consume enormous amounts of electrical energy resulting in high operating costs and carbon dioxide emissions. Dynamic consolidation of virtual machines (VMs) using live migration and switching idle nodes to the sleep mode allows Cloud providers to optimize resource usage and reduce energy consumption. However, the obligation of providing high quality of service to customers leads to the necessity in dealing with the energy-performance trade-off, as aggressive consolidation may lead to performance degradation. Because of the variability of workloads experienced by modern applications, the VM placement should be optimized continuously in an online manner. To understand the implications of the online nature of the problem, we conduct a competitive analysis and prove competitive ratios of optimal online deterministic algorithms for the single VM migration and dynamic VM consolidation problems. Furthermore, we propose novel adaptive heuristics for dynamic consolidation of VMs based on an analysis of historical data from the resource usage by VMs. The proposed algorithms significantly reduce energy consumption, while ensuring a high level of adherence to the service level agreement. We validate the high efficiency of the proposed algorithms by extensive simulations using real-world workload traces from more than a thousand PlanetLab VMs. Copyright 2011 John Wiley {\&} Sons, Ltd.},
archivePrefix = {arXiv},
arxivId = {1006.0308},
author = {Beloglazov, Anton and Buyya, Rajkumar},
doi = {10.1002/cpe.1867},
eprint = {1006.0308},
isbn = {1532-0634},
issn = {15320626},
journal = {Concurrency Computation Practice and Experience},
keywords = {Cloud computing,Green IT,dynamic consolidation,resource management,virtualization},
number = {13},
pages = {1397--1420},
pmid = {23335858},
title = {{Optimal online deterministic algorithms and adaptive heuristics for energy and performance efficient dynamic consolidation of virtual machines in Cloud data centers}},
volume = {24},
year = {2012}
}
@article{Pinheiro:2015eu,
author = {Pinheiro, Rodrigo L and Landa-Silva, Dario and Atkin, Jason},
doi = {10.1145/2739480.2754721},
isbn = {9781450334723},
journal = {2015 Genetic and Evolutionary Computation Conference (GECCO 2015)},
pages = {735--742},
title = {{Analysis of Objectives Relationships in Multiobjective Problems Using Trade-Off Region Maps}},
year = {2015}
}
@article{Xavier:2017jl,
abstract = {The more large-scale data centers infrastructure costs increase, the
more simulation-based evaluations are needed to understand better the
trade-off between energy and performance and support the development of
new energy-aware resource allocation policies. Specifically, in the
cloud computing field, various simulators are able to predict and
measure the behavior of applications on different architectures using
different resource allocation policies. Yet, only a few of them have the
ability to simulate energy-saving strategies, and none of them support
the complete advanced configuration and power interface (ACPI)
specification. ACPI defines a terminology for all possible power states
of a machine and their associated power rate. The hardware industry has
relied on ACPI to provide up-to-date standard interfaces for hardware
discovery, configuration, power management, and monitoring, enabling a
better understanding of the energy consumption level of different
hardware states, referred to as ACPI G-states, S-states, and P-states.
In this paper, we improve the modeling and simulation of the ACPI
G/S-states and show not only that these states offer different
energy-saving levels but also that state transitions consume energy. In
addition, we model the latency to transit between two states and the
effects on the turnaround time when the transitions are not performed
conservatively. Furthermore, the equations provide essential information
to quantify the trade-off between energy consumption and performance and
assist in the analysis/decision on which strategy fits better in the
environment and how it could be refined. Our expanded energy model was
implemented in CloudSim and validated with simulation-based experiments
with a very high level of accuracy, with a standard deviation of at most
6{\%}. Copyright (C) 2016 John Wiley {\&} Sons, Ltd.},
author = {Xavier, Miguel G. and Rossi, F{\'{a}}bio D. and {De Rose}, C{\'{e}}sar A.F. and Calheiros, Rodrigo N. and Gomes, Danielo G.},
doi = {10.1002/cpe.3839},
issn = {15320634},
journal = {Concurrency Computation },
keywords = {Green IT,cloud computing,modeling and simulation,resource management},
month = {Feb},
number = {4},
pages = {e3839},
title = {{Modeling and simulation of global and sleep states in ACPI-compliant energy-efficient cloud environments}},
volume = {29},
year = {2017}
}
@book{Hyvarinen:2004vj,
author = {Oja, Aapo Hyv{\"{a}}rinen and Juha Karhunen and Erkki},
booktitle = {Analysis},
month = {apr},
pages = {1--17},
publisher = {John Wiley {\&} Sons},
title = {{Independent Component Analysis}},
year = {2001}
}

@article{:2002vv,
author = {Lodi, Andrea and Martello, Silvano and Monaci, Michele},
journal = {European Journal of Operational Research},
mendeley-groups = {proposal},
title = {{Two-dimensional packing problems - A survey.}},
year = {2002}
}
@article{Moritz:2015hv,
author = {Moritz, Ruby L.V. and Reich, Enrico and Schwarz, Maik and Bernt, Matthias and Middendorf, Martin},
doi = {10.1016/j.ejor.2014.10.044},
issn = {03772217},
journal = {European Journal of Operational Research},
keywords = {Ant colony optimization,Flow shop scheduling problem,Genetic algorithms,Multi objective optimization,Ranking relations},
number = {2},
pages = {454--464},
title = {{Refined ranking relations for selection of solutions in multi objective metaheuristics}},
volume = {243},
year = {2015}
}

@incollection{Deng:1998fk,
address = {Boston, MA},
author = {Deng, X},
booktitle = {Multilevel Optimization: Algorithms and Applications},
pages = {149--164},
publisher = {Springer, Boston, MA},
title = {{Complexity issues in bilevel linear programming}},
url = {http://link.springer.com/chapter/10.1007/978-1-4613-0307-7{\_}6},
year = {1998}
}
@article{Bentley:1980dz,
author = {Bentley, Jon Louis},
journal = {Commun. ACM},
number = {4},
pages = {214--229},
title = {{Multidimensional Divide-and-Conquer.}},
volume = {23},
year = {1980}
}
@article{Energy_5,
abstract = {There is growing incentive to reduce the power consumed by large-scale data centers that host online services such as banking, retail commerce, and gaming. Virtualization is a promising approach to consolidating multiple online services onto a smaller number of computing resources. A virtualized server environment allows computing resources to be shared among multiple performance-isolated platforms called virtual machines. By dynamically provisioning virtual machines, consolidating the workload, and turning servers on and off as needed, data center operators can maintain the desired quality-of-service (QoS) while achieving higher server utilization and energy efficiency. We implement and validate a dynamic resource provisioning framework for virtualized server environments wherein the provisioning problem is posed as one of sequential optimization under uncertainty and solved using a lookahead control scheme. The proposed approach accounts for the switching costs incurred while provisioning virtual machines and explicitly encodes the corresponding risk in the optimization problem. Experiments using the Trade6 enterprise application show that a server cluster managed by the controller conserves, on average, 26{\%} of the power required by a system without dynamic control while still maintaining QoS goals.},
author = {Kusic, Dara and Kephart, Jeffrey O. and Hanson, James E. and Kandasamy, Nagarajan and Jiang, Guofei},
doi = {10.1007/s10586-008-0070-y},
isbn = {9780769531755},
issn = {13867857},
journal = {Cluster Computing},
keywords = {Power management,Predictive control,Resource provisioning,Virtualization},
number = {1},
pages = {1--15},
title = {{Power and performance management of virtualized computing environments via lookahead control}},
volume = {12},
year = {2009}
}
@article{Hindman:2011ux,
abstract = {We present Mesos, a platform for sharing commod- ity clusters between multiple diverse cluster computing frameworks, such as Hadoop and MPI. Sharing improves cluster utilization and avoids per-framework data repli- cation. Mesos shares resources in a fine-grained man- ner, allowing frameworks to achieve data locality by taking turns reading data stored on each machine. To support the sophisticated schedulers of today's frame- works, Mesos introduces a distributed two-level schedul- ing mechanism called resource offers. Mesos decides how many resources to offer each framework, while frameworks decide which resources to accept and which computations to run on them. Our results show that Mesos can achieve near-optimal data locality when shar- ing the cluster among diverse frameworks, can scale to 50,000 (emulated) nodes, and is resilient to failures.},
author = {Hindman, Benjamin and Konwinski, Andy and Platform, A and Resource, Fine- Grained and Zaharia, Matei},
doi = {10.1109/TIM.2009.2038002},
isbn = {0018-9456},
issn = {00189456},
journal = {Proceedings of the {\ldots}},
pages = {32},
title = {{Mesos: A platform for fine-grained resource sharing in the data center}},
url = {http://static.usenix.org/events/nsdi11/tech/full{\_}papers/Hindman{\_}new.pdf},
year = {2011}
}
@book{Anonymous:2016vv,
address = {Cham},
author = {{De Paoli Stefan Schulte Einar Broch Johnsen}, Flavio},
doi = {10.1007/978-3-319-67262-5},
isbn = {978-3-319-67261-8},
month = {Aug},
publisher = {Springer International Publishing},
title = {{Service-Oriented and Cloud Computing}},
url = {https://link-springer-com.sire.ub.edu/content/pdf/10.1007{\%}2F978-3-319-67262-5.pdf},
year = {2016}
}
@article{Hieu:2015ub,
abstract = {—Virtual machine consolidation aims at reducing the number of active physical servers in a data center, with the goal to reduce the total power consumption. In this context, most of the existing solutions rely on aggressive virtual machine migration, thus resulting in unnecessary overhead and energy wastage. This article presents a virtual machine consolidation algorithm with usage prediction (VMCUP) for improving the energy efficiency of cloud data centers. Our algorithm is executed during the virtual machine consolidation process to estimate the short-term future CPU utilization based on the local history of the considered servers. The joint use of current and predicted CPU utilization metrics allows a reliable characterization of overloaded and underloaded servers, thereby reducing both the load and the power consumption after consolidation. We evaluate our proposed solution through simulations on real workloads from the PlanetLab and the Google Cluster Data datasets. In comparison with the state of the art, the obtained results show that consolidation with usage prediction reduces the total migrations and the power consumption of the servers while complying with the service level agreement.},
author = {Hieu, Nguyen Trung and {Di Francesco}, Mario and Yla-Jaaski, Antti},
doi = {10.1109/CLOUD.2015.104},
isbn = {9781467372879},
issn = {1939-1374},
journal = {Proceedings - 2015 IEEE 8th International Conference on Cloud Computing, CLOUD 2015},
keywords = {Cloud computing,Data centers,Resource prediction,Virtual machine consolidation,Virtual machine migration},
pages = {750--757},
title = {{Virtual Machine Consolidation with Usage Prediction for Energy-Efficient Cloud Data Centers}},
year = {2015}
}
@article{Sprott:2004wt,
author = {Sprott, David and Wilkes, Lawrence},
journal = {The Architecture Journal},
number = {1},
pages = {10--17},
title = {{Understanding Service-Oriented Architecture}},
volume = {1},
year = {2004}
}
@inproceedings{Oduguwa:2002kr,
abstract = {In most real-life problems such as rolling system, design decision-making can be hierarchical and the search space is unknown. Bi-level optimisation problem (BLP) is an operation research technique for solving real life hierarchical decision-making problems. There are a number of different algorithms developed based on classical optimisation methods to solve different classes of the BLP problems where the search space is known. There also exist a number of problems in the BLP which current algorithms are not sufficiently robust to solve. In this paper, a bi-level genetic algorithm (BiGA) is proposed to solve different classes of the BLP problems within a single framework. BiGA is an elitist optimisation algorithm developed to encourage limited asymmetric cooperation between the two players. The performance of the algorithm is illustrated using test functions. The results suggest that BiGA algorithm is robust to solve different classes of the BLP problems and demonstrates potential for real life problems.},
author = {Oduguwa, V. and Roy, R.},
booktitle = {Proceedings - 2002 IEEE International Conference on Artificial Intelligence Systems},
pages = {322--327},
publisher = {IEEE Comput. Soc},
title = {{Bi-level optimisation using genetic algorithm}},
year = {2002}
}
@article{nsgaii,
abstract = {MuItiobjective evolutionary algorithms (EAs) that use nondominated sorting and sharing have been criticized mainly for their: 1) O(MN3) computational complexity (where M is the number of objectives and N is the population size); 2) nonelitism approach; and 3) the need for specifying a sharing parameter. In this paper, we suggest a nondominated sorting-based multiobjective EA (MOEA), called nondominated sorting genetic algorithm Il (NSGA-II). which alleviates all the above three difficulties. Specifically, a fast nondominated sorting approach with O(MN2) computational complexity is presented. Also, a selection operator is presented that creates a mating pool by combining the parent and offspring populations and selecting the best (with respect to fitness and spread) N solutions. Simulation results on difficult test problems show that the proposed NSGA-II, in most problems, is able to find much better spread of solutions and better convergence near the true Pareto-optimal front compared to Pareto-archived evolution strategy and strength-Pareto EA-two other elitist MOEAs that pay special attention to creating a diverse Pareto-optimal front. Moreover, we modify the definition of dominance in order to solve constrained multiobjective problems efficiently. Simulation results of the constrained NSGA-II on a number of test problems, including a five-objective seven-constraint nonlinear problem, are compared with another constrained muItiobjective optimizer and much better performance of NSGA-II is observed.},
author = {Deb, K and Pratab, S and Agarwal, S and Meyarivan, T},
doi = {10.1109/4235.996017},
isbn = {1089-778X},
issn = {1089778X},
journal = {IEEE Transactions on Evolutionary Computing},
month = {Apr},
number = {2},
pages = {182--197},
title = {{A Fast and Elitist Multiobjective Genetic Algorithm: NGSA-II}},
volume = {6},
year = {2002}
}
@booklet{Borodin:2cY4439E,
author = {Borodin, A and El, R},
title = {{Yaniv (1998): Online Computation and Competitive Analysis}}
}
@article{Soltesz:2007cu,
abstract = {Hypervisors, popularized by Xen and VMware, are quickly becoming commodity. They are appropriate for many usage scenarios, but there are scenarios that require system virtualization with high degrees of both isolation and efficiency. Examples include HPC clusters, the Grid, hosting centers, and PlanetLab. We present an alternative to hypervisors that is better suited to such scenarios. The approach is a synthesis of prior work on resource containers and security containers applied to general-purpose, time-shared operating systems. Examples of such container-based systems include Solaris 10, Virtuozzo for Linux, and Linux-VServer. As a representative instance of container-based systems, this paper describes the design and implementation of Linux-VServer. In addition, it contrasts the architecture of Linux-VServer with current generations of Xen, and shows how Linux-VServer provides comparable support for isolation and superior system efficiency.},
author = {Soltesz, Stephen and P{\"{o}}tzl, Herbert and Fiuczynski, Marc E. and Bavier, Andy and Peterson, Larry},
doi = {10.1145/1272998.1273025},
isbn = {978-1-59593-636-3},
issn = {01635980},
journal = {ACM SIGOPS Operating Systems Review},
number = {3},
pages = {275},
title = {{Container-based operating system virtualization}},
url = {http://portal.acm.org/citation.cfm?doid=1272998.1273025},
volume = {41},
year = {2007}
}
@article{Marinescu:2013wr,
abstract = {Abstract Purpose – This paper aims to explore the educational potential of “cloud computing” (CC), and how it could be exploited in enhancing engagement among educational researchers and educators to better understand and improve their practice, in increasing the quality of their students' learning outcomes, and, thus, in advancing the scholarship of teaching and learning (SoTL) in a higher education context. Design/methodology/approach – Adoption of the ideals of SoTL is considered an important approachforsalvaging the higher educationlandscapearoundtheworldthat is currently inastate offlux and evolution as a result of rapid advances in information and communications technology, and the subsequent changing needs of the digital natives. The study is based on ideas conceptualised from reading several editorialsandarticlesonserver virtualisation technologyandcloudcomputingin several journals, with the eSchool News as the most important one. The paper identifies two cloud computing tools, their salient features and describeshowcloud computing can be used to achieve the ideals of SoTL. Findings – The study reports that the cloud as a ubiquitous computing tool and a powerful platform can enable educators to practise the ideals of SoTL. Two of the most useful free “cloud computing” applications are the Google Apps for Education which is a free online suite of tools that includes Gmail for e-mail and Google Docs for documents, spreadsheets, and presentations, and Microsoft's cloud service (Live@edu) including the SkyDrive. Using the cloud approach, everybody can work on the same document at the same time to make corrections as well as improve it dynamically in a collaborative manner. Practical implications – Cloud computing has a significant place in higher education in that the appropriate use of cloud computing tools can enhance engagement among students, educators, and researchers in a cost effective manner. There are security concerns but they do not overshadow the benefits. Originality/value – The paper provides insights into the possibility of using cloud computing delivery for originating a new instructional paradigm that makes a shift possible from the traditional practice of teaching as a private affair to a peer-reviewed transparent process, and makes it known how student learning can be improved generally, not only in one's own classroom but also beyond it. Keywords Botswana, Teaching, Educational development, Digital storage, Search engines, Virtual learning environments Paper},
archivePrefix = {arXiv},
arxivId = {1003.4074},
author = {Thomas, P.Y.},
doi = {10.1108/02640471111125177},
eprint = {1003.4074},
isbn = {9783642106644},
issn = {0264-0473},
journal = {The Electronic Library},
number = {2},
pages = {214--224},
pmid = {21650144},
title = {{Cloud computing}},
url = {http://www.emeraldinsight.com/doi/10.1108/02640471111125177},
volume = {29},
year = {2011}
}
@article{Han:2017jz,
abstract = {Cloud computing has become an indispensable infrastructure that provides multi-granularity services to support large applications in the Industrial Internet of Things (IIOT), Cloud data centers have been built or extensively enlarged to cope with the growing computation and storage requirements of IIOT. The energy consumption of cloud data centers is dramatically increasing, which has created a lot of problems with greenhouse gas emissions and service costs. Server consolidation is a popular approach to reduce cloud data centers' energy consumption by minimizing the number of active physical machines. Most of the extant research has focused on server reduction in the consolidation process, but unbalanced resource utilization among different physical machines can cause the waste of physical resources. This paper proposes a resource-utilization-aware energy efficient server consolidation algorithm (RUAEE) that can be used to improve resource utilization while reducing the number of virtual machine live migrations. Experimental results show that RUAEE can reduce the energy consumption and service-level agreement (SLA) violation in cloud data center.},
author = {Han, Guangjie and Que, Wenhui and Jia, Gangyong and Zhang, Wenbo},
doi = {10.1016/j.jnca.2017.07.011},
issn = {10958592},
journal = {Journal of Network and Computer Applications},
keywords = {Energy efficient,Industrial Internet of Things (IIOT),Resource-utilization-aware,Server consolidation},
month = {Jul},
title = {{Resource-utilization-aware energy efficient server consolidation algorithm for green computing in IIOT}},
year = {2017}
}
@article{SLA_metric,
abstract = {Effective SLAs are extremely important to assure business continuity, customer satisfaction and trust. The metrics used to measure and manage performance compliance to SLA commitments are the heart of a successful agreement and are a critical long term success factor. Lack of experience in the use and automation of performance metrics causes problems for many organizations as they attempt to formulate their SLA strategies and set the metrics needed to support those strategies. This paper contributes to a systematic categorization of SLA contents with a particular focus on SLA metrics. The intended goal is to support the design and implementation of automatable SLAs based on efficient metrics for automated monitoring and reporting. The categorization facilitates design decisions, analysis of existing SLAs and helps to identify responsibilities for critical IT processes in disruption management during the execution of SLAs.},
author = {Paschke, Adrian and Schnappinger-Gerull, Elisabeth},
isbn = {3-88579-174-9},
issn = {18632351},
journal = {Service Oriented Electronic Commerce},
number = {25-40},
pages = {25--40},
title = {{A Categorization Scheme for SLA Metrics}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.93.1800{\&}rep=rep1{\&}type=pdf},
volume = {80},
year = {2006}
}
@article{Tan:2011jd,
abstract = {Understanding the resource utilization in computing clouds is critical for efficient resource planning and better operational performance. In this paper, we propose two ways, from microscopic and macroscopic perspectives, to predict the resource consumption for data centers by statistically characterizing resource usage patterns. The first approach focuses on the usage prediction for a specific node. Compared to the basic method of calibrating AR models for CPU usages separately, we find that using both CPU and memory usage data can improve the forecasting performance. The second approach is based on Principal Component Analysis (PCA) to identify resource usage patterns across different nodes. Using the identified patterns, we can reduce the number of parameters for predicting the resource usage on multiple nodes. In addition, using the principal components obtained from PCA, we propose an optimization framework to optimally consolidate VMs into a number of physical servers and in the meanwhile reduce the resource usage variability. The evaluation of the proposed approaches is based on traces collected from a production cloud environment.},
author = {Tan, Jian and Dube, Parijat and Meng, Xiaoqiao and Zhang, Li},
doi = {10.1109/ICDCSW.2011.53},
isbn = {9780769543864},
issn = {1545-0678},
journal = {Proceedings - International Conference on Distributed Computing Systems},
pages = {14--19},
title = {{Exploiting resource usage patterns for better utilization prediction}},
year = {2011}
}
@article{Hameed:2016cmb,
author = {Zomaya, Abdul HameedAlireza KhoshkbarforoushhaRajiv RanjanPrem Prakash JayaramanJoanna KolodziejPavan BalajiSherali ZeadallyQutaibah Marwan MalluhiNikos TziritasAbhinav VishnuSamee U KhanAlbert},
journal = {Computing},
number = {7},
pages = {751--774},
title = {{A survey and taxonomy on energy efficient resource allocation techniques for cloud computing systems}},
volume = {NA},
year = {2014}
}
@article{Varasteh:2015fu,
abstract = {Data centers and their applications are exponentially growing. Consequently, their energy consumption and environmental impacts have also become increasingly more important. Virtualization technologies are widely used in modern data centers to ease the management of the data center and to reduce its energy consumption. Data centers that employ virtualization technologies are typically called virtualized or cloud data centers. Virtualization technologies enable virtual machine (VM) live migration, which allows the VMs to be freely moved among physical machines (PMs) with negligible downtime. Thus, several VMs can be packed on a single PM so as to let the PM run in its more energy-efficient working condition. This technique is called server consolidation and is an effective and widely used approach to reduce total energy consumption in data centers. Server consolidation can be done in various ways and by considering various parameters and effects. This paper presents a survey and taxonomy for server consolidation techniques in cloud data centers. Special attention has been devoted to the parameters and algorithmic approaches used to consolidate VMs onto PMs. In this end, we also discuss open challenges and suggest areas for further research.},
author = {Varasteh, Amir and Goudarzi, Maziar},
doi = {10.1109/JSYST.2015.2458273},
issn = {19379234},
journal = {IEEE Systems Journal},
keywords = {Cloud computing,data center,energy efficiency,resource allocation,server consolidation,virtualization},
number = {2},
pages = {772--783},
title = {{Server Consolidation Techniques in Virtualized Data Centers: A Survey}},
volume = {11},
year = {2017}
}
@article{Felter:2015ki,
abstract = {IBM Research Report Isolation and resource control for cloud applications has traditionally been achieve through the use of virtual machines. Deploying applications in a VM results in reduced performance due to the extra levels of abstraction. In a cloud environment, this results in loss efficiency for the infrastructure. Newer advances in container-based virtualization simplifies the deployment of applications while isolating them from one another. In this paper, we explore the performance of traditional virtual machine deployments, and contrast them with the use of Linux containers. We use a suite of workloads that stress the CPU, memory, storage and networking resources. Our results show that containers result in equal or better performance than VM in almost all cases. Both VMs and containers require tuning to support I/O-intensive applicaions. We also discuss the implications of our performance results for future cloud architecture.},
author = {Felter, Wes and Ferreira, Alexandre and Rajamony, Ram and Rubio, Juan},
doi = {10.1109/ISPASS.2015.7095802},
isbn = {978-1-4799-1957-4},
journal = {2015 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
pages = {171--172},
title = {{An updated performance comparison of virtual machines and Linux containers}},
url = {http://ieeexplore.ieee.org/document/7095802/},
year = {2015}
}
@article{Fan:2007jr,
abstract = {Large-scale Internet services require a computing infrastructure that can beappropriately described as a warehouse-sized computing system. The cost ofbuilding datacenter facilities capable of delivering a given power capacity tosuch a computer can rival the recurring energy consumption costs themselves.Therefore, there are strong economic incentives to operate facilities as closeas possible to maximum capacity, so that the non-recurring facility costs canbe best amortized. That is difficult to achieve in practice because ofuncertainties in equipment power ratings and because power consumption tends tovary significantly with the actual computing activity. Effective powerprovisioning strategies are needed to determine how much computing equipmentcan be safely and efficiently hosted within a given power budget. In this paper we present the aggregate power usage characteristics of largecollections of servers (up to 15 thousand) for different classes ofapplications over a period of approximately six months. Those observationsallow us to evaluate opportunities for maximizing the use of the deployed powercapacity of datacenters, and assess the risks of over-subscribing it. We findthat even in well-tuned applications there is a noticeable gap (7 - 16{\%})between achieved and theoretical aggregate peak power usage at the clusterlevel (thousands of servers). The gap grows to almost 40{\%} in wholedatacenters. This headroom can be used to deploy additional compute equipmentwithin the same power budget with minimal risk of exceeding it. We use ourmodeling framework to estimate the potential of power management schemes toreduce peak power and energy usage. We find that the opportunities for powerand energy savings are significant, but greater at the cluster-level (thousandsof servers) than at the rack-level (tens). Finally we argue that systems needto be power efficient across the activity range, and not only at peakperformance levels.},
archivePrefix = {arXiv},
arxivId = {arXiv:1006.1401v2},
author = {Fan, Xiaobo and Weber, Wolf-Dietrich and Barroso, Luiz Andre},
doi = {10.1145/1273440.1250665},
eprint = {arXiv:1006.1401v2},
isbn = {9781595937063},
issn = {01635964},
journal = {ACM SIGARCH Computer Architecture News},
keywords = {energy efficiency,power modeling,power provisioning},
number = {June},
pages = {13},
pmid = {25246403},
title = {{Power provisioning for a warehouse-sized computer}},
volume = {35},
year = {2007}
}
@article{Burke:2006ei,
abstract = {The bin-packing problem is a well known NP-Hard optimisation problem, and, over the years, many heuristics have been developed to generate good quality solutions. This paper outlines a genetic programming system which evolves a heuristic that decides whether to put a piece in a bin when presented with the sum of the pieces already in the bin and the size of the piece that is about to be packed. This heuristic operates in a fixed framework that iterates through the open bins, applying the heuristic to each one, before deciding which bin to use. The best evolved programs emulate the functionality of the human designed first-fit heuristic. Thus, the contribution of this paper is to demonstrate that genetic programming can be employed to automatically evolve bin packing heuristics which are the same as high quality heuristics which have been designed by humans.},
author = {Burke, Edmund K and Hyde, Matthew R and Kendall, Graham},
doi = {10.1007/11844297_87},
isbn = {978-3-540-38990-3},
issn = {03029743},
journal = {Parallel Problem Solving from Nature - PPSN IX},
keywords = {genetic algorithms,genetic programming},
number = {Chapter 87},
pages = {860--869},
title = {{Evolving Bin Packing Heuristics with Genetic Programming}},
url = {http://www.cs.nott.ac.uk/{~}mvh/ppsn2006.pdf},
volume = {4193},
year = {2006}
}
@article{Wang:2011di,
author = {Wang, Yuping},
journal = {INFORMS Journal on Computing},
keywords = {2010,29,accepted by michel gendreau,accepted september 2010,articles in advance december,august 2006,bilevel programming,evolutionary algorithm,former area editor for,global optimization,heuristic search and learning,history,june 2009,june 2010,published online in,received,revised september 2007},
number = {4},
pages = {618--629},
title = {{A New Evolutionary Algorithm for a Class of Nonlinear Bilevel Programming Problems and Its Global Convergence}},
volume = {23},
year = {2011}
}
@article{Magalhaes:2015ep,
abstract = {Workload modeling enables performance analysis and simulation of cloud resource management policies, which allows cloud providers to improve their systems' Quality of Service (QoS) and researchers to evaluate new policies without deploying expensive large scale environments. However, workload modeling is challenging in the context of cloud computing due to the virtualization layer overhead, insufficient tracelogs available for analysis, and complex workloads. These factors contribute to a lack of methodologies and models to characterize applications hosted in the cloud. To tackle the above issues, we propose a web application model to capture the behavioral patterns of different user profiles and to support analysis and simulation of resources utilization in cloud environments. A model validation was performed using graphic and statistical hypothesis methods. An implementation of our model is provided as an extension of the CloudSim simulator.},
author = {Magalh{\~{a}}es, Deborah and Calheiros, Rodrigo N. and Buyya, Rajkumar and Gomes, Danielo G.},
doi = {10.1016/j.compeleceng.2015.08.016},
issn = {00457906},
journal = {Computers {\&} Electrical Engineering},
keywords = {Cloud computing,Distribution analysis,Simulation,Web applications,Workload modeling and characterization},
pages = {69--81},
title = {{Workload modeling for resource usage analysis and simulation in cloud computing}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S004579061500302X},
volume = {47},
year = {2015}
}
@book{Eberhart:1995tj,
author = {Kennedy, J and EBERHART, C},
booktitle = {Piscataway, NJ: IEEE, c1995},
publisher = {Perth},
title = {{Particle swarm optimiza-tion: proceeding of IEEE International Conference onNeural Networks}},
volume = {1948},
year = {1942}
}
@article{Shi:2011ke,
abstract = {Motivated by the limit on the power usage effectiveness (PUE) of the data centers, the potential benefit of the consolidation, and the impetus of achieving maximum return on investment (ROI) on the cloud computing market, we investigate VM placement in the data center, formulate a multi-level generalized assignment problem (MGAP) for maximizing the profit under the service level agreement and the power budget constraint based on the model of a virtualized data center, and solve it with a first-fit heuristic. Numerical simulations show that the first-fit heuristic is effective in solving the large-scale instances of the MGAP with the sampled simulation setups.},
author = {Shi, Weiming and Hong, Bo},
doi = {10.1109/UCC.2011.28},
isbn = {978-1-4577-2116-8},
journal = {2011 Fourth IEEE International Conference on Utility and Cloud Computing},
keywords = {-profit maximization,Data Center,Multi-level Generalized Assignment Problem,Profit Maximization,VM Placement,data center,multi-level generalized assignment problem,vm placement},
pages = {138--145},
title = {{Towards Profitable Virtual Machine Placement in the Data Center}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6123491{\%}5Cnhttp://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6123491},
year = {2011}
}
@article{Holland:1962fy,
abstract = {The purpose of this paper is to outline a theory of automata appropriate to the properties, requirements and questions of adaptation. The conditions that such a theory should satisfy come from not one but several fields: It should be possible to formulate, at least in an abstract version, some of the key hypotheses and problems from relevant parts of biology, particularly the areas concerned with molecular control and neurophysiology. The work in theoretical genetics initiated by R. A. Fisher 5 and Sewall Wright 24 should find a natural place in the theory. At the same time the rigorous methods of automata theory should be brought to bear (particularly those parts concerned with growing automata 1, 2, 3, 7, 8, 12, 15, 18, 23). Finally the theory should include among its models abstract counterparts of artificial adaptive systems currently being studied, systems such as Newell-Shaw-Simon's "General Problem Solver" 13, Selfridge's "Pandemonium" 17, von Neumann's self-reproducing automata 22 and Turing's morphogenetic systems 19, 20.},
author = {Holland, John H.},
doi = {10.1145/321127.321128},
isbn = {978-3-642-19156-5 978-3-642-19157-2},
issn = {00045411},
journal = {Journal of the ACM},
number = {3},
pages = {297--314},
title = {{Outline for a Logical Theory of Adaptive Systems}},
volume = {9},
year = {1962}
}
@article{Forsman:2015ca,
author = {Forsman, Mattias and Glad, Andreas and Lundberg, Lars and Ilie, Dragos},
doi = {10.1016/j.jss.2014.11.044},
issn = {0164-1212},
journal = {Elsevier},
keywords = {live migration,load balancing,virtualization},
pages = {110--126},
title = {{Algorithms for Automated Live Migration of Virtual Machines Mattias}},
url = {http://dx.doi.org/10.1016/j.jss.2014.11.044},
volume = {101},
year = {2014}
}
@article{2009LNCS.5931...24B,
abstract = {Cloud computing aims to power the next generation data centers and enables application service providers to lease data center capabilities for deploying applications depending on user QoS (Quality of Service) requirements. Cloud applications have different composition, configuration, and deployment requirements. Quantifying the performance of resource allocation policies and application scheduling algorithms at finer details in Cloud computing environments for different application and service models under varying load, energy performance (power consumption, heat dissipation), and system size is a challenging problem to tackle. To simplify this process, in this paper we propose CloudSim: an extensible simulation toolkit that enables modelling and simulation of Cloud computing environments. The CloudSim toolkit supports modelling and creation of one or more virtual machines (VMs) on a simulated node of a Data Center, jobs, and their mapping to suitable VMs. It also allows simulation of multiple Data Centers to enable a study on federation and associated policies for migration of VMs for reliability and automatic scaling of applications.},
archivePrefix = {arXiv},
arxivId = {0907.4878},
author = {Buyya, Rajkumar and Ranjan, Rajiv and Calheiros, Rodrigo N.},
doi = {10.1109/HPCSIM.2009.5192685},
eprint = {0907.4878},
isbn = {9781424449071},
journal = {Proceedings of the 2009 International Conference on High Performance Computing and Simulation, HPCS 2009},
number = {Chapter 4},
pages = {1--11},
title = {{Modeling and simulation of scalable cloud computing environments and the cloudsim toolkit: Challenges and opportunities}},
volume = {cs.DC},
year = {2009}
}
@article{Sinha:2013tn,
abstract = {Bilevel optimization problems are a class of challenging optimization problems, which contain two levels of optimization tasks. In these problems, the optimal solutions to the lower level problem become possible feasible candidates to the upper level problem. Such a requirement makes the optimization problem difficult to solve, and has kept the researchers busy towards devising methodologies, which can efficiently handle the problem. Despite the efforts, there hardly exists any effective methodology, which is capable of handling a complex bilevel problem. In this paper, we introduce bilevel evolutionary algorithm based on quadratic approximations (BLEAQ) of optimal lower level variables with respect to the upper level variables. The approach is capable of handling bilevel problems with different kinds of complexities in relatively smaller number of function evaluations. Ideas from classical optimization have been hybridized with evolutionary methods to generate an efficient optimization algorithm for generic bilevel problems. The efficacy of the algorithm has been shown on two sets of test problems. The first set is a recently proposed SMD test set, which contains problems with controllable complexities, and the second set contains standard test problems collected from the literature. The proposed method has been evaluated against two benchmarks, and the performance gain is observed to be significant.},
archivePrefix = {arXiv},
arxivId = {1303.3901},
author = {Sinha, Ankur and Malo, Pekka and Deb, Kalyanmoy},
eprint = {1303.3901},
month = {Mar},
title = {{Efficient Evolutionary Algorithm for Single-Objective Bilevel Optimization}},
url = {http://arxiv.org/abs/1303.3901},
year = {2013}
}
@inproceedings{Zhu:2006in,
author = {Zhu, Xiaobo and Yu, Qian and Wang, Xianjia},
booktitle = {Cognitive Informatics, 2006. ICCI 2006. 5th IEEE Intl. Conf. on},
pages = {126--131},
publisher = {IEEE},
title = {{A Hybrid Differential Evolution Algorithm for Solving Nonlinear Bilevel Programming with Linear Constraints}},
volume = {1},
year = {2006}
}
@article{Pahl:2015ek,
author = {Pahl, Claus and Bozen-bolzano, Libera Universit{\`{a}} and Pahl, Claus},
doi = {10.1109/MCC.2015.51},
journal = {IEEE Cloud Computing},
number = {September},
pages = {24--31},
title = {{Containerisation and the PaaS Cloud Containerisation and the PaaS Cloud}},
volume = {2},
year = {2015}
}
@article{Vaquero:2011gb,
abstract = {Scalability is said to be one of the major advantages brought by the cloud paradigm and, more specifically, the one that makes it different to an "advanced outsourcing" solution. However, there are some important pending issues before making the dreamed automated scaling for applications come true. In this paper, the most notable initiatives towards whole application scalability in cloud environments are presented. We present relevant efforts at the edge of state of the art technology, providing an encompassing overview of the trends they each follow. We also highlight pending challenges that will likely be addressed in new research efforts and present an ideal scalable cloud system.},
author = {Vaquero, Luis M. and Rodero-Merino, Luis and Buyya, Rajkumar},
doi = {10.1145/1925861.1925869},
isbn = {0146-4833},
issn = {01464833},
journal = {ACM SIGCOMM Computer Communication Review},
number = {1},
pages = {45},
title = {{Dynamically scaling applications in the cloud}},
url = {http://portal.acm.org/citation.cfm?doid=1925861.1925869},
volume = {41},
year = {2011}
}
@article{Kunkle:2008bz,
abstract = {This paper describes a new load balancing framework for high-performance clustered storage systems. The framework provides a general method for reconfiguring a system facing dynamic workloads that simultaneously balances load and minimizes the cost of reconfiguration. The framework can be used for automatic reconfiguration or to present an administrator with a range of (near) optimal reconfiguration options, allowing a tradeoff between load distribution and reconfiguration cost. The framework supports a wide range of measures for load imbalance and reconfiguration cost, as well as several optimization techniques. We demonstrate the effectiveness of the this framework by using it to balance the workload on a NetApp ONTAP GX system, a commercial scale-out clustered NFS server implementation. The evaluation scenario considers consolidating two real world systems, with hundreds of users each: a six-node clustered storage system with 120 TB of usable storage, storing home directories; and a legacy system, supporting three email severs.},
author = {Kunkle, Daniel and Schindler, Jiri},
doi = {10.1007/978-3-540-89894-8_9},
isbn = {354089893X},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {1},
pages = {57--72},
title = {{A load balancing framework for clustered storage systems}},
volume = {5374 LNCS},
year = {2008}
}
@article{Mishra:2012kx,
abstract = {Virtualization is a key concept in enabling the "computing-as-a- service" vision of cloud-based solutions. Virtual machine related features such as flexible resource provisioning, and isolation and migration of machine state have improved efficiency of resource usage and dynamic resource provisioning capabilities. Live virtual machine migration transfers {\^{A}}¿state{\^{A}}¿ of a virtual machine from one physical machine to another, and can mitigate overload conditions and enables uninterrupted maintenance activities. The focus of this article is to present the details of virtual machine migration techniques and their usage toward dynamic resource management in virtualized environments. We outline the components required to use virtual machine migration for dynamic resource management in the virtualized cloud environment. We present categorization and details of migration heuristics aimed at reducing server sprawl, minimizing power consumption, balancing load across physical machines, and so on. We conclude with a discussion of open research problems in the area. {\textcopyright} 2012 IEEE.},
author = {Mishra, M and Das, A and Kulkarni, P and Sahoo, A},
doi = {10.1109/MCOM.2012.6295709},
issn = {01636804 (ISSN)},
journal = {IEEE Communications Magazine},
keywords = {Dynamic resource management,Dynamic resource provisioning,Dynamics,Environmental management,Flexible resources,Machine state,Maintenance activity,Migration technique,Natural resources management,Overload condition,Research problems,Resource allocation,Resource usage,Virtual machines,Virtual reality,Virtualizations,Virtualized environment},
number = {9},
pages = {34--40},
title = {{Dynamic resource management using virtual machine migrations}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866296555{\&}partnerID=40{\&}md5=0383e9730b9417d53ac1b8def7824b8b},
volume = {50},
year = {2012}
}
@article{Sinha:2017et,
abstract = {Bilevel optimization is defined as a mathematical program, where an optimization problem contains another optimization problem as a constraint. These problems have received significant attention from the mathematical programming community. Only limited work exists on bilevel problems using evolutionary computation techniques; however, recently there has been an increasing interest due to the proliferation of practical applications and the potential of evolutionary algorithms in tackling these problems. This paper provides a comprehensive review on bilevel optimization from the basic principles to solution strategies; both classical and evolutionary. A number of potential application problems are also discussed. To offer the readers insights on the prominent developments in the field of bilevel optimization, we have performed an automated text-analysis of an extended list of papers published on bilevel optimization to date. This paper should motivate evolutionary computation researchers to pay more attention to this practical yet challenging area.},
archivePrefix = {arXiv},
arxivId = {1705.06270},
author = {Sinha, Ankur and Malo, Pekka and Deb, Kalyanmoy},
doi = {10.1109/TEVC.2017.2712906},
eprint = {1705.06270},
isbn = {1089-778X VO - PP},
issn = {1089-778X},
journal = {IEEE Transactions on Evolutionary Computation},
pages = {1},
title = {{A Review on Bilevel Optimization: From Classical to Evolutionary Approaches and Applications}},
url = {http://arxiv.org/abs/1705.06270},
year = {2017}
}
@article{Tomas:2013iv,
abstract = {Despite the potential given by the combination of multi-tenancy and virtualization, resource utilization in today's data centers is still low. We identify three key characteristics of cloud services and infrastructure as-a-service management practices: burstiness in service workloads, fluctuations in virtual machine resource usage over time, and virtual machines being limited to pre-defined sizes only. Based on these characteristics, we propose scheduling and admission control algorithms that incorporate resource overbooking to improve utilization. A combination of modeling, monitoring, and prediction techniques is used to avoid overpassing the total infrastructure capacity. A performance evaluation using a mixture of workload traces demonstrates the potential for significant improvements in resource utilization while still avoiding overpassing the total capacity. {\textcopyright} 2013 ACM.},
author = {Tom{\'{a}}s, Luis and Tordsson, Johan},
doi = {10.1145/2494621.2494627},
isbn = {9781450321723},
journal = {Proceedings of the 2013 ACM Cloud and Autonomic Computing Conference on - CAC '13},
pages = {1},
title = {{Improving cloud infrastructure utilization through overbooking}},
url = {http://dl.acm.org/citation.cfm?doid=2494621.2494627},
year = {2013}
}
@article{Feller:2011vs,
author = {Feller, Eugen and Rilling, Louis and Morin, Christine},
journal = {GRID},
title = {{Energy-Aware Ant Colony Based Workload Placement in Clouds Energy-Aware Ant Colony Based Workload Placement in Clouds}},
year = {2011}
}
@inproceedings{dubois2015autonomic,
abstract = {The spot instance model is a virtual machine pricing scheme in which unused resources of cloud providers are offered to the highest bidder. This leads to the formation of a spot price, whose fluctuations can determine customers to be overbid by other users and lose the virtual machine they rented. In this paper we propose a heuristic to automate the decision on: (i) which and how many resources to rent in order to run a cloud application, (ii) how to map the application components to the rented resources, and (iii) what spot price bids to use in order to minimize the total bid price while maintaining an acceptable level of performance. To drive the decision making, our algorithm combines a multi-class queueing network model of the application with a Markov model that describes the stochastic evolution of the spot price and its influence on virtual machine reliability. We show, using a model developed for a real enterprise application and historical traces of the Amazon EC2 spot instance prices, that our heuristic finds low cost solutions that indeed guarantee the required levels of performance. The performance of our heuristic method is compared to that of nonlinear programming and shown to markedly accelerate the finding of low-cost optimal solutions.},
author = {Dubois, Daniel J. and Casale, Giuliano},
booktitle = {Proceedings - 2015 International Conference on Cloud and Autonomic Computing, ICCAC 2015},
doi = {10.1109/ICCAC.2015.21},
isbn = {0769556361},
keywords = {application mapping,bidding strategy,cloud provisioning,fluid-approximated queueing networks,random environment,resource allocation,spot cloud},
organization = {IEEE},
pages = {57--68},
title = {{Autonomic Provisioning and Application Mapping on Spot Cloud Resources}},
year = {2015}
}
@article{Somani:2009ho,
abstract = {Modern data centers use virtual machine based implementation for numerous advantages like resource isolation, hardware utilization, security and easy management. Applications are generally hosted on different virtual machines on a same physical machine. Virtual machine monitor like Xen is a popular tool to manage virtual machines by scheduling them to use resources such as CPU, memory and network. Performance isolation is the desirable thing in virtual machine based infrastructure to meet service level objectives. Many experiments in this area measure the performance of applications while running the applications in different domains, which gives an insight into the problem of isolation. In this paper we run different kind of benchmarks simultaneously in Xen environment to evaluate the isolation strategy provided by Xen. Results are presented and discussed for different combinations and a case of I/O intensive applications with low response latency has been presented.},
author = {Somani, Gaurav and Chaudhary, Sanjay},
doi = {10.1109/CLOUD.2009.78},
isbn = {9780769538402},
journal = {CLOUD 2009 - 2009 IEEE International Conference on Cloud Computing},
pages = {41--48},
title = {{Application performance isolation in virtualization}},
year = {2009}
}
@article{Jung:2008vb,
abstract = {Creating good adaptation policies is critical to building complex autonomic systems since it is such policies that de- fine the system configuration used in any given situation. While online approaches based on control theory and rule- based expert systems are possible solutions, each has its disadvantages. Here, a hybrid approach is described that uses modeling and optimization offline to generate suitable configurations, which are then encoded as policies that are used at runtime. The approach is demonstrated on the prob- lem of providing dynamic management in virtualized con- solidated server environments that host multiple multi-tier applications. Contributions include layered queuing mod- els for Xen-based virtual machine environments, a novel optimization technique that uses a combination of bin pack- ing and gradient search, and experimental results that show that automatic offline policy generation is viable and can be accurate even with modest computational effort.},
author = {Jung, Gueyoung and Joshi, Kaustubh R. and Hiltunen, Matti A. and Schlichting, Richard D. and Pu, Calton},
doi = {10.1109/ICAC.2008.21},
isbn = {9780769531755},
journal = {5th International Conference on Autonomic Computing, ICAC 2008},
pages = {23--32},
title = {{Generating adaptation policies for multi-tier applications in consolidated server environments}},
year = {2008}
}
@article{Waldspurger:2002db,
abstract = {VMware ESX Server is a thin software layer designed to multiplex hardware resources efﬁciently among virtual machines running unmodiﬁed commodity operating systems. This paper introduces several novel ESX Server mechanisms and policies for managing memory. A ballooning technique$\backslash$nreclaims the pages considered least valuable by the operating system running in a virtual machine. An idle memory tax achieves efﬁcient memory utilization while maintaining performance isolation guarantees. Content-based page sharing and hot I/O page remapping exploit transparent page remapping to eliminate redundancy and reduce copying overheads. These techniques are combined to efﬁciently support virtual machine workloads that overcommit memory. },
author = {Waldspurger, Carl A.},
doi = {10.1145/844128.844146},
isbn = {0163-5980},
issn = {01635980},
journal = {ACM SIGOPS Operating Systems Review},
month = {Dec},
number = {SI},
pages = {181},
pmid = {12370846},
title = {{Memory resource management in VMware ESX server}},
url = {http://portal.acm.org/citation.cfm?doid=844128.844146},
volume = {36},
year = {2002}
}
@article{Beloglazov:2010dt,
abstract = {The rapid growth in demand for computational power driven by modern service applications combined with the shift to the Cloud computing model have led to the establishment of large-scale virtualized data centers. Such data centers consume enormous amounts of electrical energy resulting in high operating costs and carbon dioxide emissions. Dynamic consolidation of virtual machines (VMs) and switching idle nodes off allow Cloud providers to optimize resource usage and reduce energy consumption. However, the obligation of providing high quality of service to customers leads to the necessity in dealing with the energy-performance trade-off. We propose a novel technique for dynamic consolidation of VMs based on adaptive utilization thresholds, which ensures a high level of meeting the Service Level Agreements (SLA). We validate the high efficiency of the proposed technique across different kinds of workloads using workload traces from more than a thousand PlanetLab servers.},
archivePrefix = {arXiv},
arxivId = {arXiv:1006.1401v2},
author = {Beloglazov, Anton and Buyya, Rajkumar},
doi = {10.1145/1890799.1890803},
eprint = {arXiv:1006.1401v2},
isbn = {9781450304535},
issn = {0920-8542},
journal = {Proceedings of the 8th International Workshop on Middleware for Grids, Clouds and e-Science},
keywords = {cloud computing,green it,vm placement},
number = {December 2010},
pages = {6},
title = {{Adaptive Threshold-Based Approach for Energy-Efficient Consolidation of Virtual Machines in Cloud Data Centers}},
year = {2011}
}
@article{Wilcox:2011ea,
abstract = {We formally define multi-capacity bin packing, a generalization of conventional bin packing, and develop an algorithm called Reordering Grouping Genetic Algorithm (RGGA) to assign VMs to servers. We first test RGGA on conventional bin packing problems and show that it yields excellent results but much more efficiently. We then generate a multi-constraint test set, and demonstrate the effectiveness of RGGA in this context. Lastly, we show the applicability of RGGA in its desired context by using it to develop an assignment of real virtual machines to servers.},
author = {Wilcox, David and McNabb, Andrew and Seppi, Kevin},
doi = {10.1109/CEC.2011.5949641},
isbn = {9781424478347},
journal = {2011 IEEE Congress of Evolutionary Computation, CEC 2011},
keywords = {Bin Packing,Genetic Algorithm,Virtualization},
pages = {362--369},
title = {{Solving virtual machine packing with a Reordering Grouping Genetic Algorithm}},
year = {2011}
}
@article{Bobroff:2007ec,
abstract = {A dynamic server migration and consolidation algorithm is introduced. The algorithm is shown to provide substantial improvement over static server consolidation in reducing the amount of required capacity and the rate of service level agreement violations. Benefits accrue for workloads that are variable and can be forecast over intervals shorter than the time scale of demand variability. The management algorithm reduces the amount of physical capacity required to support a specified rate of SLA violations for a given workload by as much as 50{\%} as compared to static consolidation approach. Another result is that the rate of SLA violations at fixed capacity may be reduced by up to 20{\%}. The results are based on hundreds of production workload traces across a variety of operating systems, applications, and industries.},
author = {Bobroff, Norman and Kochut, Andrzej and Beaty, Kirk},
doi = {10.1109/INM.2007.374776},
isbn = {1424407990},
issn = {1-4244-0798-2},
journal = {10th IFIP/IEEE International Symposium on Integrated Network Management 2007, IM '07},
pages = {119--128},
pmid = {20210421},
title = {{Dynamic placement of virtual machines for managing SLA violations}},
year = {2007}
}
@article{Ferreto:2011iia,
abstract = {Virtualization has become a key technology for simplifying service management and reducing energy costs in data centers. One of the challenges faced by data centers is to decide when, how, and which virtual machines (VMs) have to be consolidated into a single physical server. Server consolidation involves VM migration, which has a direct impact on service response time. Most of the existing solutions for server consolidation rely on eager migrations, which try to minimize the number of physical servers running VMs. These solutions generate unnecessary migrations due to unpredictable workloads that require VM resizing. This paper proposes an LP formulation and heuristics to control VM migration, which prioritize virtual machines with steady capacity. We performed experiments using TU-Berlin and Google data center workloads to compare our migration control strategy against existing eager-migration-based solutions. We observed that avoiding migration of VMs with steady capacity reduces the number of migrations with minimal penalty in the number of physical servers. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
author = {Ferreto, Tiago C. and Netto, Marco A.S. and Calheiros, Rodrigo N. and {De Rose}, C{\'{e}}sar A.F.},
doi = {10.1016/j.future.2011.04.016},
isbn = {0167-739X},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Data centers,Migration control,Server consolidation,Virtualization},
number = {8},
pages = {1027--1034},
title = {{Server consolidation with migration control for virtualized data centers}},
volume = {27},
year = {2011}
}
@article{Mann:2015ua,
author = {Mann, Zoltan Adam},
journal = {Proceedings of the 9th Hungarian -Japanese Symposium on Discrete Mathematics and Its Applications},
pages = {21--30},
title = {{Approximability of VM allocation : Much harder than bin packing}},
year = {2015}
}
@article{Vicente:1994ie,
abstract = {The bilevel programming problem involves two optimiza-tion problems where the data of the first one is implicitly determined by the solution of the second. In this paper, we introduce two descent methods for a special instance of bilevel programs where the inner problem is strictly convex quadratic. The first algorithm is based on pivot steps and may not guarantee local optimality. A modified steepest descent algorithm is presented to overcome this drawback. New rules for computing exact stepsizes are introduced and a hybrid approach that combines both strategies is discussed. It is proved that checking local optimality in bilevel programming is a NP-hard problem.},
author = {Vicente, L. and Savard, G. and J{\'{u}}dice, J.},
doi = {10.1007/BF02191670},
issn = {00223239},
journal = {Journal of Optimization Theory and Applications},
keywords = {Bilevel programming,computational complexity,nonconvex and nondifferentiable optimization,quadratic programming},
month = {May},
number = {2},
pages = {379--399},
title = {{Descent approaches for quadratic bilevel programming}},
volume = {81},
year = {1994}
}
@article{Wen:1991kt,
abstract = {Multi-level programming is characterized as mathematical programming to solve decentralized planning problems. The decision variables are partitioned among ordered levels. A decision-maker at one level of the hierarchy may have his own objective function and decision space, but may be influenced by other levels. During the last 10 years, a special case of the multi-level programming problem, the linear bi-level programming (BLP) problem, has been studied with increasing interest in the area of mathematical programming problems. This paper attempts to review the literature on the linear BLP problems. It presents the basic models and the characterizations of the problem, the areas for application, the existing solution approaches, and the related models and areas for further research.},
author = {Wen, Ue-Pyng and Hsu, Shuh-Tzy},
doi = {10.1057/jors.1991.23},
isbn = {01605682},
issn = {01605682},
journal = {The Journal of the Operational Research Society},
keywords = {bi-level programming,decentralized planning,decision-making},
month = {Feb},
number = {2},
pages = {125--133},
title = {{Linear Bi-Level Programming Problems -- A Review}},
url = {http://www.jstor.org/stable/2583177},
volume = {42},
year = {1991}
}
@inproceedings{Grimes:2016ia,
abstract = {{\textcopyright} 2016 IEEE.Energy consumption in data centres accounts for a significant proportion of national energy usage in many countries. One approach for reducing energy consumption is to improve the server usage efficiency via workload consolidation. However, there are two primary reasons why this is not done to a large extent. The first reason is that greater consolidation could result in violations of Service Level Agreements (SLAs) if resources are over-utilised. The second reason is that users specify the requirements of a virtual machine (VM) based on the maximum estimated usage for each resource over the whole life span of the VM, and usually over-estimate these maximum values to avoid possible contract violations. Typically, the VM will have significantly lower resource usage in most time periods. Recently, a number of methods have been proposed to predict resource usage of VMs. We show that although these prediction techniques are efficient when their performances are measured using well known metrics, a low prediction error can still result in significant violations of SLAs if not handled properly during workload allocation. Our results emphasise the importance of analysing workload prediction in conjunction with workload allocation techniques. We examine the impact of using predicted resource usage for optimal server consolidation. We investigate the occurrences of over-utilised resources on servers due to under-predicted resource usage. We propose methods to reduce the likelihood of such occurrences, both through the enforcement of safety capacities on the server side, and through biasing towards over-prediction on the VM side. The results indicate that an appropriate balance can be found between energy savings and non-violation of SLAs.},
author = {Grimes, Diarmuid and Mehta, Deepak and Osullivan, Barry and Birke, Robert and Chen, Lydia and Scherer, Thomas and Castineiras, Ignacio},
booktitle = {Proceedings - 2016 IEEE 24th International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems, MASCOTS 2016},
doi = {10.1109/MASCOTS.2016.60},
isbn = {9781509034314},
pages = {271--276},
publisher = {IEEE},
title = {{Robust server consolidation: Coping with peak demand underestimation}},
year = {2016}
}
@book{Anonymous:XSpanrUd,
abstract = {In this paper we introduce an examination timetabling problem from the MARA University of Technology (UiTM). UiTM is the largest university in Malaysia. It has 13 branch campuses and offers 144 programmes, delivered by 18 faculties. This dataset differs from the others reported in the literature due to weekend constraints that have to be observed. We present their examination timetabling problem with respect to its size, complexity and constraints. We analyse their real-world data, and produce solutions utilising a tabu-search-based hyper-heuristic. Since this is a new dataset, and no solutions have been published in the literature, we can only compare our results with an existing manual solution. We find that our solution is at least 80{\%} better with respect to proximity cost. We also compare our approach against a benchmark dataset and show that our method is able to produce good quality results.},
address = {Berlin, Heidelberg},
archivePrefix = {arXiv},
arxivId = {9780201398298},
doi = {10.1007/BFb0055877},
eprint = {9780201398298},
isbn = {978-3-540-64979-3},
issn = {0302-9743},
pmid = {4520227},
publisher = {Springer, Berlin, Heidelberg},
title = {{Practice and Theory of Automated Timetabling II}},
url = {http://link.springer.com/10.1007/BFb0055877},
volume = {1408},
year = {1998}
}
@article{:2017ff,
author = {Sun, Yu and White, Jules and Li, Bo and Walker, Michael and Turner, Hamilton},
doi = {10.1007/s10515-016-0191-0},
isbn = {1051501601},
issn = {15737535},
journal = {Automated Software Engineering},
keywords = {Automated software deployment,Bin packing,QoS performance,Resource allocation and optimization},
number = {1},
pages = {101--137},
title = {{Automated QoS-oriented cloud resource optimization using containers}},
volume = {24},
year = {2017}
}
@techreport{Mell:2011jj,
abstract = {The National Institute of Standards and Technology (NIST) developed this document in furtherance of its statutory responsibilities under the Federal Information Security Management Act (FISMA) of 2002, Public Law 107-347. NIST is responsible for developing standards and guidelines, including minimum requirements, for providing adequate information security for all agency operations and assets; but such standards and guidelines shall not apply to national security systems. This guideline is consistent with the requirements of the Office of Management and Budget (OMB) Circular A-130, Section 8b(3), Securing Agency Information Systems, as analyzed in A-130, Appendix IV: Analysis of Key Sections. Supplemental information is provided in A-130, Appendix III. This guideline has been prepared for use by Federal agencies. It may be used by nongovernmental organizations on a voluntary basis and is not subject to copyright, though attribution is desired. Nothing in this document should be taken to contradict standards and guidelines made mandatory and binding on Federal agencies by the Secretary of Commerce under statutory authority, nor should these guidelines be interpreted as altering or superseding the existing authorities of the Secretary of Commerce, Director of the OMB, or any other Federal official.},
address = {Gaithersburg, MD},
archivePrefix = {arXiv},
arxivId = {2305-0543},
author = {Mell, P M and Grance, T},
doi = {10.6028/NIST.SP.800-145},
eprint = {2305-0543},
institution = {National Institute of Standards and Technology, Gaithersburg, MD},
isbn = {1047-6210},
issn = {00845612},
pmid = {21450758},
title = {{The NIST definition of cloud computing}},
url = {http://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-145.pdf},
year = {2011}
}
@article{Rosen:2013wt,
author = {Rosen, Rami},
journal = {Haifux},
number = {May},
pages = {1--120},
title = {{Resource management : Linux kernel Namespaces and cgroups}},
url = {papers3://publication/uuid/9F7054CD-3FF1-4B4D-A7AB-A378243E8FA0},
year = {2013}
}
@article{evo_str,
author = {{Lee, K. Y. and Yang}, F. F.},
journal = {IEEE Transactions on Power Systems},
number = {1},
pages = {101--108},
title = {{Optimal reactive power planning using evolutionary algorithms: A comparative study for evolutionary programming, evolutionary strategy, genetic algorithm and linear programming}},
volume = {13},
year = {1998}
}
@article{Wold:1987jq,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Analysis, Biosystems Data},
doi = {10.1007/SpringerReference_205537},
eprint = {arXiv:1011.1669v3},
isbn = {0387954422},
issn = {00401706},
journal = {Analysis},
month = {Aug},
number = {1-3},
pages = {1--31},
pmid = {21435900},
title = {{Principal Component Analysis}},
volume = {2},
year = {2004}
}
@article{Gupta:2008ul,
abstract = {The problem of server sprawl is common in data centers of most business organizations. It is most often the case that an application is run on dedicated servers. This leads to situations where organizations end up having numerous servers that remain under-utilized most of the times. The servers, in such scenarios, are allocated more resources (disk, cpu and memory) than are justified by their present workloads. Consolidating multiple underutilized servers into a fewer number of non-dedicated servers that can host multiple applications is an effective tool for businesses to enhance their returns on investment. The problem can be modeled as a variant of the bin packing problem where items to be packed are the servers being consolidated and bins are the target servers. The sizes of the servers/items being packed are resource utilizations which are obtained from the performance trace data. Here we describe a novel two stage heuristic algorithm for taking care of the "bin-item" incompatibility constraints that are inherent in any server consolidation problem. The model is able to solve extremely large instances of problem in a reasonable amount of time.},
author = {Gupta, Rohit and Bose, Sumit Kumar and Sundarrajan, Srikanth and Chebiyam, Manogna and Chakrabarti, Anirban},
doi = {10.1109/SCC.2008.39},
isbn = {9780769532837},
journal = {Proceedings - 2008 IEEE International Conference on Services Computing, SCC 2008},
pages = {39--46},
title = {{A two stage heuristic algorithm for solving the server consolidation problem with item-item and bin-item incompatibility constraints}},
volume = {2},
year = {2008}
}
@article{Piraghaj:2015dv,
abstract = {One of the major challenges that cloud providers face is minimizing power consumption of their data centers. To this point, majority of current research focuses on energy efficient management of resources in the Infrastructure as a Service model and through virtual machine consolidation. However, containers are increasingly gaining popularity and going to be major deployment model in cloud environment and specifically in Platform as a Service. This paper focuses on improving the energy efficiency of servers for this new deployment model by proposing a framework that consolidates containers on virtual machines. We first formally present the container consolidation problem and then we compare a number of algorithms and evaluate their performance against metrics such as energy consumption, Service Level Agreement violations, average container migrations rate, and average number of created virtual machines. Our proposed framework and algorithms can be utilized in a private cloud to minimize energy consumption, or alternatively in a public cloud to minimize the total number of hours the virtual machines leased.},
author = {Piraghaj, Sareh Fotuhi and Dastjerdi, Amir Vahid and Calheiros, Rodrigo N. and Buyya, Rajkumar},
doi = {10.1109/DSDIS.2015.67},
isbn = {9781509002146},
journal = {Proceedings - 2015 IEEE International Conference on Data Science and Data Intensive Systems; 8th IEEE International Conference Cyber, Physical and Social Computing; 11th IEEE International Conference on Green Computing and Communications and 8th IEEE International Conference on Internet of Things, DSDIS/CPSCom/GreenCom/iThings 2015},
keywords = {Cloud Computing,Container as a Service,Energy Efficiency,Virtualization},
pages = {368--375},
title = {{A Framework and Algorithm for Energy Efficient Container Consolidation in Cloud Data Centers}},
year = {2015}
}
@article{Hyser:2007ti,
abstract = {We present a high level overview of a virtual machine placement system in which an autonomic controller dynamically manages the mapping of virtual machines onto physical hosts in accordance with policies specified by the user. By closely monitoring virtual machine activity and employing advanced policies for dynamic workload placement, such an autonomic solution can achieve substantial cost savings from better utilization of computing resources and less frequent overload situations.},
author = {Hyser, Chris and Mckee, Bret and Gardner, Rob and Watson, Bj},
journal = {{\ldots} Packard Laboratories, Tech. {\ldots}},
keywords = {Architecture,Automation,Manageability,Virtualization,data center,placemen,virtual machine},
number = {October},
title = {{Autonomic virtual machine placement in the data center}},
url = {https://www.hpl.hp.com/techreports/2007/HPL-2007-189.pdf},
year = {2007}
}
@article{Gmach:2012en,
abstract = {Cloud computing has emerged as a new and alternative approach for providing computing services. Customers acquire and release resources by requesting and returning virtual machines to the cloud. Different service models and pricing schemes are offered by cloud service providers. This can make it difficult for customers to compare cloud services and select an appropriate solution. Cloud Infrastructure-as-a-Service vendors offer a t-shirt approach for Virtual Machines (VMs) on demand. Customers can select from a set of fixed size VMs and vary the number of VMs as their demands change. Private clouds often offer another alternative, called time-sharing, where the capacity of each VM is permitted to change dynamically. With this approach each virtual machine is allocated a dynamic amount of CPU and memory resources over time to better utilize available resources. We present a tool that can help customers make informed decisions about which approach works most efficiently for their workloads in aggregate and for each workload separately. A case study using data from an enterprise customer with 312 workloads demonstrates the use of the tool. It shows that for the given set of workloads the t-shirt model requires almost twice the number of physical servers as the time share model. The costs for such infrastructure must ultimately be passed on to the customer in terms of monetary costs or performance risks. We conclude that private and public clouds should consider offering both resource sharing models to meet the needs of customers. {\textcopyright} 2012 IEEE.},
author = {Gmach, Daniel and Rolia, Jerry and Cherkasova, Ludmila},
doi = {10.1109/CCGrid.2012.68},
isbn = {9780769546919},
journal = {Proceedings - 12th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing, CCGrid 2012},
keywords = {cloud service models,data center efficiency,resource sharing,virtual machine sizing,virtualization},
pages = {539--546},
title = {{Selling t-shirts and time shares in the cloud}},
year = {2012}
}
@article{Falkenauer:1996hv,
abstract = {The grouping genetic algorithm (GGA) is a genetic algorithm heavily modified to suit the structure of grouping problems. Those are the problems where the aim is to find a good partition of a set or to group together the members of the set. The bin packing problem (BPP) is a well known NP-hard grouping problem: items of various sizes have to be grouped inside bins of fixed capacity. On the other hand, the reduction method of Martello and Toth, based on their dominance criterion, constitutes one of the best OR techniques for optimization of the BPP to date. In this article, we first describe the GGA paradigm as compared to the classic Holland-style GA and the ordering GA. We then show how the bin packing GGA can be enhanced with a local optimization inspired by the dominance criterion. An extensive experimental comparison shows that the combination yields an algorithm superior to either of its components.},
author = {Falkenauer, Emanuel},
doi = {10.1007/BF00226291},
issn = {1381-1231},
journal = {Journal of Heuristics},
number = {1},
pages = {5--30},
title = {{A hybrid grouping genetic algorithm for bin packing}},
url = {http://link.springer.com/10.1007/BF00226291},
volume = {2},
year = {1996}
}
@article{Fehling:2014tl,
author = {Fehling, Christoph and Leymann, Frank and Retter, Ralph and Schupeck, Walter and Arbitter, Peter},
title = {{Cloud Computing Patterns: Fundamentals to Design, Build, and Manage Could Applications}},
year = {2014}
}
@article{DeJong:1992vk,
author = {{De Jong}, K.A.},
journal = {parallel Problems Solving},
title = {{Are Genetic Algorithms Functions Optimizers?}},
year = {1992}
}
@article{Rong:2016js,
abstract = {Big data applications have become increasingly popular with the appearance of cloud computing and green computing. Therefore, internet service providers (ISPs) need to build data centers for data storage and data processing under the cloud service pattern. However, data centers often consume a significant amount of energy and lead to pollutant emissions. In recent years, the high energy consumption and environmental pollution of data centers have become a pressing issue. This paper reviews the progress of energy-saving technologies in high-performance computing, energy conservation technologies for computer rooms and renewable energy applications during the construction and operation of data centers. From multiple perspectives of energy consumption, cost reduction, and environment protection, a comprehensive set of strategies are proposed to maximize data centers' efficiency and minimize the environmental impact. This paper also provides energy-saving trends for data centers in the future. {\textcopyright} 2016 Elsevier Ltd. All rights reserved.},
author = {Rong, H.a and Zhang, H.a and Xiao, S.a and Li, C.b and Hu, C.c},
doi = {10.1016/j.rser.2015.12.283},
journal = {Renewable and Sustainable Energy Reviews},
month = {May},
pages = {674--691},
title = {{Optimizing energy consumption for data centers}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84954271719{\&}partnerID=40{\&}md5=c45910c1477c5877579f38bf4f628644},
volume = {58},
year = {2016}
}
@article{Jung:2010dt,
abstract = {Server consolidation based on virtualization is an important technique for improving power efficiency and resource utilization in cloud infrastructures. However, to ensure satisfactory performance on shared resources under changing application workloads, dynamic management of the resource pool via online adaptation is critical. The inherent tradeoffs between power and performance as well as between the cost of an adaptation and its benefits make such management challenging. In this paper, we present Mistral, a holistic controller framework that optimizes power consumption, performance benefits, and the transient costs incurred by various adaptations and the controller itself to maximize overall utility. Mistral can handle multiple distributed applications and large-scale infrastructures through a multi-level adaptation hierarchy and scalable optimization algorithm. We show that our approach outstrips other strategies that address the tradeoff between only two of the objectives (power, performance, and transient costs).},
author = {Jung, Gueyoung and Hiltunen, Matti A. and Joshi, Kaustubh R. and Schlichting, Richard D. and Pu, Calton},
doi = {10.1109/ICDCS.2010.88},
isbn = {9780769540597},
issn = {10636927},
journal = {Proceedings - International Conference on Distributed Computing Systems},
pages = {62--73},
title = {{Mistral: Dynamically managing power, performance, and adaptation cost in cloud infrastructures}},
year = {2010}
}
@booklet{Banzhaf:1998wc,
author = {Banzhaf, W and Nordin, P and Keller, R E and Francone, F D},
title = {{Genetic programming: an introduction}},
year = {1998}
}
@article{KirjnerNeto:1998ef,
author = {Kirjner-Neto, C and Polak, E and {Der Kiureghian}, a},
journal = {J. Optim. Theory Appl.},
keywords = {outer approximations,problem reformulation techniques,reliability constraints},
number = {I},
pages = {1--16},
title = {{An outer approximation approach to reliability-based optimal design of structures}},
volume = {98},
year = {1998}
}
@article{Wood:2009cv,
abstract = {Many data center virtualization solutions, such as VMware ESX, employ content-based page sharing to consolidate the resources of multiple servers. Page sharing identifies virtual machine memory pages with identical content and consolidates them into a single shared page. This technique, implemented at the host level, applies only between VMs placed on a given physical host. In a multi- server data center, opportunities for sharing may be lost because the VMs holding identical pages are resident on different hosts. In order to obtain the full benefit of content-based page sharing it is necessary to place virtual machines such that VMs with similar memory content are located on the same hosts. In this paper we present Memory Buddies, a memory sharing- aware placement system for virtual machines. This system includes a memory fingerprinting system to efficiently determine the sharing potential among a set of VMs, and compute more efficient place- ments. In addition it makes use of live migration to optimize VM placement as workloads change. We have implemented a prototype Memory Buddies system with VMware ESX Server and present experimental results on our testbed, as well as an analysis of an extensive memory trace study. Evaluation of our prototype using a mix of enterprise and e-commerce applications demonstrates an increase of data center capacity (i.e. number of VMs supported) of 17{\%}, while imposing low overhead and scaling to as many as a thousand servers.},
author = {Wood, Timothy and Tarasuk-Levin, G},
doi = {10.1145/1508293.1508299},
isbn = {9781605583754},
issn = {0163-5980},
journal = {VEE '09: Proceedings of the 2009 ACM SIGPLAN/SIGOPS international conference on Virtual execution environment},
keywords = {consolidation,page sharing,virtualization},
pages = {31--40},
title = {{Memory buddies: exploiting page sharing for smart colocation in virtualized data centers}},
url = {http://dl.acm.org/citation.cfm?id=1508299},
year = {2009}
}
@inproceedings{Kennedy:1997hd,
abstract = {The particle swarm algorithm adjusts the trajectories of a$\backslash$npopulation of {\&}ldquo;particles{\&}rdquo; through a problem space on the$\backslash$nbasis of information about each particle's previous best performance and$\backslash$nthe best previous performance of its neighbors. Previous versions of the$\backslash$nparticle swarm have operated in continuous space, where trajectories are$\backslash$ndefined as changes in position on some number of dimensions. The paper$\backslash$nreports a reworking of the algorithm to operate on discrete binary$\backslash$nvariables. In the binary version, trajectories are changes in the$\backslash$nprobability that a coordinate will take on a zero or one value.$\backslash$nExamples, applications, and issues are discussed},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Kennedy, J. and Eberhart, R.C.},
booktitle = {1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation},
doi = {10.1109/ICSMC.1997.637339},
eprint = {arXiv:1011.1669v3},
isbn = {0-7803-4053-1},
issn = {1062-922X},
pages = {4--8},
pmid = {20646090},
publisher = {IEEE},
title = {{A discrete binary version of the particle swarm algorithm}},
volume = {5},
year = {1997}
}

@article{Jarboui:2007in,
abstract = {This paper presents a new clustering approach based on the combinatorial particle swarm optimization (CPSO) algorithm. Each particle is represented as a string of length n (where n is the number of data points) the ith element of the string denotes the group number assigned to object i. An integer vector corresponds to a candidate solution to the clustering problem. A swarm of particles are initiated and fly through the solution space for targeting the optimal solution. To verify the efficiency of the proposed CPSO algorithm, comparisons with a genetic algorithm are performed. Computational results show that the proposed CPSO algorithm is very competitive and outperforms the genetic algorithm. ?? 2007 Elsevier Inc. All rights reserved.},
author = {Jarboui, B. and Cheikh, M. and Siarry, P. and Rebai, A.},
doi = {10.1016/j.amc.2007.03.010},
issn = {00963003},
journal = {Applied Mathematics and Computation},
keywords = {Combinatorial particle swarm optimization,Genetic algorithms,Particle swarm optimization,Partitional clustering},
mendeley-groups = {proposal},
number = {2},
pages = {337--345},
title = {{Combinatorial particle swarm optimization (CPSO) for partitional clustering problem}},
volume = {192},
year = {2007}
}
@article{Liao:2007dl,
abstract = {Particle swarm optimization (PSO) is a novel metaheuristic inspired by the flocking behavior of birds. The applications of PSO to scheduling problems are extremely few. In this paper, we present a PSO algorithm, extended from discrete PSO, for flowshop scheduling. In the proposed algorithm, the particle and the velocity are redefined, and an efficient approach is developed to move a particle to the new sequence. To verify the proposed PSO algorithm, comparisons with a continuous PSO algorithm and two genetic algorithms are made. Computational results show that the proposed PSO algorithm is very competitive. Furthermore, we incorporate a local search scheme into the proposed algorithm, called PSO-LS. Computational results show that the local search can be really guided by PSO in our approach. Also, PSO-LS performs well in flowshop scheduling with total flow time criterion, but it requires more computation times.},
author = {Liao, Ching-Jong and Tseng, Chao-Tang and Luarn, Pin},
doi = {10.1016/j.cor.2005.11.017},
isbn = {0305-0548},
issn = {03050548},
journal = {Computers {\&} Operations Research},
keywords = {Metaheuristic,Particle swarm optimization,Scheduling},
mendeley-groups = {proposal},
number = {10},
pages = {3099--3111},
title = {{A discrete version of particle swarm optimization for flowshop scheduling problems}},
url = {www.elsevier.com/locate/cor},
volume = {34},
year = {2007}
}

@article{SoteloFigueroa:2013be,
abstract = {The development of low-level heuristics for solving instances of a problem is relatedto the knowledge of an expert. He needs to analyze several components from the problema(sic)(TM) s instance and to think out an specialized heuristic for solving the instance. However if any inherent component to the instance gets changes, then the designed heuristic may not work as it used to do it. In this paper it is presented a novel approach to generated low-level heuristics; the proposed approach implements micro-Differential Evolution for evolving an indirect representation of the Bin Packing Problem. It was used the Hard28 instance, which is a well-known and referenced Bin Packing Problem instance. The heuristics obtained by the proposed approach were compared against the well-know First-Fit heuristic, the results of packing that were gotten for each heuristic were analized by the statistic non-parametric test known as Wilcoxon Signed-Rank test.},
author = {Sotelo-Figueroa, Marco Aurelio and Soberanes, H{\'{e}}ctor Jos{\'{e}} Puga and Carpio, Juan Mart{\'{i}}n and {Fraire Huacuja}, H{\'{e}}ctor J. and Reyes, Laura Cruz and {Soria Alcaraz}, Jorge Alberto},
doi = {10.1007/978-3-642-33021-6_28},
isbn = {9783642330209},
issn = {1860949X},
journal = {Studies in Computational Intelligence},
number = {Chapter 28},
pages = {349--359},
title = {{Evolving bin packing heuristic using micro-differential evolution with indirect representation}},
volume = {451},
year = {2013}
}
@article{1992gppc.book.....K,
author = {Koza, John},
isbn = {0262111705},
journal = {Complex adaptive systems},
keywords = {book evolution},
title = {{Genetic Programming: On the Programming of Computers by Means of Natural Selection (Complex Adaptive Systems)}},
url = {c:/Daniel/Work/Library/workLibrary.Data/PDF/0882872180/Koza1992.pdf{\%}5Cnhttp://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20{\&}path=ASIN/0262111705},
year = {1992}
}
@article{Malhotra:2011ut,
abstract = {Genetic Algorithm is a search heuristic that mimics the process of evaluation. Genetic Algorithms can be applied to process controllers for their optimization using natural operators. This paper discusses the concept and design procedure of Genetic Algorithm as an optimization tool. Further, this paper explores the well established methodologies of the literature to realize the workability and applicability of genetic algorithms for process control applications. Genetic Algorithms are applied to direct torque control of induction motor drive, speed control of gas turbine, speed control of DC servo motor for the optimization of control parameters in this work. The simulations were carried out in simulink package of MATLAB. The simulation results show better optimization of hybrid genetic algorithm controllers than fuzzy standalone and conventional controllers.},
author = {Malhotra, Rahul and Singh, Narinder and Singh, Yaduvir},
doi = {10.5539/cis.v4n2p39},
issn = {1913-8997},
journal = {Computer and Information Science},
keywords = {dtc induction motor,evolutionary algorithms,genetic algorithms,turbine compressor system},
number = {2},
pages = {39--54},
title = {{Genetic Algorithms : Concepts , Design for Optimization of Process Controllers}},
volume = {4},
year = {2011}
}
@article{Manvi:2014hm,
abstract = {The cloud phenomenon is quickly becoming an important service in Internet computing. Infrastructure as a Service (IaaS) in cloud computing is one of the most significant and fastest growing field. In this service model, cloud providers offer resources to users/machines that include computers as virtual machines, raw (block) storage, firewalls, load balancers, and network devices. One of the most pressing issues in cloud computing for IaaS is the resource management. Resource management problems include allocation, provisioning, requirement mapping, adaptation, discovery, brokering, estimation, and modeling. Resource management for IaaS in cloud computing offers following benefits: scalability, quality of service, optimal utility, reduced overheads, improved throughput, reduced latency, specialized environment, cost effectiveness and simplified interface. This paper focuses on some of the important resource management techniques such as resource provisioning, resource allocation, resource mapping and resource adaptation. It brings out an exhaustive survey of such techniques for IaaS in cloud computing, and also put forth the open challenges for further research. {\textcopyright} 2013 Elsevier Ltd.},
author = {Manvi, S S and {Krishna Shyam}, G},
doi = {10.1016/j.jnca.2013.10.004},
isbn = {1084-8045},
issn = {10958592 (ISSN)},
journal = {Journal of Network and Computer Applications},
keywords = {Cloud computing,Resource management},
number = {1},
pages = {424--440},
title = {{Resource management for Infrastructure as a Service (IaaS) in cloud computing: A survey}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84899069262{\&}partnerID=40{\&}md5=e08b8171e7404e8edb2fd1e115e9b25e},
volume = {41},
year = {2014}
}
@article{Anselmi:2008ik,
abstract = {In this paper, we address the service consolidation problem: given a data-center, a set of servers and a set of multi-tiered services or applications, the problem is to allocate services to the available servers in order to minimize the number of servers to use while avoiding the overloading of system resources and satisfying end-to-end response time constraints. Exploiting queueing networks theory, we describe a number of linear and non-linear combinatorial optimization problems related to the server consolidation problem. Since their solution is difficult to obtain through standard solution techniques, we propose accurate heuristics which quickly compute a sub-optimal solution and let us deal with hundreds of servers and applications. Experimental results illustrate the impact of the consolidation in data-centers and show that the heuristic solution is almost very close to the optimum.},
author = {Anselmi, Jonatha and Amaldi, Edoardo and Cremonesi, Paolo},
doi = {10.1109/SEAA.2008.31},
isbn = {9780769532769},
issn = {1089-6503},
journal = {EUROMICRO 2008 - Proceedings of the 34th EUROMICRO Conference on Software Engineering and Advanced Applications, SEAA 2008},
pages = {345--352},
title = {{Service consolidation with end-to-end response time constraints}},
year = {2008}
}
@article{Vogels:2008bg,
abstract = {Virtualization's main application in the enterprise is still server consolidation. As effective as that is, we are likely to see a very different picture a number of years from now, where virtualization will be the key enabling technology for a series of strategic changes in IT. Adaptive resource management using utility computing will be essential to success in an economy with increasing uncertainty. Adapting quickly to new customer demands, new business relationships, and cancelled contracts will be a key business enabler in the modern enterprise, regardless of whether the enterprise executes a software-as-a-service strategy or uses the resource in a more traditional manner. Virtualization will change the way we do testing, with QA departments getting access to a greater variety of resources than they ever had before - at a much lower cost to the business. Similarly, companies that were not proficient in handling reliability, fault tolerance, and business continuity will find in virtualization a new tool that will allow them to make significant progress toward these goals without rewriting all of their software. {\textcopyright} 2008 ACM.},
author = {Vogels, Werner},
doi = {10.1145/1348583.1348590},
issn = {15427730},
journal = {Queue},
number = {1},
pages = {20},
title = {{Beyond server consolidation}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-70350220242{\&}partnerID=tZOtx3y1},
volume = {6},
year = {2008}
}
@article{Chen:2013ft,
author = {Chen, Jianhai and Chiew, Kevin and Ye, Deshi and Zhu, Liangwei and Chen, Wenzhi},
doi = {10.1109/AINA.2013.22},
isbn = {9780769549538},
issn = {1550445X},
journal = {Proceedings - International Conference on Advanced Information Networking and Applications, AINA},
keywords = {Affinity grouping,Cloud computing,Resource allocation,VM packing,Virtual cluster,Virtualization},
pages = {235--242},
title = {{AAGA: Affinity-Aware grouping for allocation of virtual machines}},
year = {2013}
}
@article{Gerofi:2013bd,
abstract = {Checkpoint-recovery based Virtual Machine (VM) replication is an emerging approach towards accommodating VM installations with high availability. However, it comes with the price of significant performance degradation of the application executed in the VM due to the large amount of state that needs to be synchronized between the primary and the backup machines. It is therefore critical to find new ways for attaining good performance, and at the same time, maintaining fault tolerant execution. In this paper, we present a novel approach to improve the performance of services deployed over replicated virtual machines by exploiting data similarity within the VM's memory image to reduce the network traffic during synchronization. For identifying similar memory areas, we propose a bit density based hash function, upon which, we build a content addressable hash table. We present a quantitative analysis on the degree of similarity we found in various workloads, and introduce a lightweight compression method, which, compared to existing replication techniques, reduces network traffic by up to 80{\%} and yields a performance improvement over 90{\%} for certain latency sensitive applications.},
author = {Gerofi, Balazs and Vass, Zoltan and Ishikawa, Yutaka},
doi = {10.1016/j.future.2012.06.008},
isbn = {9780769545929},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Checkpoint,Fault tolerance,Hypervisor,Recovery,Virtualization},
number = {4},
pages = {1085--1095},
title = {{Utilizing memory content similarity for improving the performance of highly available virtual machines}},
volume = {29},
year = {2013}
}
@article{Chebiyyam:2009uq,
author = {Chebiyyam, M and Malviya, Rashi and Bose, SK and Sundarrajan, Srikanth},
journal = {Perform or Perish},
number = {1},
pages = {65--75},
title = {{Server consolidation: Leveraging the benefits of virtualization}},
url = {http://www.infosys.com/infosys-labs/publications/Documents/SETLabs-briefings-performance-engineering.pdf{\#}page=67},
volume = {7},
year = {2009}
}
@article{Holt:2004fs,
abstract = {{\textless}b{\textgreater}...{\textless}/b{\textgreater} Please update your bookmarks. {\textless}b{\textgreater}Author's{\textless}/b{\textgreater} {\textless}b{\textgreater}retrospective{\textless}/b{\textgreater}$\backslash$non '{\textless}b{\textgreater}Forecasting{\textless}/b{\textgreater} {\textless}b{\textgreater}seasonals{\textless}/b{\textgreater} and {\textless}b{\textgreater}trends{\textless}/b{\textgreater} by {\textless}b{\textgreater}exponentially{\textless}/b{\textgreater}$\backslash$n{\textless}b{\textgreater}weighted{\textless}/b{\textgreater} {\textless}b{\textgreater}moving{\textless}/b{\textgreater} {\textless}b{\textgreater}averages{\textless}/b{\textgreater}'. Charles C. {\textless}b{\textgreater}Holt{\textless}/b{\textgreater}.$\backslash$nInternational Journal of {\textless}b{\textgreater}Forecasting{\textless}/b{\textgreater}, 2004, vol. 20, issue$\backslash$n1, pages 11-13. Date: 2004. {\textless}b{\textgreater}...{\textless}/b{\textgreater}},
author = {Holt, C C},
journal = {International Journal of Forecasting},
month = {Jan},
number = {1},
pages = {11--13},
title = {{Author's retrospective on'Forecasting seasonals and trends by exponentially weighted moving averages'}},
volume = {20},
year = {2004}
}
@article{Jat:2011us,
abstract = {The university course timetabling problem is a combinatorial optimisation problem in which a set of events has to be scheduled in time slots and located in suitable rooms. The design of course timetables for academic institutions is a very difﬁcult task because it is an NP-hard problem. This paper proposes a genetic algorithm with a guided search strategy and a local search technique for the university course timetabling problem. The guided search strategy is used to create offspring into the population based on a data structure that stores information extracted from previous good individuals. The local search technique is used to improve the quality of individuals. The proposed genetic algorithm is tested on a set of benchmark problems in comparison with a set of state-of-the-art methods from the literature. The experimental results show that the proposed genetic algorithm is able to produce promising results for the university course timetabling problem.},
author = {Naseem, Sadaf and Shengxiang, Jat},
journal = {Multidisciplinary International Conference on Scheduling : Theory and Applications (MISTA 2009)},
number = {August},
pages = {10--12},
title = {{A Guided Search Genetic Algorithm for the University Course Timetabling Problem}},
url = {http://www.schedulingconference.org/previous/publications/download/index.php?key=2009-180-191-P{\&}filename=mista.bib},
volume = {6622},
year = {2009}
}
@article{Tan:2017fa,
author = {Tan, Boxiong and Ma, Hui and Mei, Yi},
doi = {10.1109/CEC.2017.7969618},
isbn = {9781509046010},
journal = {CEC},
pages = {2574--2581},
title = {{A NSGA-II-based Approach for Service Resource Allocation in Cloud}},
year = {2017}
}

@article{Frey:2013gp,
author = {Frey, S and Fittkau, F and Hasselbring, W},
isbn = {9781467330763},
journal = {ICSE},
pages = {512--521},
title = {{Search-based genetic optimization for deployment and reconfiguration of software in the cloud. International Conference on Software Engineering (ICSE-13). San Francisco, CA, USA,18--26 May 2013}},
year = {2013}
}

@article{Piraghaj:2016bw,
author = {Piraghaj, Sareh Fotuhi and Calheiros, Rodrigo N. and Chan, Jeffrey and Dastjerdi, Amir Vahid and Buyya, Rajkumar},
doi = {10.1093/comjnl/bxv106},
issn = {0010-4620},
journal = {The Computer Journal},
keywords = {cloud computing,energy efficiency,virtualization,workload characterization},
number = {2},
pages = {208--224},
title = {{Virtual machine customization and task mapping model for efficient allocation of cloud data center resources}},
volume = {59},
year = {2016}
}
@article{Chaisiri:2012cva,
author = {{D. Sreenivasan} and {P. Gayathri} and {R. Anitha} and {P. Dhivya}},
journal = {IEEE transactions on service computing},
number = {2},
pages = {14--16},
title = {{Optimization of resource provisioning in cloud}},
volume = {5},
year = {2012}
}
@article{Verma:2009wi,
abstract = {Server consolidation has emerged as a promising technique to reduce the energy costs of a data center. In this work, we present the first detailed analysis of an enterprise server workload from the perspective of finding characteristics for consolidation. We observe significant potential for power savings if consolidation is performed using off-peak values for application demand. However, these savings come up with associated risks due to consolidation, particularly when the correlation between applications is not considered. We also investigate the stability in utilization trends for low-risk consolidation. Using the insights from the workload analysis, two new consolidation methods are designed that achieve significant power savings, while containing the performance risk of consolidation. We present an implementation of the methodologies in a consolidation planning tool and provide a comprehensive evaluation study of the proposed methodologies.},
author = {Verma, Akshat and Dasgupta, Gargi and Nayak, T K and De, P and Kothari, R},
doi = {10.1139/F07-116},
isbn = {0706-652X$\backslash$r1205-7533},
issn = {0706-652X},
journal = {Proceedings of the 2009 conference on USENIX Annual technical conference},
pages = {28},
title = {{Server Workload Analysis for Power Minimization using Consolidation}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.148.6793},
volume = {1505},
year = {2009}
}
@article{Energy_6,
author = {Fanara, Andrew},
title = {{Report to Congress on Server and Data Center Energy Efficiency Public Law 109-431}},
year = {2007}
}
@book{Anonymous:EjRdlW3q,
address = {Boston, MA},
publisher = {Springer, Boston, MA},
title = {{Multilevel Optimization: Algorithms and Applications}}
}
@article{Zio:2011iq,
abstract = {In many multiobjective optimization problems, the Pareto Fronts and Sets contain a large number of solutions and this makes it difficult for the decision maker to identify the preferred ones. A possible way to alleviate this difficulty is to present to the decision maker a subset of a small number of solutions representatives of the Pareto Front characteristics. In this paper, a two-steps procedure is presented, aimed at identifying a limited number of representative solutions to be presented to the decision maker. Pareto Front solutions are first clustered into "families", which are then synthetically represented by a "head-of-the-family" solution. Level Diagrams are then used to represent, analyse and interpret the Pareto Front reduced to its head-of-the-family solutions. The procedure is applied to a reliability allocation case study of literature, in decision-making contexts both without or with explicit preferences by the decision maker on the objectives to be optimized. {\textcopyright} 2010 Elsevier B.V. All rights reserved.},
author = {Zio, E. and Bazzo, R.},
doi = {10.1016/j.ejor.2010.10.021},
isbn = {0377-2217},
issn = {03772217},
journal = {European Journal of Operational Research},
keywords = {Fuzzy preference assignment,Genetic algorithms,Level Diagrams,Multiobjective optimization,Redundancy allocation,Subtractive clustering},
number = {3},
pages = {624--634},
title = {{A clustering procedure for reducing the number of representative solutions in the Pareto Front of multiobjective optimization problems}},
volume = {210},
year = {2011}
}
@article{Crovella:1997et,
author = {Crovella, Mark E and Bestavros, Azer},
journal = {ACM SIGMETRICS Performance Evaluation Review},
number = {1},
pages = {160--169},
title = {{Self-similarity in World Wide Web traffic: Evidence and possible causes}},
volume = {24},
year = {1996}
}
@article{Cho:2016kz,
abstract = {Data centers are approximately 50 times more energy-intensive than general buildings. The rapidly increasing energy demand for data center operation has motivated efforts to better understand data center electricity use and to identify strategies that reduce the environmental impact. This research is presented analytical approach to the energy efficiency optimization of high density data center, in a synergy with relevant performance analysis of corresponding case study. This paper builds on data center energy modeling efforts by characterizing climate and cooling system differences among data centers and then evaluating their consequences for building energy use. Representative climate conditions for four regions are applied to data center energy models for several different prototypical cooling types. This includes cooling system, supplemental cooling solutions, design conditions and controlling the environment of ICT equipment were generally used for each climate zone, how these affect energy efficiency, and how the prioritization of system selection is derived. Based on the climate classification and the required operating environmental conditions for data centers suggested by the ASHRAE TC 9.9, a dedicated data center energy evaluation tool was taken to examine the potential energy savings of the cooling technology. Incorporating economizer use into the cooling systems would increase the variation in energy efficiency among geographic regions, indicating that as data centers become more energy efficient, their locations will have an increasing effect on overall energy demand. The proposal for the most energy-optimized data center is given by each climate zone.},
author = {Cho, Jinkyun and Kim, Yundeok},
doi = {10.1016/j.apenergy.2015.12.099},
issn = {03062619},
journal = {Applied Energy},
keywords = {Climate zone,Data center,Dedicated cooling system,Economizer,Energy simulation,Power usage effectiveness (PUE)},
month = {Mar},
pages = {967--982},
title = {{Improving energy efficiency of dedicated cooling system and its contribution towards meeting an energy-optimized data center}},
volume = {165},
year = {2016}
}
@article{Sundararajan:2015tn,
abstract = {—In Infrastructure-as-a-Service cloud data centers, services are provided to cloud customers in the form of virtual machines. Cloud customers can place restrictions on these ser-vices by specifying affinity and anti-affinity constraints. Load imbalance is one key issue that cloud data centers regularly face when running these services. Load imbalance arises when existing services are stopped either by the cloud customers or in the event of host power cycling. One way to achieve load balance in such situations is to perform load rebalancing. Load rebalancing is a process of migrating services among hosts to ensure uniform resource distribution. By doing load rebalanc-ing, SLA violations due to resource shortages on over-utilized hosts can be mitigated. The benefits of load rebalancing come at the expense of migration cost. The presence of affinity and anti-affinity constraints make the load rebalancing challenging. In this paper, we focus on load rebalancing of services with affinity and anti-affinity constraints by applying a novel genetic algorithm. Our objective function aims at reducing the number of migrations and variation of available resources in the hosts. Experimental results show that our algorithm achieves a good resource balance while being computationally efficient.},
author = {Sundararajan, Priya Krishnan and Feller, Eugen and Forgeat, Julien and Mengshoel, Ole J},
doi = {10.1109/CLOUD.2015.92},
isbn = {978-1-4673-7287-9},
journal = {CLOUD},
keywords = {-cloud computing,ge-,load rebalancing,netic algorithm,resource management,virtualization},
number = {1975},
title = {{A Constrained Genetic Algorithm for Rebalancing of Services in Cloud Data Centers}},
year = {2015}
}
@article{Canon:2010go,
abstract = {A schedule is said to be robust if it is able to absorb some degree of uncertainty in task or communication durations while maintaining a stable solution. This intuitive notion of robustness has led to a lot of different metrics and almost no heuristics. In this paper, we perform an experimental study of these different metrics and show how they are correlated to each other. Additionally, we propose different strategies for minimizing the makespan while maximizing the robustness: from an evolutionary metaheuristic (best solutions but longer computation time) to more simple heuristics making approximations (medium quality solutions but fast computation time). We compare these different approaches experimentally and show that we are able to find different approximations of the Pareto front for this bicriteria problem.},
author = {Canon, Louis Claude and Jeannot, Emmanuel},
doi = {10.1109/TPDS.2009.84},
isbn = {0123456789},
issn = {10459219},
journal = {IEEE Transactions on Parallel and Distributed Systems},
keywords = {DAG,Makespan,Robustness,Stochastic scheduling},
number = {4},
pages = {532--546},
title = {{Evaluation and optimization of the robustness of dag schedules in heterogeneous environments}},
volume = {21},
year = {2010}
}
@article{Khanna:2006vq,
abstract = {As businesses have grown, so has the need to deploy I/T applications rapidly to support the expanding business processes. Often, this growth was achieved in an unplanned way: each time a new application was needed a new server along with the application software was deployed and new storage elements were purchased. In many cases this has led to what is often referred to as "server sprawl", resulting in low server utilization and high system management costs. An architectural approach that is becoming increasingly popular to address this problem is known as server virtualization. In this paper we introduce the concept of server consolidation using virtualization and point out associated issues that arise in the area of application performance. We show how some of these problems can be solved by monitoring key performance metrics and using the data to trigger migration of virtual machines within physical servers. The algorithms we present attempt to minimize the cost of migration and maintain acceptable application performance levels},
author = {Khanna, G. and Beaty, K. and Kar, G. and Kochut, A.},
doi = {10.1109/NOMS.2006.1687567},
isbn = {1-4244-0142-9},
issn = {15421201},
journal = {2006 IEEE/IFIP Network Operations and Management Symposium NOMS 2006},
pages = {373--381},
title = {{Application Performance Management in Virtualized Server Environments}},
url = {http://ieeexplore.ieee.org/document/1687567/},
year = {2006}
}
@article{Farahnakian:2016gt,
author = {Farahnakian, Fahimeh and Pahikkala, Tapio and Liljeberg, Pasi and Plosila, Juha and Hieu, Nguyen Trung and Tenhunen, Hannu},
doi = {10.1109/TCC.2016.2617374},
issn = {2168-7161},
journal = {IEEE Transactions on Cloud Computing},
number = {99},
pages = {1--1},
title = {{Energy-aware VM Consolidation in Cloud Data Centers Using Utilization Prediction Model}},
url = {http://ieeexplore.ieee.org/document/7593250/},
volume = {PP},
year = {2016}
}
@article{Bianco:2009ej,
abstract = {In this work we consider the following hazmat transportation network design problem. A given set of hazmat shipments has to be shipped over a road transportation network in order to transport a given amount of hazardous materials from specific origin points to specific destination points, and we assume there are regional and local government authorities that want to regulate the hazmat transportations by imposing restrictions on the amount of hazmat traffic over the network links. In particular, the regional authority aims to minimize the total transport risk induced over the entire region in which the transportation network is embedded, while local authorities want the risk over their local jurisdictions to be the lowest possible, forcing the regional authority to assure also risk equity. We provide a linear bilevel programming formulation for this hazmat transportation network design problem that takes into account both total risk minimization and risk equity. We transform the bilevel model into a single-level mixed integer linear program by replacing the second level (follower) problem by its KKT conditions and by linearizing the complementary constraints, and then we solve the MIP problem with a commercial optimization solver. The optimal solution may not be stable, and we provide an approach for testing its stability and for evaluating the range of its solution values when it is not stable. Moreover, since the bilevel model is difficult to be solved optimally and its optimal solution may not be stable, we provide a heuristic algorithm for the bilevel model able to always find a stable solution. The proposed bilevel model and heuristic algorithm are experimented on real scenarios of an Italian regional network. {\textcopyright} 2008 Elsevier Ltd. All rights reserved.},
author = {Bianco, Lucio and Caramia, Massimiliano and Giordani, Stefano},
doi = {10.1016/j.trc.2008.10.001},
isbn = {0968-090X},
issn = {0968090X},
journal = {Transportation Research Part C: Emerging Technologies},
keywords = {Bilevel optimization,Hazardous materials,Hazmat transportation network design,Heuristic algorithm},
month = {Apr},
number = {2},
pages = {175--196},
title = {{A bilevel flow model for hazmat transportation network design}},
volume = {17},
year = {2009}
}
@article{Shen:2015hm,
abstract = {Business-critical workloads -- web servers, mail servers, app servers, etc. -- are increasingly hosted in virtualized data enters acting as Infrastructure-as-a-Service clouds (cloud data enters). Understanding how business-critical workloads demand and use resources is key in capacity sizing, in infrastructure operation and testing, and in application performance management. However, relatively little is currently known about these workloads, because the information is complex -- larges-scale, heterogeneous, shared-clusters -- and because datacenter operators remain reluctant to share such information. Moreover, the few operators that have shared data (e.g., Google and several supercomputing centers) have enabled studies in business intelligence (MapReduce), search, and scientific computing (HPC), but not in business-critical workloads. To alleviate this situation, in this work we conduct a comprehensive study of business-critical workloads hosted in cloud data enters. We collect two large-scale and long-term workload traces corresponding to requested and actually used resources in a distributed datacenter servicing business-critical workloads. We perform an in-depth analysis about workload traces. Our study sheds light into the workload of cloud data enters hosting business-critical workloads. The results of this work can be used as a basis to develop efficient resource management mechanisms for data enters. Moreover, the traces we released in this work can be used for workload verification, modelling and for evaluating resource scheduling policies, etc.},
archivePrefix = {arXiv},
arxivId = {arXiv:1302.5679v1},
author = {Shen, Siqi and {Van Beek}, Vincent and Iosup, Alexandru},
doi = {10.1109/CCGrid.2015.60},
eprint = {arXiv:1302.5679v1},
isbn = {9781479980062},
issn = {03029743},
journal = {Proceedings - 2015 IEEE/ACM 15th International Symposium on Cluster, Cloud, and Grid Computing, CCGrid 2015},
keywords = {Characterization,Datacenters,Workload},
pages = {465--474},
pmid = {23335858},
title = {{Statistical characterization of business-critical workloads hosted in cloud datacenters}},
year = {2015}
}
@inproceedings{Angelo:2013ee,
abstract = {Bilevel programming problems are characterized by two optimization problems which are hierarchically related, where to each feasible upper level solution an optimal solution in the lower level problem must be associated. These problems appear in many practical applications, and a variety of solution techniques can be found in the literature. In this paper, an algorithm is proposed which uses differential evolution to solve both the upper and lower level optimization problems. Several test problems from the literature are solved in order to assess the performance of the proposed method.},
author = {Angelo, J S and Krempser, E and Barbosa, H J C},
booktitle = {Proceedings of IEEE Congress on Evolutionary Computation},
doi = {10.1109/CEC.2013.6557606},
isbn = {9781479904549},
pages = {470--477},
publisher = {IEEE},
title = {{Differential Evolution for Bilevel Programming}},
year = {2013}
}
@article{Colson:2007bu,
abstract = {This paper is devoted to bilevel optimization, a branch of mathematical programming of both practical and theoretical interest. Starting with a simple example, we proceed towards a general formulation. We then present fields of application, focus on solution approaches, and make the connection with MPECs (Mathematical Programs with Equilibrium Constraints).},
author = {Colson, Beno{\^{i}}t and Marcotte, Patrice and Savard, Gilles},
doi = {10.1007/s10479-007-0176-2},
isbn = {0254-5330},
issn = {02545330},
journal = {Annals of Operations Research},
keywords = {Bilevel programming,Mathematical programs with equilibrium constraints,Nonlinear programming,Optimal pricing},
number = {1},
pages = {235--256},
title = {{An overview of bilevel optimization}},
volume = {153},
year = {2007}
}
@article{Chaisiri:2015bf,
abstract = {{\textcopyright} 2015 IEEE.Security-as-a-Service (SECaaS), pay-per-use cloud-based services that provides information security measures via the cloud, are increasingly used by corporations to maintain their systems' security posture. Customers often have to provision these SECaaS services based on the potential subscription costs incurred. However, these security services are unable to deal with all possible types of threats. A single threat (e.g. malicious insiders) can result in the loss of valuable data and revenue. Hence, it is also common to see corporations (i.e. cloud customers) manage their risks by purchasing cyber insurance to cover costs and liabilities due to unforeseen losses. A balance between service allocation cost and insurance is often required but not well studied. In this paper, we propose an optimized SECaaS provisioning framework that enables customers to optimally allocate security services from SECaaS providers to their applications, while managing risks from information security breaches via purchasing cyber insurance policies. Finding the right balance is a great challenge, and the solutions of the security service allocation and insurance management are obtained through solving an optimization model derived from stochastic programming with a three-stage recourse. Simulations were conducted to evaluate this optimization model. We exposed our model to several uncertain information parameters and the results are promising - demonstrating an effective approach to balance customers' security requirements while keeping service subscription and insurance policy costs low.},
author = {Chaisiri, Sivadon and Ko, Ryan K.L. and Niyato, Dusit},
doi = {10.1109/Trustcom.2015.403},
isbn = {9781467379519},
journal = {Proceedings - 14th IEEE International Conference on Trust, Security and Privacy in Computing and Communications, TrustCom 2015},
keywords = {Cloud security,Cloud security economics,Cyber insurance,Optimization,Resource allocation,Stochastic programming},
pages = {426--433},
title = {{A joint optimization approach to security-as-a-service allocation and cyber insurance management}},
volume = {1},
year = {2015}
}
@article{Xiong:2014jq,
abstract = {Presently, massive energy consumption in cloud data center tends to be an escalating threat to the environment. To reduce energy consumption in cloud data center, an energy efficient virtual machine allocation algorithm is proposed in this paper based on a proposed energy efficient multiresource allocation model and the particle swarm optimization (PSO) method. In this algorithm, the fitness function of PSO is defined as the total Euclidean distance to determine the optimal point between resource utilization and energy consumption. This algorithmcan avoid falling into local optima which is common in traditional heuristic algorithms. Compared to traditional heuristic algorithms MBFD andMBFH, our algorithm shows significantly energy savings in cloud data center and also makes the utilization of systemresources reasonable at the same time},
author = {Xiong, An-ping and Xu, Chun-xiang},
doi = {10.1155/2014/816518},
issn = {1024-123X},
journal = {Mathematical Problems in Engineering},
number = {6},
pages = {1--8},
title = {{Energy Efficient Multiresource Allocation of Virtual Machine Based on PSO in Cloud Data Center}},
url = {http://www.hindawi.com/journals/mpe/2014/816518/},
volume = {2014},
year = {2014}
}
@article{Zhang:2016cx,
abstract = {Efficient resource provision which can guarantee the satisfactory cloud computing services to the end user, lays the foundation for the success of commercial competition. In the Iaas model, services are deployed as virtual machines in the cloud computing infrastructure. So the resource provision problem is reduced to how to place the virtual machines to support the requested services. According to the deployment sequence, from the view point of top-to-down, we hackle the different deployment phases and the pursued objectives in each phase. More than 150 articles in the latest years are surveyed and the state art of the algorithms to realize these objectives is viewed. Techniques employed in these algorithms are categorized and analyzed systematically. Especially, the deficiency of the traditional research formulation to address the innovation of cloud computing is explored. At last, several tightly related topics, i.e., virtual machine migration, forecast methods, stability and availability are discussed.},
author = {Zhang, Jiangtao and Huang, Hejiao and Wang, Xuan},
doi = {10.1016/j.jnca.2015.12.018},
issn = {10958592},
journal = {Journal of Network and Computer Applications},
keywords = {Cloud computing,Iaas,Resource provision algorithms,Survey},
pages = {23--42},
title = {{Resource provision algorithms in cloud computing: A survey}},
volume = {64},
year = {2016}
}
@article{AlRoomi:2013te,
abstract = {Cloud computing is emerging as a promising field offering a variety of computing services to end users. These services are offered at different prices using various pricing schemes and techniques. End users will favor the service provider offering the best QoS with the lowest price. Therefore, applying a fair pricing model will attract more customers and achieve higher revenues for service providers. This work focuses on comparing many employed and proposed pricing models techniques and highlights the pros and cons of each. The comparison is based on many aspects such as fairness, pricing approach, and utilization period. Such an approach provides a solid ground for designing better models in the future. We have found that most approaches are theoretical and not implemented in the real market, although their simulation results are very promising. Moreover, most of these approaches are biased toward the service provider.},
author = {Al-Roomi, May and Al-Ebrahim, Shaikha and Buqrais, Sabika and Ahmad, Imtiaz},
doi = {10.14257/ijgdc.2013.6.5.09},
issn = {20054262},
journal = {International Journal of Grid and Distributed Computing},
keywords = {charging models,cloud computing,fairness,pricing models,survey},
number = {5},
pages = {93--106},
title = {{Cloud Computing Pricing Models: A Survey}},
url = {http://dx.doi.org/10.14257/ijgdc.2013.6.5.09},
volume = {6},
year = {2013}
}
@article{Ghodsi:2011vm,
abstract = {We consider the problem of fair resource allocation in a system containing different resource types, where each user may have different demands for each resource. To address this problem, we propose Dominant Resource Fairness (DRF), a generalization of max-min fairness to multiple resource types. We show that DRF, unlike other possible policies, satisﬁes several highly desirable properties. First, DRF incentivizes users to share resources, by ensuring that no user is better off if resources are equally partitioned among them. Second, DRF is strategy-proof, as a user cannot increase her allocation by lying about her requirements. Third, DRF is envyfree, as no user would want to trade her allocation with that of another user. Finally, DRF allocations are Pareto efﬁcient, as it is not possible to improve the allocation of a user without decreasing the allocation of another user. We have implemented DRF in the Mesos cluster resource manager, and show that it leads to better throughput and fairness than the slot-based fair sharing schemes in current cluster schedulers.},
author = {Ghodsi, Ali and Zaharia, Matei and Hindman, Benjamin and Konwinski, Andy and Shenker, Scott and Stoica, Ion},
journal = {Ratio},
number = {1},
pages = {24--24},
title = {{Dominant Resource Fairness : Fair Allocation of Multiple Resource Types Maps Reduces}},
url = {http://www.usenix.org/events/nsdi11/tech/full{\_}papers/Ghodsi.pdf},
volume = {167},
year = {2011}
}
@article{Burke:2012gs,
abstract = {The literature shows that one-, two-, and three-dimensional bin packing and knapsack packing are difficult problems in operational research. Many techniques, including exact, heuristic, and metaheuristic approaches, have been investigated to solve these problems and it is often not clear which method to use when presented with a new instance. This paper presents an approach which is motivated by the goal of building computer systems which can design heuristic methods. The overall aim is to explore the possibilities for automating the heuristic design process. We present a genetic programming system to automatically generate a good quality heuristic for each instance. It is not necessary to change the methodology depending on the problem type (one-, two-, or three-dimensional knapsack and bin packing problems), and it therefore has a level of generality unmatched by other systems in the literature. We carry out an extensive suite of experiments and compare with the best human designed heuristics in the literature. Note that our heuristic design methodology uses the same parameters for all the experiments. The contribution of this paper is to present a more general packing methodology than those currently available, and to show that, by using this methodology, it is possible for a computer system to design heuristics which are competitive with the human designed heuristics from the literature. This represents the first packing algorithm in the literature able to claim human competitive results in such a wide variety of packing domains.},
author = {Burke, Edmund K. and Hyde, Matthew R. and Kendall, Graham and Woodward, John},
doi = {10.1162/EVCO_a_00044},
isbn = {1063-6560},
issn = {1063-6560},
journal = {Evolutionary Computation},
month = {Mar},
number = {1},
pages = {63--89},
pmid = {21609273},
title = {{Automating the Packing Heuristic Design Process with Genetic Programming}},
url = {http://www.mitpressjournals.org/doi/10.1162/EVCO{\_}a{\_}00044},
volume = {20},
year = {2012}
}
@article{Jeyarani:2012fg,
abstract = {Cloud computing aims at providing dynamic leasing of server capabilities as scalable, virtualized services to end users. Our work focuses on the Infrastructure as a Service (IaaS) model where custom Virtual Machines (VM) are launched in appropriate servers available in a data center. The cloud data center taken into consideration is heterogeneous and large scale in nature. Such a resource pool is basically characterized by high resource dynamics caused by non-linear variation in the availability of processing elements, memory size, storage capacity, bandwidth and power drawn resulting from the sporadic nature of workload. Apart from the said resource dynamics, our proposed work also considers the processor transitions to various sleep states and their corresponding wake up latencies that are inherent in contemporary enterprise servers. The primary objective of the proposed metascheduler is to map efficiently a set of VM instances onto a set of servers from a highly dynamic resource pool by fulfilling resource requirements of maximum number of workloads. As the cloud data centers are overprovisioned to meet the unexpected workload surges, huge power consumption has become one of the major issues of concern. We have proposed a novel metascheduler called Adaptive Power-Aware Virtual Machine Provisioner (APA-VMP) that schedules the workload in such a way that the total incremental power drawn by the server pool is minimum without compromising the performance objectives. The APA-VMP makes use of swarm intelligence methodology to detect and track the changing optimal target servers for VM placement very efficiently. The scenario was experimented by novel Self-adaptive Particle Swarm Optimization (SAPSO) for VM provisioning, which makes best possible use of the power saving states of idle servers and instantaneous workload on the operational servers. It is evident from the results that there is a significant reduction in the power numbers against the existing strategies. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
author = {Jeyarani, R. and Nagaveni, N. and {Vasanth Ram}, R.},
doi = {10.1016/j.future.2011.06.002},
isbn = {0167-739X},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Cloud computing,Dynamic Voltage Frequency Scaling (DVFS),Dynamic adaptive PSO,Particle Swarm Optimization (PSO),Power conservation,Power saving states,Resource dynamics,VM provisioning},
number = {5},
pages = {811--821},
title = {{Design and implementation of adaptive power-aware virtual machine provisioner (APA-VMP) using swarm intelligence}},
volume = {28},
year = {2012}
}
@article{Herskovits:2000be,
author = {Herskovits, J and Leontiev, A and Dias, G and Santos, G},
journal = {Struc. Multidiscipl. Optim.},
keywords = {bi-,contact problem,interior point algorithm,level programming,shape optimization},
number = {3},
pages = {214--221},
title = {{Contact shape optimization: a bilevel programming approach}},
volume = {20},
year = {2000}
}
@book{Fogel:1962wv,
author = {Fogel, Lawrence Jerome},
booktitle = {Industrial Research},
number = {2},
pages = {14--19},
publisher = {Industrial research},
title = {{Autonomous Automata}},
volume = {4},
year = {1962}
}
@inproceedings{Energy_1,
author = {Computing, Cluster and Microsoft, Aman Kansal},
booktitle = {Proceedings of the 2008 conference on Power aware computing and systems},
number = {November 2008},
organization = {San Diego, California},
pages = {1--5},
title = {{Energy-Aware Consolidation for Cloud Computing Energy Aware Consolidation for Cloud Computing}},
year = {2016}
}
@booklet{Rechenberg:a_aZDNtZ,
abstract = {170 S. mit 36 Abb. Frommann-Holzboog-Verlag. Stuttgart . Broschiert},
author = {Rechenberg, Ingo},
booktitle = {Feddes Repertorium},
doi = {10.1002/fedr.19750860506},
isbn = {978-3772803734},
issn = {00148962},
number = {5},
pages = {337--337},
title = {{Evolutionsstrategie - Optimierung technischer Systeme nach Prinzipien der biologischen Evolution}},
url = {10.1002/fedr.19750860506},
volume = {86},
year = {1971}
}
@article{Esposito:2016br,
abstract = {Microservices can be broadly defined as the design of service-oriented software using a set of small services. In a microservice architecture, application complexity is distributed among narrowly focused and independently deployable units of computation. Such complexity can result in security vulnerabilities. Trustworthiness is also an issue when dealing with microservices. Moreover, there may be gaps in existing legal frameworks with regard to this technology. Solutions to these issues must seek balance between security and performance.},
author = {Esposito, Christian and Castiglione, Aniello and Choo, Kim Kwang Raymond},
doi = {10.1109/MCC.2016.105},
isbn = {2325-6095 VO  - 3},
issn = {23256095},
journal = {IEEE Cloud Computing},
keywords = {Trustworthiness,cloud computing,cloud infrastructure,microservices,security},
number = {5},
pages = {10--14},
title = {{Challenges in Delivering Software in the Cloud as Microservices}},
volume = {3},
year = {2016}
}
@article{Beloglazov:2012ji,
abstract = {Cloud computing offers utility-oriented IT services to users worldwide. Based on a pay-as-you-go model, it enables hosting of pervasive applications from consumer, scientific, and business domains. However, data centers hosting Cloud applications consume huge amounts of electrical energy, contributing to high operational costs and carbon footprints to the environment. Therefore, we need Green Cloud computing solutions that can not only minimize operational costs but also reduce the environmental impact. In this paper, we define an architectural framework and principles for energy-efficient Cloud computing. Based on this architecture, we present our vision, open research challenges, and resource provisioning and allocation algorithms for energy-efficient management of Cloud computing environments. The proposed energy-aware allocation heuristics provision data center resources to client applications in a way that improves energy efficiency of the data center, while delivering the negotiated Quality of Service (QoS). In particular, in this paper we conduct a survey of research in energy-efficient computing and propose: (a) architectural principles for energy-efficient management of Clouds; (b) energy-efficient resource allocation policies and scheduling algorithms considering QoS expectations and power usage characteristics of the devices; and (c) a number of open research challenges, addressing which can bring substantial benefits to both resource providers and consumers. We have validated our approach by conducting a performance evaluation study using the CloudSim toolkit. The results demonstrate that Cloud computing model has immense potential as it offers significant cost savings and demonstrates high potential for the improvement of energy efficiency under dynamic workload scenarios.},
author = {Beloglazov, Anton and Abawajy, Jemal and Buyya, Rajkumar},
doi = {10.1016/j.future.2011.04.017},
isbn = {0167-739X},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {cloud computing,dynamic consolidation,energy efficiency,green,green it,resource management,virtualization},
number = {5},
pages = {755--768},
title = {{Energy-aware resource allocation heuristics for efficient management of data centers for Cloud computing}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167739X11000689{\%}5Cnhttp://www.sciencedirect.com/science/article/pii/S0167739X11000689},
volume = {28},
year = {2011}
}
@article{Xiao:2015ik,
abstract = {Power saving of data centers has become an urgent problem in recent years. For a virtualized data center, optimizing the placement of virtual machines (VMs) dynamically is one of the most effective methods for power savings. Based on a deep study on VMs placement, a solution is proposed and described in this paper to solve the problem of dynamic placement of VMs toward optimization of their energy consumptions. A computational model of energy consumption is proposed and built. A novel algorithm based on evolutionary game theory is also presented, which successfully addresses the challenges faced by dynamic placement of VMs. It is proved that the proposed algorithm can reach the optimal solutions theoretically. Experimental results also demonstrate that, by adjusting VMs placement dynamically, the energy consumption can be reduced correspondingly. In comparison with the existing state of the arts, our proposed method outperforms other five algorithms tested and achieves savings of 30–40{\%} on energy consumption. [ABSTRACT FROM AUTHOR]},
author = {Xiao, Zhijiao and Jiang, Jianmin and Zhu, Yingying and Ming, Zhong and Zhong, Shenghua and Cai, Shubin},
issn = {01641212},
journal = {Journal of Systems {\&} Software},
keywords = {COMPUTER algorithms,DATA libraries,Dynamic VMs placement,ENERGY consumption,Energy consumption,Evolutionary game theory,GAME theory,MATHEMATICAL optimization,VIRTUAL machine systems},
pages = {260--272},
title = {{A solution of dynamic VMs placement problem for energy consumption optimization based on evolutionary game theory.}},
url = {10.1016/j.jss.2014.12.030{\%}5Cnhttp://search.ebscohost.com/login.aspx?direct=true{\&}db=bth{\&}AN=100682526{\&}site=ehost-live},
volume = {101},
year = {2015}
}
@article{Xu:2010vh,
abstract = {Server consolidation using virtualization technology has become increasingly important for improving data center efficiency. It enables one physical server to host multiple independent virtual machines (VMs), and the transparent movement of workloads from one server to another. Fine-grained virtual machine resource allocation and reallocation are possible in order to meet the performance targets of applications running on virtual machines. On the other hand, these capabilities create demands on system management, especially for large-scale data centers. In this paper, a two-level control system is proposed to manage the mappings of workloads to VMs and VMs to physical resources. The focus is on the VM placement problem which is posed as a multi-objective optimization problem of simultaneously minimizing total resource wastage, power consumption and thermal dissipation costs. An improved genetic algorithm with fuzzy multi-objective evaluation is proposed for efficiently searching the large solution space and conveniently combining possibly conflicting objectives. The simulation-based evaluation using power-consumption and thermal-dissipation models based on profiling of a Blade Center, demonstrates the good performance, scalability and robustness of our proposed approach. Compared with four well-known bin-packing algorithms and two single-objective approaches, the solutions obtained from our approach seek good balance among the conflicting objectives while others cannot.},
author = {Xu, Jing and Fortes, Jos{\'{e}} A.B.},
doi = {10.1109/GreenCom-CPSCom.2010.137},
isbn = {9780769543314},
journal = {Proceedings - 2010 IEEE/ACM International Conference on Green Computing and Communications, GreenCom 2010, 2010 IEEE/ACM International Conference on Cyber, Physical and Social Computing, CPSCom 2010},
pages = {179--188},
title = {{Multi-objective virtual machine placement in virtualized data center environments}},
year = {2010}
}
@inproceedings{Xavier:2013fy,
author = {Xavier, Miguel G and Neves, Marcelo Veiga and Rossi, Fabio D and Ferreto, Tiago C and Lange, Tobias and {De Rose}, Cesar A F},
booktitle = {21st Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP)},
pages = {233--240},
publisher = {IEEE},
title = {{Performance evaluation of container-based virtualization for high performance computing environments}},
year = {2013}
}
@article{Speitkamp:2010ck,
abstract = {Today's data centers offer IT services mostly hosted on dedicated physical servers. Server virtualization provides a technical means for server consolidation. Thus, multiple virtual servers can be hosted on a single server. Server consolidation describes the process of combining the workloads of several different servers on a set of target servers. We focus on server consolidation with dozens or hundreds of servers, which can be regularly found in enterprise data centers. Cost saving is among the key drivers for such projects. This paper presents decision models to optimally allocate source servers to physical target servers while considering real-world constraints. Our central model is proven to be an NP-hard problem. Therefore, besides an exact solution method, a heuristic is presented to address large-scale server consolidation projects. In addition, a preprocessing method for server load data is introduced allowing for the consideration of quality-of-service levels. Extensive experiments were conducted based on a large set of server load data from a data center provider focusing on managerial concerns over what types of problems can be solved. Results show that, on average, server savings of 31 percent can be achieved only by taking cycles in the server workload into account.},
author = {Speitkamp, Benjamin and Bichler, Martin},
doi = {10.1109/TSC.2010.25},
isbn = {1939-1374},
issn = {19391374},
journal = {IEEE Transactions on Services Computing},
keywords = {Management of services delivery,data center management services,modeling of resources,optimization of services systems},
number = {4},
pages = {266--278},
title = {{A mathematical programming approach for server consolidation problems in virtualized data centers}},
volume = {3},
year = {2010}
}
@article{Lien:2007it,
abstract = {The power consumption of a streaming-media server can be obtained in real time by using the virtual-instrumentation software described in this paper without using an additional hardware meter. We have built a model to estimate the power consumption from the observation of experimental data that consists of tracking the CPU utilization and detecting the operation parameters of the measured servers. When calculated in real time, the CPU utilization can respond to the dynamic change of the power consumption of the measured servers. The operation parameters represent the hardware configuration of the measured servers. We also propose three methods to obtain these parameters: filled- manually, hardware-revised, and software-revised parameter estimating. We have constructed the virtual-instrumentation software according to this power model to measure the power consumption of the streaming-media server. To facilitate the measurement process, we have also designed a suitable graphic-user interface for it. Our virtual-instrumentation software with three parameter-estimating methods has been tested by way of comparison with measurement results obtained by a power meter. The average power values of the hardware-revised method are found to yield mean errors of the estimate within 3{\%}. The mean error of the software-revised method is within 6{\%}. However, the filled- manually method may underestimate the power consumption by as much as 11{\%}.},
author = {Lien, Chia Hung and Bai, Ying Wen and Lin, Ming Bo},
doi = {10.1109/TIM.2007.904554},
issn = {00189456},
journal = {IEEE Transactions on Instrumentation and Measurement},
keywords = {CPU utilization,Parameter-estimating method,Power measurement,Power modeling,Real-time monitoring,Streaming-media server,Virtual-instrumentation software},
number = {5},
pages = {1859--1870},
title = {{Estimation by software for the power consumption of streaming-media servers}},
volume = {56},
year = {2007}
}
@article{Yazir:2010bk,
author = {Matthews, Chris and Farahbod, Roozbeh and Neville, Stephen and Guitouni, Adel and Ganti, Sudhakar and Coady, Yvonne},
doi = {10.1109/CLOUD.2010.66},
isbn = {9780769541303},
journal = {IEEE CLOUD},
pages = {91--98},
title = {{Dynamic Resource Allocation in Computing Clouds using Distributed Multiple Criteria Decision Analysis}},
year = {2010}
}
@inproceedings{Poli:2007kt,
author = {Poli, Riccardo and Woodward, John and Burke, Edmund K},
booktitle = {2007 IEEE Congress on Evolutionary Computation},
pages = {3500--3507},
publisher = {IEEE},
title = {{A histogram-matching approach to the evolution of bin-packing strategies}},
year = {2007}
}
@article{Apte:2010vf,
author = {Apte, Renuka and Hu, Liting and Schwan, Karsten and Ghosh, Arpan},
doi = {10.1049/et:20070101},
journal = {HotCloud '10 Proceedings of the 2nd USENIX conference on Hot topics in cloud computing},
pages = {17--17},
title = {{Look who's talking: discovering dependencies between virtual machines using CPU utilization}},
url = {http://portal.acm.org/citation.cfm?id=1863120},
year = {2010}
}
@article{Xie:2011fj,
author = {Xie, Juanying},
doi = {10.4304/jcp.6.2.271-279},
journal = {JCP},
number = {2},
pages = {271--279},
title = {{An Efficient Global K-means Clustering Algorithm}},
volume = {6},
year = {2011}
}
@article{Clark:2005ud,
author = {Clark, Christopher and Fraser, Keir and Hand, Steven and Hansen, Jacob Gorm and Jul, Eric and Limpach, Christian and Pratt, Ian and Warfield, Andrew},
journal = {Proceedings of the 2nd USENIX Symposium on Networked Systems Design and Implementation},
month = {May},
pages = {273--286},
title = {{Live Migration of Virtual Machines}},
year = {2005}
}
@article{Feitelson:2002kn,
author = {Feitelson, Dror G},
isbn = {3-540-44252-9},
journal = {Performance Evaluation of Complex Systems: Techniques and Tools, Performance 2002, Tutorial Lectures},
number = {Chapter 6},
pages = {114--141},
title = {{Workload Modeling for Performance Evaluation}},
url = {http://dl.acm.org/citation.cfm?id=647414.725164},
volume = {2459},
year = {2002}
}
@article{Deshpande:2012jf,
abstract = {Docs. {\#}23. Identical Page(instances of applications, libraries, or operating system pages) 를 모든 VM에 대하여 migration 하는게 아니라 하나의 page 만 다른 rack으로 copy한 후에 같은 rack 안에서는 duplicate 하므로 여러가지 migration performance 를 향상시키는데 IRLM(inter-rack live migration)을 소개.},
author = {Deshpande, Umesh and Kulkarni, Unmesh and Gopalan, Kartik},
doi = {10.1145/2287056.2287062},
isbn = {9781450313445},
journal = {Vtdc '12},
keywords = {all or part of,is granted without fee,live migration,operating systems,or hard copies of,permission to make digital,personal or classroom use,provided that copies are,this work for,virtual machines},
pages = {19--26},
title = {{Inter-rack Live Migration of Multiple Virtual Machines}},
year = {2012}
}
@article{Piraghaj:2015uf,
abstract = {There has been a growing effort in decreasing energy consumption of large-scale cloud data centers via maximization of host-level utilization and load balancing techniques. However, with the recent introduction of Container as a Service (CaaS) by cloud providers, maximizing the utilization at virtual machine (VM) level becomes essential. To this end, this paper focuses on finding efficient virtual machine sizes for hosting containers in such a way that the workload is executed with minimum wastage of resources on VM level. Suitable VM sizes for containers are calculated, and application tasks are grouped and clustered based on their usage patterns obtained from historical data. Furthermore, tasks are mapped to containers and containers are hosted on their associated VM types. We analyzed clouds' trace logs from Google cluster and consider the cloud work- load variances, which is crucial for testing and validating our proposed solutions. Experimental results showed up to 7.55{\%} improvement in the average energy consumption compared to baseline scenarios where the virtual machine sizes are fixed. In addition, comparing to the baseline scenarios, the total number of VMs instantiated for hosting the containers is also improved by 68{\%} on average. I.},
author = {Piraghaj, Sareh Fotuhi and Dastjerdi, Amir Vahid and Calheiros, Rodrigo N. and Buyya, Rajkumar},
doi = {10.1109/SERVICES.2015.14},
isbn = {9781467372756},
journal = {Proceedings - 2015 IEEE World Congress on Services, SERVICES 2015},
keywords = {Cloud Computing,Container as a Service,Energy Efficiency,Virtual Machine},
pages = {31--38},
title = {{Efficient Virtual Machine Sizing for Hosting Containers as a Service}},
year = {2015}
}
@article{Barham:2003cj,
abstract = {Numerous systems have been designed which use virtualization to subdivide the ample resources of a modern computer. Some require specialized hardware, or cannot support commodity operating systems. Some target 100{\%} binary compatibility at the expense of performance. Others sacrifice security or functionality for speed. Few offer resource isolation or performance guarantees; most provide only best-effort provisioning, risking denial of service.This paper presents Xen, an x86 virtual machine monitor which allows multiple commodity operating systems to share conventional hardware in a safe and resource managed fashion, but without sacrificing either performance or functionality. This is achieved by providing an idealized virtual machine abstraction to which operating systems such as Linux, BSD and Windows XP, can be ported with minimal effort.Our design is targeted at hosting up to 100 virtual machine instances simultaneously on a modern server. The virtualization approach taken by Xen is extremely efficient: we allow operating systems such as Linux and Windows XP to be hosted simultaneously for a negligible performance overhead --- at most a few percent compared with the unvirtualized case. We considerably outperform competing commercial and freely available solutions in a range of microbenchmarks and system-wide tests.},
author = {Barham, Paul and Dragovic, Boris and Fraser, Keir and Hand, Steven and Harris, Tim and Ho, Alex and Neugebauer, Rolf and Pratt, Ian and Warfield, Andrew},
doi = {10.1145/945445.945462},
isbn = {1581137575},
issn = {01635980},
journal = {Proceedings of the nineteenth ACM symposium on Operating systems principles  - SOSP '03},
pages = {164},
pmid = {685953},
title = {{Xen and the art of virtualization}},
url = {http://portal.acm.org/citation.cfm?doid=945445.945462},
year = {2003}
}
@article{Energy_9,
abstract = {In this paper we study the problem of energy-aware resource allocation for hosting long-term services or on-demand computing jobs in clusters, e.g., deployed as part of computing infrastructures. We formalize the problem as three constrained optimization problems: maximize job performance under power consumption constraints, minimize power consumption under job performance constraints, and optimize a linear combination of power consumption and job performance. These problems are NP-hard but, given an instance, a bound on the optimal solution can be computed via a rational linear program. We propose polynomial heuristics for all three problems. Simulation experiments show that in all three cases some heuristics can achieve results close to optimal, i.e., lead to good job performance while conserving energy. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
author = {Borgetto, Damien and Casanova, Henri and {Da Costa}, Georges and Pierson, Jean Marc},
doi = {10.1016/j.future.2011.04.018},
isbn = {0167-739X},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Allocation,Energy-aware,Heuristic,Optimal,Service},
number = {5},
pages = {769--779},
title = {{Energy-aware service allocation}},
volume = {28},
year = {2012}
}
@inproceedings{Energy_3,
author = {Bohra, Ata E Husain and Chaudhary, Vipin},
booktitle = {Parallel {\&} Distributed Processing, Workshops and Phd Forum (IPDPSW), 2010 IEEE International Symposium on},
organization = {Ieee},
pages = {1--8},
title = {{VMeter Power modelling for virtualized clouds.pdf}},
year = {2010}
}
@article{Wood:2009fn,
author = {Wood, Timothy and Shenoy, Prashant J and Venkataramani, Arun and Yousif, Mazin S},
journal = {Computer Networks},
number = {17},
pages = {2923--2938},
title = {{Sandpiper - Black-box and gray-box resource management for virtual machines.}},
volume = {53},
year = {2009}
}
@article{Panigrahy:2011wk,
abstract = {Inspired by virtual machine placement problems, we study heuristics for the Vector Bin Packing problem, where we are required to pack {\$}n{\$} items represented by {\$}d{\$}-dimensional vectors, into as few bins of size {\$}1{\^{}}d{\$} each as possible. We systematically study variants of the First Fit Decreasing (FFD) algorithm that have been proposed for this problem. Inspired by bad instances for FFD-type algorithms, we propose new geometric heuristics that run nearly as fast as FFD for reasonable values of {\$}n{\$} and {\$}d{\$}. We report on empirical evaluations of the FFD-based, as well as the new heuristics on large families of distributions. We identify which FFD variants work best in most cases and show that our new heuristics usually outperform FFD-based heuristics and can sometimes reduce the number of bins used by up to ten percent. Further, in all cases where we were able to compute the optimal solution we found our new heuristics within few percent of optimal. We conclude that these new heuristics are an excellent alternative to FFD-based heuristics and are prime candidates to be used in practice.},
author = {Panigrahy, Rina and Talwar, Kunal and Uyeda, Lincoln and Wieder, Udi},
journal = {Research.Microsoft.Com},
title = {{Heuristics for Vector Bin Packing}},
url = {http://research.microsoft.com/pubs/147927/VBPackingESA11.pdf},
year = {2011}
}
@book{Anonymous:lWt0RSgd,
abstract = {Cloud computing has become integrated into all sectors, from business to quotidian life. Since it has revolutionized modern computing, there is a need for updated research related to the architecture and frameworks necessary to maintain its efficiency.},
author = {Chen, Jianwen and Zhang, Yan and Gottschalk, Ron},
isbn = {9781522507598},
pages = {507},
publisher = {IGI Global},
series = {Advances in Systems Analysis, Software Engineering, and High Performance Computing},
title = {{Handbook of Research on End-to-End Cloud Computing Architecture Design}},
year = {2016}
}
@inproceedings{Dua:2014bw,
abstract = {—PaaS vendors face challenges in efficiently providing services with the growth of their offerings. In this paper, we explore how PaaS vendors are using containers as a means of hosting Apps. The paper starts with a discussion of PaaS Use case and the current adoption of Container based PaaS archi- tectures with the existing vendors. We explore various container implementations - Linux Containers, Docker, Warden Container, lmctfy and OpenVZ.We look at how each of this implementation handle Process, FileSystem and Namespace isolation. We look at some of the unique features of each container and how some of them reuse base Linux Container implementation or differ from it. We also explore how IaaSlayer itself has started providing support for container lifecycle management along with Virtual Machines. In the end, we look at factors affecting container implementation choices and some of the features missing from the existing implementations for the next generation PaaS.},
archivePrefix = {arXiv},
arxivId = {0521865719 9780521865715},
author = {Dua, Rajdeep and Raja, A. Reddy and Kakadia, Dharmesh},
booktitle = {Proceedings - 2014 IEEE International Conference on Cloud Engineering, IC2E 2014},
doi = {10.1109/IC2E.2014.41},
eprint = {0521865719 9780521865715},
isbn = {9781479937660},
issn = {2373-3845},
keywords = {container,paas,virtualization},
pages = {610--614},
pmid = {11242594},
publisher = {IEEE},
title = {{Virtualization vs containerization to support PaaS}},
year = {2014}
}
@article{Mishra:2011bz,
author = {Mishra, Mayank and Sahoo, Anirudha},
doi = {10.1109/CLOUD.2011.38},
isbn = {9780769544601},
journal = {IEEE CLOUD},
pages = {275--282},
title = {{On Theory of VM Placement : Anomalies in Existing Methodologies and Their Mitigation Using a Novel Vector Based Approach}},
year = {2011}
}
@article{Calheiros:2011fs,
author = {Calheiros, Rodrigo N. and Ranjan, Rajiv and Buyya, Rajkumar},
doi = {10.1109/HPCSIM.2009.5192685},
isbn = {978-1-4577-1336-1},
journal = {2009 International Conference on High Performance Computing {\&} Simulation},
pages = {295--304},
title = {{Virtual Machine Provisioning Based on Analytical Performance and QoS in Cloud Computing Environments}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6047198{\%}5Cnhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5192685},
year = {2009}
}
@inproceedings{Urgaonkar:2005kh,
abstract = {Dynamic capacity provisioning is a useful technique for handling the multi-time-scale variations seen in Internet workloads. In this paper, we propose a novel dynamic provisioning technique for multitier Internet applications that employs (i) a flexible queuing model to determine how much resources to allocate to each tier of the application, and (ii) a combination of predictive and reactive methods that determine when to provision these resources, both at large and small time scales. Our experiments on a forty-machine Linux-based hosting platform demonstrate the responsiveness of our technique in handling dynamic workloads. In one scenario where a flash crowd caused the workload of a three-tier application to double, our technique was able to double the application capacity within five minutes, thus maintaining response time targets},
author = {Urgaonkar, Bhuvan and Shenoy, Prashant and Chandra, Abhishek and Goyal, Pawan},
booktitle = {Proceedings - Second International Conference on Autonomic Computing, ICAC 2005},
doi = {10.1109/ICAC.2005.27},
isbn = {0769522769},
issn = {15564665},
pages = {217--228},
publisher = {IEEE},
title = {{Dynamic provisioning of multi-tier Internet applications}},
volume = {2005},
year = {2005}
}
@article{Leinberger:1999fs,
abstract = {Multi-capacity bin-packing is a generalization of the classical one-dimensional bin-packing problem in which the bin capacity and the item sizes are represented by d-dimensional vectors. Previous work in d-capacity bin-packing algorithms analyzed var...},
author = {Leinberger, William and Karypis, George and Kumar, Vipin},
doi = {10.1109/ICPP.1999.797428},
isbn = {0-7695-0350-0},
issn = {0190-3918},
journal = {Icpp},
keywords = {algorithm},
pages = {404--412},
title = {{Multi-Capacity Bin Packing Algorithms with Applications to Job Scheduling under Multiple Constraints.}},
url = {http://dx.doi.org/10.1109/ICPP.1999.797428},
year = {1999}
}
@article{Luiz:2010go,
abstract = {Energy efficiency has become one of the key challenges for a large$\backslash$nclass of electronic systems. Longer time between battery recharges$\backslash$nis highly desirable for battery-powered devices such as mobile phones,$\backslash$ndigital cameras, Internet tablets, and electronic organizers. Energy$\backslash$nefficiency is also important for electronic systems powered from$\backslash$nthe electric grid since it may reduce power consumption and the cooling$\backslash$nrequirements. Power savings are possible because electronic systems$\backslash$ngenerally have an idle state, when, for example, the processor can$\backslash$nrun in a low-power state. Thus, the correct estimation of the workload$\backslash$nmodel plays an essential role in the decision of which and when a$\backslash$npower state transition should be performed by the electronic system.$\backslash$nThis paper introduces a multisize sliding window workload estimation$\backslash$ntechnique for dynamic power management (DPM) in nonstationary environments.$\backslash$nThis technique reduces both the effects of identification delay and$\backslash$nsampling error present in the previous fixed-size sliding window$\backslash$napproach. The system is modeled by discrete-time Markov chains and$\backslash$nthe model offers a rigorous mathematical formulation of the problem$\backslash$nand allows one to obtain an excellent trade-off between performance$\backslash$nand power consumption.},
author = {Luiz, Saulo O D and Perkusich, Angelo and {Lima, Sr.}, Antonio M N},
doi = {10.1109/TC.2010.90},
issn = {0018-9340},
journal = {IEEE Transactions on Computers},
keywords = {Batteries,Digital cameras,Energy consumption,Energy efficiency,Energy management,Internet,Markov models,Markov processes,Mathematical model,Mobile handsets,Power management,Power system management,Power system modeling,discrete-time Markov chain,dynamic power management,electric grid,energy efficiency,energy-aware systems,estimation.,multisize sliding window,power aware computing,workload estimation,workload model},
number = {12},
pages = {1625--1639},
title = {{Multisize Sliding Window in Workload Estimation for Dynamic Power Management}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5453349},
volume = {59},
year = {2010}
}
@article{Jung:2010iw,
abstract = {Server consolidation based on virtualization is an important technique for improving power efficiency and resource utilization in cloud infrastructures. However, to ensure satisfactory performance on shared resources under changing application workloads, dynamic management of the resource pool via online adaptation is critical. The inherent tradeoffs between power and performance as well as between the cost of an adaptation and its benefits make such management challenging. In this paper, we present Mistral, a holistic controller framework that optimizes power consumption, performance benefits, and the transient costs incurred by various adaptations and the controller itself to maximize overall utility. Mistral can handle multiple distributed applications and large-scale infrastructures through a multi-level adaptation hierarchy and scalable optimization algorithm. We show that our approach outstrips other strategies that address the tradeoff between only two of the objectives (power, performance, and transient costs).},
author = {Jung, Gueyoung and Hiltunen, Matti A. and Joshi, Kaustubh R. and Schlichting, Richard D. and Pu, Calton},
doi = {10.1109/ICDCS.2010.88},
isbn = {9780769540597},
issn = {10636927},
journal = {Proceedings - International Conference on Distributed Computing Systems},
pages = {62--73},
title = {{Mistral: Dynamically managing power, performance, and adaptation cost in cloud infrastructures}},
year = {2010}
}
@article{Gandhi:2009dt,
abstract = {Server farms today consume more than 1.5{\%} of the total electricity in the U.S. at a cost of nearly {\$}4.5 billion. Given the rising cost of energy, many industries are now seeking solutions for how to best make use of their available power. An important question which arises in this context is how to distribute available power among servers in a server farm so as to get maximum performance. By giving more power to a server, one can get higher server frequency (speed). Hence it is commonly believed that, for a given power budget, performance can be maximized by operating servers at their highest power levels. However, it is also conceivable that one might prefer to run servers at their lowest power levels, which allows more servers to be turned on for a given power budget. To fully understand the effect of power allocation on performance in a server farm with a fixed power budget, we introduce a queueing theoretic model, which allows us to predict the optimal power allocation in a variety of scenarios. Results are verified via extensive experiments on an IBM BladeCenter. We find that the optimal power allocation varies for different scenarios. In particular, it is not always optimal to run servers at their maximum power levels. There are scenarios where it might be optimal to run servers at their lowest power levels or at some intermediate power levels. Our analysis shows that the optimal power allocation is non-obvious and depends on many factors such as the power-to-frequency relationship in the processors, the arrival rate of jobs, the maximum server frequency, the lowest attainable server frequency and the server farm configuration. Furthermore, our theoretical model allows us to explore more general settings than we can implement, including arbitrarily large server farms and different power-to-frequency curves. Importantly, we show that the optimal power allocation can significantly improve server farm performance, by a factor of typically 1.4 and as much as a factor of 5 in some cases.},
author = {Gandhi, Anshul and Harchol-Balter, Mor and Das, Rajarshi and Lefurgy, Charles},
doi = {10.1145/1555349.1555368},
isbn = {9781605585116},
issn = {978-1-60558-511-6},
journal = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems - SIGMETRICS '09},
pages = {157},
title = {{Optimal power allocation in server farms}},
url = {http://dl.acm.org/citation.cfm?id=1555368{\%}5Cnhttp://portal.acm.org/citation.cfm?doid=1555349.1555368},
year = {2009}
}
@article{Dempe:2006jc,
abstract = {{\textcopyright} 2006 Springer Science + Business Media, LLC.Focus in the paper is on optimality conditions for bilevel programming problems. We start with a general condition using tangent cones of the feasible set of the bilevel programming problem to derive such conditions for the optimistic bilevel problem. More precise conditions are obtained if the tangent cone possesses an explicit description as it is possible in the case of linear lower level problems. If the optimal solution of the lower level problem is a PC1-function, sufficient conditions for a global optimal solution of the optimistic bilevel problem can be formulated. In the second part of the paper relations of the bilevel programming problem to set-valued optimization problems and to mathematical programs with equilibrium constraints are given which can also be used to formulate optimality conditions for the original problem. Finally, a variational inequality approach is described which works well when the involved functions are monotone. It consists in a variational re-formulation of the optimality conditions and looking for a solution of the thus obtained variational inequality among the points satisfying the initial constraints. A penalty function technique is applied to get a sequence of approximate solutions converging to a solution of the original problem with monotone operators.},
author = {Dempe, S. and Kalashnikov, V.V. and Kalashnykova, N.},
doi = {10.1007/0-387-34221-4_1},
issn = {19316836 19316828},
journal = {Springer Optimization and Its Applications},
keywords = {[Bilevel programming, Mathematical programs with e},
month = {Oct},
number = {5-6},
pages = {505--524},
title = {{Optimality conditions for bilevel programming problems}},
volume = {2},
year = {2006}
}
@article{Legillon:2012dd,
abstract = {This article presents CoBRA, a new evolutionary algorithm, based on a coevolutionary scheme, to solve bi-level optimization problems. It handles population-based algorithms on each level, each one cooperating with the other to provide solutions for the overall problem. Moreover, in order to evaluate the relevance of CoBRA against more classical approaches, a new performance assessment methodology, based on rationality, is introduced. An experimental analysis is conducted on a bi-level distribution planning problem, where multiple manufacturing plants deliver items to depots, and where a distribution company controls several depots and distributes items from depots to retailers. The experimental results reveal significant enhancements, particularly over the lower level, with respect to a more classical approach based on a hierarchical scheme.},
author = {Legillon, Fran{\c{c}}ois and Liefooghe, Arnaud and Talbi, El Ghazali},
doi = {10.1109/CEC.2012.6256620},
isbn = {9781467315098},
issn = {1089-778X},
journal = {2012 IEEE Congress on Evolutionary Computation, CEC 2012},
keywords = {Algorithm design and analysis,Evolutionary computation,bi-level optimization,vehicle routing},
pages = {1--8},
title = {{CoBRA: A cooperative coevolutionary algorithm for bi-level optimization}},
year = {2012}
}
@booklet{Schwefel:Nd2pzTwG,
author = {Schwefel, H P},
title = {{Evolutionsstrategie und numerische Optimierung, Technische Universit{\"{a}}t Berlin, Fachbereich Verfahrenstechnik, Dr.-Ing}},
year = {1975}
}
@article{Dayarathna:2016ua,
abstract = {Data centers are critical, energy-hungry infrastructures that run large-scale Internet-based services. Energy consumption models are pivotal in designing and optimizing energy-efficient operations to curb excessive energy consumption in data centers. In this paper, we survey the state-of-the-art techniques used for energy consumption modeling and prediction for data centers and their components. We conduct an in-depth study of the existing literature on data center power modeling, covering more than 200 models. We organize these models in a hierarchical structure with two main branches focusing on hardware-centric and software-centric power models. Under hardware-centric approaches we start from the digital circuit level and move on to describe higher-level energy consumption models at the hardware component level, server level, data center level, and finally systems of systems level. Under the software-centric approaches we investigate power models developed for operating systems, virtual machines and software applications. This systematic approach allows us to identify multiple issues prevalent in power modeling of different levels of data center systems, including: i) few modeling efforts targeted at power consumption of the entire data center ii) many state-of-the-art power models are based on a few CPU or server metrics, and iii) the effectiveness and accuracy of these power models remain open questions. Based on these observations, we conclude the survey by describing key challenges for future research on constructing effective and accurate data center power models.},
author = {Dayarathna, Miyuru and Wen, Yonggang and Fan, Rui},
doi = {10.1109/COMST.2015.2481183},
isbn = {1553-877X VO - 18},
issn = {1553-877X},
journal = {IEEE Communications Surveys {\&} Tutorials},
number = {1},
pages = {732--794},
title = {{Data Center Energy Consumption Modeling: A Survey}},
url = {http://ieeexplore.ieee.org/document/7279063/},
volume = {18},
year = {2016}
}
@article{Deb:2009jh,
author = {Deb, Kalyanmoy and Sinha, Ankur},
journal = {International Conference on Evolutionary Multi-Criterion Optimization},
number = {1},
pages = {110--124},
title = {{Solving bilevel multi-objective optimization problems using evolutionary algorithms}},
volume = {5467},
year = {2009}
}
@article{Uhlig:2005do,
abstract = { A virtualized system includes a new layer of software, the virtual machine monitor. The VMM's principal role is to arbitrate accesses to the underlying physical host platform's resources so that multiple operating systems (which are guests of the VMM) can share them. The VMM presents to each guest OS a set of virtual platform interfaces that constitute a virtual machine (VM). Once confined to specialized, proprietary, high-end server and mainframe systems, virtualization is now becoming more broadly available and is supported in off-the-shelf systems based on Intel architecture (IA) hardware. This development is due in part to the steady performance improvements of IA-based systems, which mitigates traditional virtualization performance overheads. Intel virtualization technology provides hardware support for processor virtualization, enabling simplifications of virtual machine monitor software. Resulting VMMs can support a wider range of legacy and future operating systems while maintaining high performance.},
author = {Uhlig, Rich and Neiger, Gil and Rodgers, Dion and Santoni, Amy L. and Martins, Fernando C M and Anderson, Andrew V. and Bennett, Steven M. and K??gi, Alain and Leung, Felix H. and Smith, Larry},
doi = {10.1109/MC.2005.163},
isbn = {0018-9162},
issn = {00189162},
journal = {Computer},
number = {5},
pages = {48--56},
title = {{Intel virtualization technology}},
volume = {38},
year = {2005}
}
@article{Li:2011cx,
abstract = {Choosing the best-performing cloud for one's application is a critical problem for potential cloud customers. We propose CloudProphet, a trace-and-replay tool to predict a legacy application's performance if migrated to a cloud infrastructure. CloudProphet traces the workload of the application when running locally, and replays the same workload in the cloud for prediction. We discuss two key technical challenges in designing CloudProphet, and some preliminary results using a prototype implementation.},
author = {Li, Ang and Zong, Xuanran and Kandula, Srikanth and Yang, Xiaowei and Zhang, Ming},
doi = {10.1145/2043164.2018502},
isbn = {1450307973},
issn = {01464833},
journal = {ACM SIGCOMM Computer Communication Review},
keywords = {cloud computing,performace,prediction},
pages = {426--427},
title = {{CloudProphet: towards application performance prediction in cloud}},
volume = {41},
year = {2011}
}
@inproceedings{Service_1,
author = {Yau, Stephen S and An, Ho G},
booktitle = {Proceedings of the First Asia-Pacific Symposium on Internetware},
doi = {10.1145/1640206.1640209},
isbn = {9781605588728},
organization = {ACM},
pages = {1--7},
title = {{Adaptive resource allocation for service-based systems}},
url = {http://delivery.acm.org/10.1145/1650000/1640209/a3-yau.pdf?ip=161.139.153.20{\&}id=1640209{\&}acc=ACTIVE SERVICE{\&}key=69AF3716A20387ED.C758BA176ED44BB8.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}CFID=829676548{\&}CFTOKEN=99824377{\&}{\_}{\_}acm{\_}{\_}=1472099825{\_}e88a460f1147a597e485c8a8e},
year = {2009}
}
@article{Mathieu:2011dw,
author = {Mathieu, R and Pittard, L and Anandalingam, G},
journal = {RAIRO-Operations Research},
month = {Mar},
number = {1},
pages = {1--21},
title = {{Genetic algorithm based approach to bi- level linear programming}},
volume = {28},
year = {1994}
}
@article{Zhang:2010vo,
abstract = {Cloud computing has recently emerged as a new paradigm for hosting and delivering services over the Internet. Cloud computing is attractive to business owners as it eliminates the requirement for users to plan ahead for provisioning, and allows enterprises to start from the small and increase resources only when there is a rise in service demand. However, despite the fact that cloud computing offers huge opportunities to the IT industry, the development of cloud computing technology is currently at its infancy, with many issues still to be addressed. In this paper, we present a survey of cloud computing, highlighting its key concepts, architectural principles, state-of-the-art implementation as well as research challenges. The aim of this paper is to provide a better understanding of the design challenges of cloud computing and identify important research directions in this increasingly important area.},
archivePrefix = {arXiv},
arxivId = {S0167739X10002554},
author = {Zhang, Qi and Cheng, Lu and Boutaba, Raouf},
doi = {10.1007/s13174-010-0007-6},
eprint = {S0167739X10002554},
isbn = {1867482818690},
issn = {18674828},
journal = {Journal of Internet Services and Applications},
keywords = {Cloud computing,Data centers,Virtualization},
number = {1},
pages = {7--18},
pmid = {21479169},
title = {{Cloud computing: State-of-the-art and research challenges}},
volume = {1},
year = {2010}
}
@incollection{Mann:2016hx,
address = {Cham},
author = {Mann, Zolt{\'{a}}n {\'{A}}d{\'{a}}m},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-44482-6_9},
isbn = {9783319444819},
issn = {16113349},
month = {Aug},
pages = {137--151},
publisher = {Springer International Publishing},
title = {{Interplay of virtual machine selection and virtual machine placement}},
volume = {9846},
year = {2016}
}
@incollection{Zio:2012jz,
abstract = {In multiobjective optimization problems, the identified Pareto Frontiers and Sets often con- tain too many solutions, which make it difficult for the decision maker to select a preferred alternative. To facilitate the selection task, decision making support tools can be used in different instances of the multiobjective optimization search to introduce preferences on the objectives or to give a condensed representation of the solutions on the Pareto Frontier, so as to offer to the decision maker a manageable picture of the solution alternatives. This paper presents a comparison of some a priori and a posteriori decision making support methods, aimed at aiding the decision maker in the selection of the preferred solutions. The considered methods are compared with respect to their application to a case study concerning the optimization of the test intervals of the components of a safety system of a nuclear power plant. The engine for the multiobjective optimization search is based on genetic algorithms.},
address = {Paris},
author = {Zio, Enrico and Bazzo, Roberta},
booktitle = {Computational Intelligence Systems in Industrial Engineering},
doi = {10.2991/978-94-91216-77-0},
isbn = {978-94-91216-76-3},
month = {Nov},
pages = {203--230},
publisher = {Atlantis Press},
title = {{A Comparison ofMethods For Selecting Preferred Solutions inMultiobjective Decision Making}},
url = {http://www.springerlink.com/index/10.2991/978-94-91216-77-0},
volume = {6},
year = {2012}
}
@article{Armbrust:2010ee,
abstract = {Clearing the clouds away from the true potential and obstacles posed by this computing capability.},
archivePrefix = {arXiv},
arxivId = {0521865719 9780521865715},
author = {Armbrust, Michael and Stoica, Ion and Zaharia, Matei and Fox, Armando and Griffith, Rean and Joseph, Anthony D. and Katz, Randy and Konwinski, Andy and Lee, Gunho and Patterson, David and Rabkin, Ariel},
doi = {10.1145/1721654.1721672},
eprint = {0521865719 9780521865715},
isbn = {9781605589336},
issn = {00010782},
journal = {Communications of the ACM},
number = {4},
pages = {50},
pmid = {11242594},
title = {{A view of cloud computing}},
url = {http://portal.acm.org/citation.cfm?doid=1721654.1721672},
volume = {53},
year = {2010}
}
@article{Mateos:2013bm,
abstract = {Parameter Sweep Experiments (PSEs) allow scientists and engineers to conduct experiments by running the same program code against different input data. This usually results in many jobs with high computational requirements. Thus, distributed environments, particularly Clouds, can be employed to fulfill these demands. However, job scheduling is challenging as it is an NP-complete problem. Recently, Cloud schedulers based on bio-inspired techniques-which work well in approximating problems with little input information-have been proposed. Unfortunately, existing proposals ignore job priorities, which is a very important aspect in PSEs since it allows accelerating PSE results processing and visualization in scientific Clouds. We present a new Cloud scheduler based on Ant Colony Optimization, the most popular bio-inspired technique, which also exploits well-known notions from operating systems theory. Simulated experiments performed with real PSE job data and other Cloud scheduling policies indicate that our proposal allows for a more agile job handling while reducing PSE completion time. {\textcopyright} 2012 Elsevier Ltd. All rights reserved.},
author = {Mateos, Cristian and Pacini, Elina and Garino, Carlos Garc{\'{i}}a},
doi = {10.1016/j.advengsoft.2012.11.011},
isbn = {0965-9978},
issn = {09659978},
journal = {Advances in Engineering Software},
keywords = {Ant Colony Optimization,Cloud Computing,Job scheduling,Parameter sweep experiments,Swarm Intelligence,Weighted flowtime},
pages = {38--50},
title = {{An ACO-inspired algorithm for minimizing weighted flowtime in cloud-based parameter sweep experiments}},
volume = {56},
year = {2013}
}
@article{Wang:2007em,
abstract = {Computation-intensive design problems are becoming increasingly common in manufacturing industries. The computation burden is often caused by expensive analysis and simulation processes in order to reach a comparable level of accuracy as physical testing data. To address such a challenge, approximation or metamodeling techniques are often used. Metamodeling techniques have been developed from many different disciplines including statistics, mathematics, computer science, and various engineering disciplines. These metamodels are initially developed as “surrogates” of the expensive simulation process in order to improve the overall computation efficiency. They are then found to be a valuable tool to support a wide scope of activities in modern engineering design, especially design optimization. This work reviews the state-of-the-art metamodel-based techniques from a practitioner's perspective according to the role of metamodeling in supporting design optimization, including model approximation, design space exploration, problem formulation, and solving various types of optimization problems. Challenges and future development of metamodeling in support of engineering design is also analyzed and discussed.},
author = {Wang, G. Gary and Shan, S.},
doi = {10.1115/1.2429697},
isbn = {0-7918-4255-X},
issn = {10500472},
journal = {Journal of Mechanical Design},
month = {Apr},
number = {4},
pages = {370},
title = {{Review of Metamodeling Techniques in Support of Engineering Design Optimization}},
url = {http://mechanicaldesign.asmedigitalcollection.asme.org/article.aspx?articleid=1449318},
volume = {129},
year = {2007}
}
@article{Radcliffe:1991tp,
abstract = {Intrinsic parallelism is shown to have application beyond schemata and o-schemata. More general objects called formae are introduced and general operators which manipulate these are introduced and discussed. These include random, respectful recombination. The extended formalism is applied to various common representations and standard operators are analysed in the light of the formalism. 1},
author = {Radcliffe, N},
journal = {Proceedings of the 4th International Conference on Genetic Algorithms, San Mateo, California},
pages = {222--229},
title = {{Formal Analysis and Random Respectful Recombination}},
year = {1991}
}
@article{Sun:2008gq,
author = {Huijun, Sun and Ziyou, Gao},
journal = {China Journal of Highway and Transport},
month = {Apr},
number = {2},
pages = {115--119},
title = {{Bi-level programming model and solution algorithm for the location of logistics distribution centers based on the routing problem}},
volume = {16},
year = {2003}
}
@article{Bard:1982gsa,
author = {Bards, F and Science, Applied},
journal = {Computers {\&} OR},
number = {I},
pages = {77--100},
title = {{an Explicit Solution To the Multi-Level}},
volume = {9},
year = {1982}
}
@inproceedings{Farahnakian:2015gw,
abstract = {Dynamic Virtual Machine (VM) consolidation is one of the most promising solutions to reduce energy consumption and improve resource utilization in data centers. Since VM consolidation problem is strictly NP-hard, many heuristic algorithms have been proposed to tackle the problem. However, most of the existing works deal only with minimizing the number of hosts based on their current resource utilization and these works do not explore the future resource requirements. Therefore, unnecessary VM migrations are generated and the rate of Service Level Agreement (SLA) violations are increased in data centers. To address this problem, our VM consolidation method which is formulated as a bin-packing problem considers both the current and future utilization of resources. The future utilization of resources is accurately predicted using a k-nearest neighbor regression based model. In this paper, we investigate the effectiveness of VM and host resource utilization predictions in the VM consolidation task using real workload traces. The experimental results show that our approach provides substantial improvement over other heuristic algorithms in reducing energy consumption, number of VM migrations and number of SLA violations. {\textcopyright} 2015 IEEE.},
author = {Farahnakian, Fahimeh and Pahikkala, Tapio and Liljeberg, Pasi and Plosila, Juha and Tenhunen, Hannu},
booktitle = {Proceedings - 2015 IEEE 8th International Conference on Cloud Computing, CLOUD 2015},
doi = {10.1109/CLOUD.2015.58},
isbn = {9781467372879},
keywords = {Bin-paking,Dynamic VM consolidation,Energy-efficiency,SLA,Utilization prediction model},
pages = {381--388},
publisher = {IEEE},
title = {{Utilization Prediction Aware VM Consolidation Approach for Green Cloud Computing}},
year = {2015}
}
@article{Liu:2013kl,
abstract = {System virtualization is becoming pervasive and it is enabling important new computing diagrams such as cloud computing. Live virtual machine (VM) migration is a unique capability of system virtualization which allows applications to be transparently moved across physical machines with a consistent state captured by their VMs. Although live VM migration is generally fast, it is a resource-intensive operation and can impact the application performance and resource usage of the migrating VM as well as other concurrent VMs. However, existing studies on live migration performance are often based on the assumption that there are sufficient resources on the source and destination hosts, which is often not the case for highly consolidated systems. As the scale of virtualized systems such as clouds continue to grow, the use of live migration becomes increasingly more important for managing performance and reliability in such systems. Therefore, it is key to understand the performance of live VM migration under different levels of resource availability. This paper addresses this need by creating performance models for live migration which can be used to predict a VM's migration time given its application's behavior and the resources available to the migration. A series of experiments were conducted on Xen to profile the time for migrating a DomU VM running different resource-intensive applications while Dom0 is allocated different CPU shares for processing the migration. Regression methods are then used to create the performance model based on the profiling data. The results show that the VM's migration time is indeed substantially impacted by Dom0's CPU allocation whereas the performance model can accurately capture this relationship with the coefficient of determination generally higher than 90{\{}{\%}{\}}.},
author = {Liu, Haikun and Xu, Cheng-Zhong and Jin, Hai and Gong, Jiayu and Liao, Xiaofei},
doi = {10.1145/1996130.1996154},
isbn = {9781450305525},
issn = {10828907},
journal = {Proceedings of the 20th international symposium on High performance distributed computing - HPDC '11},
number = {2},
pages = {171},
title = {{Performance and energy modeling for live migration of virtual machines}},
url = {http://portal.acm.org/citation.cfm?doid=1996130.1996154},
volume = {16},
year = {2011}
}
@book{Li:2009wf,
abstract = {With the increasing prevalence of large scale cloud computing environments, how to place requested applications into available computing servers regarding to energy consumption has become an essential research problem, but existing application placement approaches are still not effective for live applications with dynamic characters. In this paper, we proposed a novel approach named EnaCloud, which enables application live placement dynamically with consideration of energy efficiency in a cloud platform. In EnaCloud, we use a Virtual Machine to encapsulate the application, which supports applications scheduling and live migration to minimize the number of running machines, so as to save energy. Specially, the application placement is abstracted as a bin packing problem, and an energy-aware heuristic algorithm is proposed to get an appropriate solution. In addition, an over-provision approach is presented to deal with the varying resource demands of applications. Our approach has been successfully implemented as useful components and fundamental services in the iVIC platform. Finally, we evaluate our approach by comprehensive experiments based on virtual machine monitor Xen and the results show that it is feasible.},
author = {Li, Bo and Li, Jianxin and Huai, Jinpeng and Wo, Tianyu and Li, Qin and Zhong, Liang},
booktitle = {CLOUD 2009 - 2009 IEEE International Conference on Cloud Computing},
doi = {10.1109/CLOUD.2009.72},
isbn = {9780769538402},
keywords = {Application placement,Cloud computing,Energy saving,Live migration,Virtual machine},
pages = {17--24},
publisher = {ACM International Conference on Cloud Computing},
title = {{EnaCloud: An energy-saving application live placement approach for cloud computing environments}},
year = {2009}
}
@article{Barroso:2007jt,
archivePrefix = {arXiv},
arxivId = {arXiv:astro-ph/0106035v2},
author = {Barroso, Luiz Andr{\'{e}} and H{\"{o}}lzle, Urs},
doi = {10.1109/MC.2007.443},
eprint = {0106035v2},
isbn = {0018-9162},
issn = {0018-9162},
journal = {Computer},
keywords = {energy-proportional computing,green computing},
number = {12},
pages = {33--37},
pmid = {4404806},
primaryClass = {arXiv:astro-ph},
title = {{The Case for Energy-Proportional Computing}},
url = {http://ieeexplore.ieee.org/document/4404806/},
volume = {40},
year = {2007}
}
@inproceedings{ga_saas,
abstract = {Cloud computing is a latest new computing paradigm where applications, data and IT services are provided over the Internet. Cloud computing has become a main medium for Software as a Service (SaaS) providers to host their SaaS as it can provide the scalability a SaaS requires. The challenges in the composite SaaS placement process rely on several factors including the large size of the Cloud network, SaaS competing resource requirements, SaaS interactions between its components and SaaS interactions with its data components. However, existing applications' placement methods in data centres are not concerned with the placement of the component's data. In addition, a Cloud network is much larger than data center networks that have been discussed in existing studies. This paper proposes a penalty-based genetic algorithm (GA) to the composite SaaS placement problem in the Cloud. We believe this is the first attempt to the SaaS placement with its data in Cloud provider's servers. Experimental results demonstrate the feasibility and the scalability of the GA.},
author = {Yusoh, Zeratul Izzah Mohd and Tang, Maolin},
booktitle = {2010 IEEE World Congress on Computational Intelligence, WCCI 2010 - 2010 IEEE Congress on Evolutionary Computation, CEC 2010},
doi = {10.1109/CEC.2010.5586151},
isbn = {9781424469109},
organization = {IEEE},
pages = {1--8},
title = {{A penalty-based genetic algorithm for the composite SaaS placement problem in the cloud}},
year = {2010}
}
@incollection{Cowling:2000ek,
address = {Berlin, Heidelberg},
author = {Cowling, Peter and Kendall, Graham and Soubeiga, Eric},
booktitle = {:Selected Papers of theThird International Conference on the Practice And Theory of Automated Timetabling, PATAT 2000},
keywords = {choice function,heuristics,hyperheuristics,local search,metaheuristics,personnel scheduling},
month = {Aug},
pages = {176--190},
publisher = {Springer, Berlin, Heidelberg},
title = {{A Hyper heuristic Approach to Scheduling a Sales Summit}},
year = {2000}
}
@article{Meng:2010gh,
abstract = {Resource provisioning in compute clouds often require an estimate of the capacity needs of Virtual Machines (VMs). The estimated VM size is the basis for allocating resources commensurate with workload demand. In contrast to the traditional practice of estimating the VM sizes individually, we propose a joint-VM sizing approach in which multiple VMs are consolidated and provisioned, based on an estimate of their aggregate capacity needs. This new approach exploits statistical multiplexing among the workload patterns of multiple VMs, i.e., the peaks and valleys in one workload pattern do not necessarily coincide with the others. Thus, the unused resources of a low utilized VM can be directed to the other co-located VMs with high utilization. Compared to individual VM based provisioning, joint-VM sizing and provisioning may lead to much higher resource utilization. This paper presents three design modules to enable the concept in practice. Specifically, a performance constraint describing the capacity need of a VM for achieving a certain level of application performance; an algorithm for estimating the size of jointly provisioning VMs; a VM selection method that seeks to find good VM combinations for being provisioned together. We showcase that the proposed three modules can be seamlessly plugged into existing applications such as resource provisioning, and providing resource guarantees for VMs. The proposed algorithms and applications are evaluated by monitoring data collected from about 16 thousand VMs in commercial data centers. These evaluations reveal more than 45{\%} improvements in terms of the overall resource utilization.},
author = {Meng, Xiaoqiao and Isci, Canturk and Kephart, Jeffrey O. and Zhang, Li and Bouillet, Eric and Pendarakis, Dimitrios},
doi = {10.1145/1809049.1809052},
isbn = {9781450300742},
journal = {Proceeding of the 7th international conference on Autonomic computing - ICAC'10},
keywords = {a key factor for,achieving economies of scale,ation,certain amount of computing,cloud computing,in a compute cloud,is configured with a,is resource,memory and i,o,provisioning,re-,sources,such as cpu,virtualization},
pages = {11},
title = {{Efficient resource provisioning in compute clouds via VM multiplexing}},
url = {http://portal.acm.org/citation.cfm?doid=1809049.1809052},
year = {2010}
}
@article{Hines:2009fv,
author = {Clark, C. and Fraser, K. and Hand, S. and Hansen, J.G. and Jul, E. and Limpach, C. and Pratt, I. and Warfield, A.},
journal = {Usenix.Org},
month = {Jul},
number = {3},
pages = {14--26},
title = {{Live migration of virtual machines}},
url = {http://www.usenix.org/events/nsdi05/tech/full{\_}papers/clark/clark{\_}html},
volume = {43},
year = {2005}
}
@article{Farahnakian:2015vj,
abstract = {Dynamic Virtual Machine (VM) consolidation is one of the most promising solutions to reduce energy consumption and improve resource utilization in data centers. Since VM consolidation problem is strictly NP-hard, many heuristic algorithms have been proposed to tackle the problem. However, most of the existing works deal only with minimizing the number of hosts based on their current resource utilization and these works do not explore the future resource requirements. Therefore, unnecessary VM migrations are generated and the rate of Service Level Agreement (SLA) violations are increased in data centers. To address this problem, our VM consolidation method which is formulated as a bin-packing problem considers both the current and future utilization of resources. The future utilization of resources is accurately predicted using a k-nearest neighbor regression based model. In this paper, we investigate the effectiveness of VM and host resource utilization predictions in the VM consolidation task using real workload traces. The experimental results show that our approach provides substantial improvement over other heuristic algorithms in reducing energy consumption, number of VM migrations and number of SLA violations. {\textcopyright} 2015 IEEE.},
author = {Farahnakian, Fahimeh and Pahikkala, Tapio and Liljeberg, Pasi and Plosila, Juha and Tenhunen, Hannu},
doi = {10.1109/CLOUD.2015.58},
isbn = {9781467372879},
journal = {Proceedings - 2015 IEEE 8th International Conference on Cloud Computing, CLOUD 2015},
keywords = {Bin-paking,Dynamic VM consolidation,Energy-efficiency,SLA,Utilization prediction model},
pages = {381--388},
title = {{Utilization Prediction Aware VM Consolidation Approach for Green Cloud Computing}},
year = {2015}
}
@article{Beloglazov:2013ht,
abstract = {Dynamic consolidation of virtual machines (VMs) is an effective way to improve the utilization of resources and energy efficiency in cloud data centers. Determining when it is best to reallocate VMs from an overloaded host is an aspect of dynamic VM consolidation that directly influences the resource utilization and quality of service (QoS) delivered by the system. The influence on the QoS is explained by the fact that server overloads cause resource shortages and performance degradation of applications. Current solutions to the problem of host overload detection are generally heuristic based, or rely on statistical analysis of historical data. The limitations of these approaches are that they lead to suboptimal results and do not allow explicit specification of a QoS goal. We propose a novel approach that for any known stationary workload and a given state configuration optimally solves the problem of host overload detection by maximizing the mean intermigration time under the specified QoS goal based on a Markov chain model. We heuristically adapt the algorithm to handle unknown nonstationary workloads using the Multisize Sliding Window workload estimation technique. Through simulations with workload traces from more than a thousand PlanetLab VMs, we show that our approach outperforms the best benchmark algorithm and provides approximately 88 percent of the performance of the optimal offline algorithm. {\textcopyright} 1990-2012 IEEE.},
author = {Beloglazov, Anton and Buyya, Rajkumar},
doi = {10.1109/TPDS.2012.240},
isbn = {2012050506},
issn = {10459219},
journal = {IEEE Transactions on Parallel and Distributed Systems},
keywords = {Distributed systems,cloud computing,dynamic consolidation,energy efficiency,host overload detection,virtualization},
number = {7},
pages = {1366--1379},
title = {{Managing overloaded hosts for dynamic consolidation of virtual machines in cloud data centers under quality of service constraints}},
volume = {24},
year = {2013}
}
@article{Han:2012vz,
author = {{Yike Guo} and {Rui Han} and {Moustafa M. Ghanem} and {Li Guo}},
doi = {doi.ieeecomputersociety.org/10.1109/CCGrid.2012.52},
journal = {Cluster Computing and the Grid, IEEE International Symposium on},
pages = {644--651},
title = {{Lightweight Resource Scaling for Cloud Applications}},
volume = {00},
year = {2012}
}
@article{Pahl:2017eo,
author = {Pahl, Claus and Brogi, Antonio and Soldani, Jacopo and Jamshidi, Pooyan},
doi = {10.1109/TCC.2017.2702586},
issn = {2168-7161},
journal = {IEEE Transactions on Cloud Computing},
keywords = {atic literature review,cloud,cluster,container,orchestration,system-,systematic mapping study},
number = {c},
pages = {1--1},
title = {{Cloud Container Technologies: a State-of-the-Art Review}},
url = {http://ieeexplore.ieee.org/document/7922500/},
volume = {7161},
year = {2017}
}
@article{Gong:2010fr,
abstract = {Cloud systems require elastic resource allocation to minimize resource provisioning costs while meeting service level objectives (SLOs). In this paper, we present a novel PRedictive Elastic reSource Scaling (PRESS) scheme for cloud systems. PRESS unobtrusively extracts fine-grained dynamic patterns in application resource demands and adjust their resource allocations automatically. Our approach leverages light-weight signal processing and statistical learning algorithms to achieve online predictions of dynamic application resource requirements. We have implemented the PRESS system on Xen and tested it using RUBiS and an application load trace from Google. Our experiments show that we can achieve good resource prediction accuracy with less than 5{\%} over-estimation error and near zero under-estimation error, and elastic resource scaling can both significantly reduce resource waste and SLO violations.},
author = {Gong, Zhenhuan and Gu, Xiaohui and Wilkes, John},
doi = {10.1109/CNSM.2010.5691343},
isbn = {9781424489084},
journal = {Proceedings of the 2010 International Conference on Network and Service Management, CNSM 2010},
pages = {9--16},
title = {{PRESS: PRedictive Elastic reSource Scaling for cloud systems}},
year = {2010}
}
@article{Gao:2013gg,
author = {Gao, Yongqiang and Guan, Haibing and Qi, Zhengwei and Hou, Yang and Liu, Liang},
doi = {10.1016/j.jcss.2013.02.004},
issn = {0022-0000},
journal = {Journal of Computer and System Sciences},
keywords = {ant colony optimization,multi-objective optimization,virtual machine placement},
number = {8},
pages = {1--13},
title = {{Journal of Computer and System Sciences A multi-objective ant colony system algorithm for virtual machine placement in cloud computing}},
url = {http://dx.doi.org/10.1016/j.jcss.2013.02.004},
volume = {1},
year = {2013}
}
@article{Lin:2015ix,
abstract = {Virtual machines hosted in virtualized data centers are important providers of computational resources in the era of cloud computing. Efficient scheduling of data centers' virtual machines can reduce the number of physical servers needed to host the virtual machines and, in turn, reduce the energy and other capital costs for maintaining the virtualized data centre. In this paper, we propose an innovative approach to achieve efficient pro-active VM scheduling. Our approach uses a multi-capacity bin packing technique that efficiently places VMs onto physical servers. We use time-series analysis techniques to extract not only low frequency information about future VM workloads but also high frequency information for VM workload correlations. We show that the proposed algorithms mathematically guarantee the VM scheduling meets the Service Level Objectives (SLO) and, moreover, guarantee statistically that the desired success probability of the SLO is met. Evaluation of our technique on production (real) workloads shows that our approach reduces by up to 15{\%} the number of physical machines. We also see improvements of up to 18{\%} for production workloads in machine utilization.},
author = {Lin, Hao and Qi, Xin and Yang, Shuo and Midkiff, Samuel},
doi = {10.1109/IPDPS.2015.90},
isbn = {9781479986484},
issn = {1530-2075},
journal = {Proceedings - 2015 IEEE 29th International Parallel and Distributed Processing Symposium, IPDPS 2015},
keywords = {Cloud computing,Scheduling,VM consolidation and provisioning},
pages = {207--216},
title = {{Workload-Driven VM Consolidation in Cloud Data Centers}},
year = {2015}
}
@article{Guzek:2015ds,
abstract = {Cloud computing is significantly reshaping the computing industry. Individuals and small organizations can benefit from using state-of-the-art services and infrastructure, while large companies are attracted by the flexibility and the speed with which they can obtain the services. Service providers compete to offer the most attractive conditions at the lowest prices. However, the environmental impact and legal aspects of cloud solutions pose additional challenges. Indeed, the new cloud-related techniques for resource virtualization and sharing and the corresponding service level agreements call for new optimization models and solutions. It is important for computational intelligence researchers to understand the novelties introduced by cloud computing. The current survey highlights and classifies key research questions, the current state of the art, and open problems.},
archivePrefix = {arXiv},
arxivId = {1209.5467},
author = {Guzek, Mateusz and Bouvry, Pascal and Talbi, El Ghazali},
doi = {10.1109/MCI.2015.2405351},
eprint = {1209.5467},
isbn = {0957-4174},
issn = {1556603X},
journal = {IEEE Computational Intelligence Magazine},
number = {2},
pages = {53--67},
pmid = {1000102567},
title = {{A survey of evolutionary computation for resource management of processing in cloud computing [review article]}},
volume = {10},
year = {2015}
}
@inproceedings{Shafer:2010vh,
abstract = {Cloud computing is gaining popularity as a way to virtualize the datacenter and increase flexibility in the use of computation resources. This type of system is best exemplified by Amazon's Elastic Compute Cloud and related products. Recently, a new open-source framework called Eucalyptus has been released that allows users to create private cloud computing grids that are API-compatible with the existing Amazon standards. Eucalyptus leverages existing virtualization technology (the KVM or Xen hypervisors) and popular Linux distributions. Through the use of automated scripts provided with Ubuntu, a private cloud can be installed, from scratch, in under 30 minutes. Here, Eucalyptus is tested using I/O intensive applications in order to determine if its performance is as good as its ease-of-use. Unfortunately, limitations in commodity I/O virtualization technology restrict the out-of-the-box storage bandwidth to 51{\%} and 77{\%} of a non-virtualized disk for writes and reads, respectively. Similarly, out-of-the-box network bandwidth to another host is only 71{\%} and 45{\%} of non-virtualized performance for transmit and receive workloads, respectively. These bottlenecks are present even on a test system massively over-provisioned in both memory and computation resources. Similar restrictions are also evident in commercial clouds provided by Amazon, showing that even after much research effort I/O virtualization bottlenecks still challenge the designers of modern systems.},
author = {Shafer, J},
booktitle = {Proceedings of the 2nd conference on I/O virtualization},
pages = {5--5},
title = {{I/O virtualization bottlenecks in cloud computing today}},
url = {http://li46-224.members.linode.com/publications/papers/shafer-wiov2010.pdf},
year = {2010}
}

@INPROCEEDINGS{6681297, 
author={S. Iturriaga and S. Nesmachnow and B. Dorronsoro and E. G. Talbi and P. Bouvry}, 
booktitle={2013 Eighth International Conference on P2P, Parallel, Grid, Cloud and Internet Computing}, 
title={A Parallel Hybrid Evolutionary Algorithm for the Optimization of Broker Virtual Machines Subletting in Cloud Systems}, 
year={2013}, 
volume={}, 
number={}, 
pages={594-599}, 
keywords={cloud computing;evolutionary computation;greedy algorithms;parallel algorithms;simulated annealing;virtual machines;broker virtual machines;cloud providers;cloud systems;distributed subpopulations model;greedy heuristics;parallel hybrid evolutionary algorithm;pre-booked resources;simulated annealing operator;Algorithm design and analysis;Cloud computing;Evolutionary computation;Optimization;Sociology;Statistics;Virtual machining;cloud computing;parallel evolutionary algorithms;scheduling}, 
doi={10.1109/3PGCIC.2013.103}, 
ISSN={}, 
month={Oct},}

@article{Tsai:2013fn,
author = {Tsai, Jinn-Tsong and Fang, Jia-Cen and Chou, Jyh-Horng},
doi = {10.1016/j.cor.2013.06.012},
isbn = {8868721503},
issn = {03050548},
journal = {Computers {\&} Operations Research},
month = {dec},
number = {12},
pages = {3045--3055},
title = {{Optimized task scheduling and resource allocation on cloud computing environment using improved differential evolution algorithm}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S030505481300169X},
volume = {40},
year = {2013}
}

@incollection{Piraghaj:gb,
author = {Piraghaj, Sareh Fotuhi and Dastjerdi, Amir Vahid and Calheiros, Rodrigo N. and Buyya, Rajkumar},
booktitle = {IGI Global},
doi = {10.4018/978-1-5225-0759-8.ch017},
isbn = {9781522507604},
pages = {410--454},
publisher = {IGI Global},
title = {{A Survey and Taxonomy of Energy Efficient Resource Management Techniques in Platform as a Service Cloud}},
year = {2016}
}

@article{Jayamohan:2004kq,
author = {Jayamohan, M.S and Rajendran, Chandrasekharan},
doi = {10.1016/S0377-2217(03)00204-2},
isbn = {9144225784},
issn = {03772217},
journal = {European Journal of Operational Research},
keywords = {Dispatching rules,Job shop,Scheduling,Weighted flowtime,Weighted tardiness},
number = {2},
pages = {307--321},
title = {{Development and analysis of cost-based dispatching rules for job shop scheduling}},
url = {http://www.sciencedirect.com/science/article/pii/S0377221703002042},
volume = {157},
year = {2004}
}

@article{Branke:2016wy,
author = {Branke, Juergen and Nguyen, Su and Pickardt, Christoph and Zhang, Mengjie},
doi = {10.1109/TEVC.2015.2429314},
isbn = {1089778X},
issn = {1089-778X},
journal = {IEEE Transactions on Evolutionary Computation},
number = {X},
pages = {1--1},
title = {{Automated Design of Production Scheduling Heuristics: A Review}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7101236},
volume = {X},
year = {2015}
}
@article{Zhou:2009dq,
author = {Zhou, H and Cheung, W and Leung, L C},
doi = {10.1016/j.ejor.2007.10.063},
isbn = {0377-2217},
issn = {03772217 (ISSN)},
journal = {European Journal of Operational Research},
keywords = {Algorithms,Flow times,Genetic algorithm,Genetic algorithms,Heuristic algorithms,Heuristic methods,Heuristics,Hybrid Genetic algorithms,Hybrid frameworks,Job shops,Job-shop scheduling problems,Makespan,NP-hard,New generations,Nuclear propulsion,Numerical experiments,Parallel evolutions,Random walks,Reinforcement learning,Scheduling,Scheduling problems,Scheduling/production,Set-ups,Shop scheduling,Total tardinesses,Total weighted tardinesses,Weighted tardinesses,With or without},
month = {may},
number = {3},
pages = {637--649},
title = {{Minimizing weighted tardiness of job-shop scheduling using a hybrid genetic algorithm}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-55149114608{\&}partnerID=40{\&}md5=988022acef4c5fea79bbe9453bb57667},
volume = {194},
year = {2009}
}
@article{AmaralArmentano:2000dk,
abstract = {This paper presents a tabu search approach to minimize total tardiness for the job shop scheduling problem. The method uses dispatching rules to obtain an initial solution and searches for new solutions in a neighborhood based on the critical paths of the jobs. Diversification and intensification strategies are suggested. For small problems the solutions' quality is evaluated against optimal solution values and for large problems the tabu search performance is compared with two heuristics proposed in the literature.},
author = {{Amaral Armentano}, Vin{\'{i}}cius and {Rig{\~{a}}o Scrich}, Cintia},
doi = {10.1016/S0925-5273(99)00014-6},
issn = {09255273},
journal = {International Journal of Production Economics},
month = {jan},
number = {2},
pages = {131--140},
title = {{Tabu search for minimizing total tardiness in a job shop}},
volume = {63},
year = {2000}
}
@article{Potts:2009eb,
abstract = {Scheduling has become a major field within operational research with several hundred publications appearing$\backslash$r$\backslash$neach year. This paper explores the historical development of the subject since the mid-1950s when the landmark$\backslash$r$\backslash$npublications started to appear. A discussion of the main topics of scheduling research for the past five decades$\backslash$r$\backslash$nis provided, highlighting the key contributions that helped shape the subject. The main topics covered in the$\backslash$r$\backslash$nrespective decades are combinatorial analysis, branch and bound, computational complexity and classification,$\backslash$r$\backslash$napproximate solution algorithms and enhanced scheduling models.},
author = {Potts, Chris N. and Strusevich, Vitaly a.},
doi = {10.1057/jors.2009.2},
isbn = {01605682},
issn = {0160-5682},
journal = {Journal of the Operational Research Society},
keywords = {T Technology (General)},
mendeley-groups = {proposal},
number = {S1},
pages = {S41--S68},
title = {{Fifty years of scheduling: a survey of milestones}},
url = {http://www.palgrave-journals.com/jors/journal/v60/ns1/abs/jors20092a.html},
volume = {60},
year = {2009}
}

@article{Laredo:2014db,
author = {Laredo, J. L. J. and Bouvry, P. and Guinand, F. and Dorronsoro, B. and Fernandes, C.},
doi = {10.1007/s10586-013-0328-x},
issn = {1386-7857},
journal = {Cluster Computing},
keywords = {optimization,scheduling,self-organization},
number = {2},
pages = {191--204},
title = {{The sandpile scheduler:How self-organized criticalitymay lead to dynamic load-balancing}},
url = {http://link.springer.com/10.1007/s10586-013-0328-x},
volume = {17},
year = {2014}
}

@article{Kousiouris:2013vg,
author = {Kousiouris, George and Menychtas, Andreas and Kyriazis, Dimosthenis and Konstanteli, Kleopatra and Gogouvitis, Spyridon V. and Katsaros, Gregory and Varvarigou, Theodora A.},
doi = {10.1109/TSC.2012.21},
isbn = {1939-1374 VO  - 6},
issn = {19391374},
journal = {IEEE Transactions on Services Computing},
keywords = {Service-oriented infrastructures,artificial neural networks,genetic algorithms,parametric services,performance analysis,performance estimation,quality of service,workload forecasting},
number = {4},
pages = {511--524},
title = {{Parametric design and performance analysis of a decoupled service-oriented prediction framework based on embedded numerical software}},
volume = {6},
year = {2013}
}

@book{Tan2016,
abstract = {{\textcopyright} Springer International Publishing Switzerland 2016. In recent years, Web services technology is becoming increasingly popular because of the convenience, low cost and capacity to be composed into high-level business processes. The service location-allocation problem for a Web service provider is critical and urgent, because some factors such as network latency can make serious effect on the quality of service (QoS). This paper presents a multi-objective optimization algorithm based on NSGA-II to solve the service location-allocation problem. A stimulated experiment is conducted using the WS-DREAM dataset. The results are compared with a single objective genetic algorithm (GA). It shows NSGA-II based algorithm can provide a set of best solutions that outperforms genetic algorithm.},
author = {Tan, Boxiong and Ma, Hui and Zhang, Mengjie},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-28270-1_21},
isbn = {9783319282695},
issn = {16113349},
mendeley-groups = {proposal,TSC},
pages = {246--257},
title = {{Optimization of Location Allocation of Web Services Using a Modified Non-dominated Sorting Genetic Algorithm}},
url = {http://link.springer.com/10.1007/978-3-319-28270-1{\_}21},
volume = {9592},
year = {2016}
}
Maximus • Now
Maximus Tann (maximus.tann@gmail.com, maximustann@gmail.com)

Send a message



@book{Tan2017,
abstract = {{\textcopyright} Springer International Publishing AG 2017. Web services are independently programmable application components which scatter over the Internet. Network latency is one of the major concerns of web service application. Thus, physical locations of web services and users should be taken into account for web service composition. In this paper, we propose a new solution based on the modified binary PSO-based (MBPSO) approach which employs an adaptive inertia technique to allocating web service locations. Although several heuristic approaches have been proposed for web service location-allocation, to our best knowledge, this is the first time applying PSO to solve the problem. A simulated experiment is done using the WS-DREAM dataset with five different complexities. To compare with genetic algorithm and original binary PSO approaches, the proposed MBPSO approach has advantages in most situations.},
author = {Tan, Boxiong and Huang, Hai and Ma, Hui and Zhang, Mengjie},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-51691-2_31},
isbn = {9783319516905},
issn = {16113349},
mendeley-groups = {proposal},
pages = {366--377},
title = {{Binary PSO for Web Service Location-Allocation}},
url = {http://link.springer.com/10.1007/978-3-319-51691-2{\_}31},
volume = {10142 LNAI},
year = {2017}
}


@Article{Yao2006,
author="Yao, Xin
and Xu, Yong",
title="Recent Advances in Evolutionary Computation",
journal="Journal of Computer Science and Technology",
year="2006",
month="Jan",
day="01",
volume="21",
number="1",
pages="1--18",
abstract="Evolutionary computation has experienced a tremendous growth in the last decade in both theoretical analyses and industrial applications. Its scope has evolved beyond its original meaning of ``biological evolution'' toward a wide variety of nature inspired computational algorithms and techniques, including evolutionary, neural, ecological, social and economical computation, etc., in a unified framework. Many research topics in evolutionary computation nowadays are not necessarily ``evolutionary''. This paper provides an overview of some recent advances in evolutionary computation that have been made in CERCIA at the University of Birmingham, UK. It covers a wide range of topics in optimization, learning and design using evolutionary approaches and techniques, and theoretical results in the computational time complexity of evolutionary algorithms. Some issues related to future development of evolutionary computation are also discussed.",
issn="1860-4749",
doi="10.1007/s11390-006-0001-4",
url="https://doi.org/10.1007/s11390-006-0001-4"
}



@article{Candeia:2010wt,
abstract = {The new ways of doing science, rooted on the unprecedented processing, communication and storage infrastructures that became available to scientists, are collectively called e-Science. Many research labs now need non-trivial computational power to run e-Science applications. Grid and voluntary computing are well-established solutions that cater to this need, but are not accessible for all labs and institutions. Besides, there is an uncertainty about the future amount of resources that will be available in such infrastructures, which prevents the researchers from planning their activities to guarantee that deadlines will be met. With the emergence of the cloud computing paradigm come new opportunities. One possibility is to run e-Science activities at resources acquired on-demand from cloud providers. However, although very low, there is a cost associated with the usage of cloud resources. Besides that, the amount of resources that can be simultaneously acquired is, in practice, limited. Another possibility is the not new idea of composing hybrid infrastructures in which the huge amount of computational resources shared by the grid infrastructures are used whenever possible and extra capacity is acquired from cloud computing providers. We here investigate how to schedule e-Science activities in such hybrid infrastructures so that deadlines are met and costs are reduced.},
author = {Candeia, David and Ara{\'{u}}jo, Ricardo and Lopes, Raquel and Brasileiro, Francisco},
doi = {10.1109/CloudCom.2010.67},
isbn = {9780769543024},
journal = {Proceedings - 2nd IEEE International Conference on Cloud Computing Technology and Science, CloudCom 2010},
pages = {343--350},
title = {{Investigating business-driven cloudburst schedulers for e-science bag-of-tasks applications}},
year = {2010}
}
@booklet{Kaplan:up01fR-k,
author = {Kaplan, James M and Forrest, William and Kindler, Noah},
booktitle = {Technical Report, McKinseyCo.},
title = {{Revolutionizing data center energy efficiency}},
year = {2008}
}
@article{Yin:2000bt,
author = {Yin, Yafeng},
journal = {Journal of Transportation Engineering},
number = {April},
pages = {115--120},
title = {{Genetic algorithm based approach for bilevel programming models}},
volume = {126},
year = {2000}
}
@book{CoffmanJr:1996ui,
abstract = {The classical one-dimensional bin packing problems has long served as a proving ground for new approaches to the analysis of approximation algorithms. In the early 1970's it was one of the first combinatorical probelms for which the idea of worst-case performance guarantees was investigated. It was also in this domain that the idea of proving lower bounds on the performance of online algorithms was first developed, and it is here that the probabilistic analysis of approximation algortithms has truly flowered. This chapter surveys the literature on worst-case and average-case behavior of approximation algorithms for one-dimensional bin packing, using each type of analysis to put the other in perspective.},
author = {Jr, Eg Coffman and Garey, M.R. and Johnson, Ds},
booktitle = {Approximation algorithms for {\ldots}},
doi = {10.1007/978-1-4419-7997-1_35},
isbn = {0-534-94968-1},
issn = {1098-6596},
month = {Aug},
number = {m},
pages = {1--53},
pmid = {25246403},
publisher = {PWS Publishing Co.},
title = {{Approximation algorithms for bin packing: A survey}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:APPROXIMATION+ALGORITHMS+FOR+BIN+PACKING+:+A+SURVEY{\#}0{\%}5Cnhttp://dl.acm.org/citation.cfm?id=241940},
year = {1996}
}
@inproceedings{Svard:2015ic,
abstract = {Efficient mapping of Virtual Machines{\~{}}(VMs) onto physical servers is a key problem for cloud infrastructure providers as hardware utilization directly impacts profit. Today, this mapping is commonly only performed when new VMs are created, but as VM workloads fluctuate and server availability varies, any initial mapping is bound to become suboptimal over time. We introduce a set of heuristic methods for continuous optimization of the VM-to-server mapping based on combinations of fundamental management actions, namely suspending and resuming physical machines, migrating VMs, and suspending and resuming VMs. By using these methods, cloud infrastructure providers can continuously optimize their server resources regardless of the predictability of the workload. To verify that our approach is applicable in real-world scenarios, we build a proof-of-concept datacenter management system that implements the proposed algorithms. The feasibility of our approach is evaluated through a combination of simulations and real experiments where our system provisions a workload of benchmark applications. Our results indicate that the proposed algorithms are feasible, that the combined management approach achieves the best results, and that the VM suspend and resume mechanism has the largest impact on provider profit.},
author = {Sv{\"{a}}rd, Petter and Li, Wubin and Wadbro, Eddie and Tordsson, Johan and Elmroth, Erik},
booktitle = {Proceedings - IEEE 7th International Conference on Cloud Computing Technology and Science, CloudCom 2015},
doi = {10.1109/CloudCom.2015.11},
isbn = {9781467395601},
keywords = {Cloud Computing,Consolidation,Heuristic Methods,Power Management,Scheduling,VM Migration},
pages = {387--396},
publisher = {IEEE},
title = {{Continuous datacenter consolidation}},
year = {2016}
}
@article{Wei:2010fn,
abstract = {Service-oriented computing and cloud computing have a reciprocal relationship - one provides computing of services and the other provides services of computing. Although service-oriented computing in cloud computing environments presents a new set of research challenges, the authors believe the combination also provides potentially transformative opportunities.},
author = {Wei, Yi and Blake, M.B.},
doi = {10.1109/MIC.2010.147},
issn = {1089-7801},
journal = {IEEE Internet Computing},
number = {6},
pages = {72--75},
title = {{Service-Oriented Computing and Cloud Computing: Challenges and Opportunities}},
url = {http://ieeexplore.ieee.org/xpl/login.jsp?tp={\&}arnumber=5617062{\&}url=http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5617062},
volume = {14},
year = {2010}
}
@article{Dosa:2013ie,
abstract = {In the bin packing problem we are given an instance consisting of a sequence of items with sizes between 0 and 1. The objective is to pack these items into the smallest possible number of bins of unit size. FirstFit algorithm packs each item into the first bin where it fits, possibly opening a new bin if the item cannot fit into any currently open bin. In early seventies it was shown that the asymptotic approximation ratio of FirstFit bin packing is equal to 1.7. We prove that also the absolute approximation ratio for FirstFit bin packing is exactly 1.7. This means that if the optimum needs OPT bins, FirstFit always uses at most $\backslash$lfloor 1.7 OPT $\backslash$rfloor bins. Furthermore we show matching lower bounds for a majority of values of OPT, i.e., we give instances on which FirstFit uses exactly $\backslash$lfloor 1.7 OPT $\backslash$rfloor bins. Such matching upper and lower bounds were previously known only for finitely many small values of OPT. The previous published bound on the absolute approximation ratio of FirstFit was 12/7 $\backslash$approx 1.7143. Recently a bound of 101/59 $\backslash$approx 1.7119 was claimed.},
author = {{Gy{\"{o}}rgy D{\'{o}}sa}, and Jiř{\'{i}} Sgall},
doi = {10.4230/LIPIcs.xxx.yyy.p},
isbn = {9783939897507},
issn = {18688969},
journal = {{\ldots} on Theoretical Aspects of Computer Science {\ldots}},
keywords = {4230,and phrases approximation algorithms,bin packing,digital object identifier 10,first fit,lipics,online algorithms,p,xxx,yyy},
pages = {1--15},
title = {{First Fit bin packing: A tight analysis}},
url = {http://drops.dagstuhl.de/opus/volltexte/2013/3963/},
volume = {i},
year = {2013}
}
@article{Sindelar:2011bu,
abstract = {Virtualization technology enables multiple virtual machines (VMs) to run on a single physical server. VMs that run on the same physical server can share memory pages that have identical content, thereby reducing the overall memory requirements on the server. We develop sharing-aware algorithms that can colocate VMs with similar page content on the same physical server to optimize the benefits of inter-VM sharing. We show that inter-VM sharing occurs in a largely hierarchical fashion, where the sharing can be attributed to VM's running the same OS platform, OS version, software libraries, or applications. We propose two hierarchical sharing models: a tree model and a more general cluster-tree model. Using a set of VM traces, we show that up to 67{\%} percent of the inter-VM sharing is captured by the tree model and up to 82{\%} is captured by the cluster-tree model. Next, we study two problem variants of critical interest to a virtualization service provider: the VM Maximization problem that determines the most profitable subset of the VMs that can be packed into the given set of servers, and the VM packing problem that determines the smallest set of servers that can accommodate a set of VMs. While both variants are NP-hard, we show that both admit provably good approximation schemes in the hierarchical sharing models. We show that VM maximization for the tree and cluster-tree models can be approximated in polytime to within a (1 - 1/e) factor of optimal. Further, we show that VM packing can be approximated in polytime to within a factor of O(log n) of optimal for cluster-trees and to within a factor of 3 of optimal for trees, where n is the number of VMs. Finally, we evaluate our VM packing algorithm for the tree sharing model on real-world VM traces and show that our algorithm can exploit most of the available inter-VM sharing to achieve a 32{\%} to 50{\%} reduction in servers and a 25{\%} to 57{\%} reduction in memory footprint compared to sharing-oblivious algorithms.},
author = {Sindelar, Michael and Sitaraman, Ramesh K. and Shenoy, Prashant},
doi = {10.1145/1989493.1989554},
isbn = {9781450307437},
journal = {Proceedings of the 23rd ACM symposium on Parallelism in algorithms and architectures - SPAA '11},
pages = {367},
title = {{Sharing-aware algorithms for virtual machine colocation}},
url = {http://portal.acm.org/citation.cfm?doid=1989493.1989554},
year = {2011}
}
@article{Back:1997gb,
author = {B{\"{a}}ck, Thomas and Hammel, U and Schwefel, H.-P.},
doi = {10.1109/4235.585888},
issn = {1089778X},
journal = {IEEE Trans. Evolutionary Computation},
keywords = {PHD,classi er,evolution strategies,evolution-,evolutionary computation,evolutionary program-,genetic algorithms,genetic programming,his rst issue of,ming,octobre,systems,the ieee transactions on},
number = {1},
pages = {3--17},
title = {{Evolutionary computation: comments on the history and current state}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=585888},
volume = {1},
year = {1997}
}
@article{Moens:2011gk,
abstract = {As the requirements and scale of cloud environments increase, scalable management of the cloud is needed. Centralized solutions lack scalability and fully distributed management systems only have a limited overview of the system. One of the often-studied problems in cloud environments is the application placement problem, used to decide where application instances are instantiated and how many resources to allocate to the instances. In this paper a general approach is introduced for using centralized cloud resource management algorithms in a hierarchical context, increasing the scalability of the management system while maintaining a high placement quality. The management system itself is executed on the cloud, further increasing scalability and robustness. The proposed method uses aggregation techniques to generate input values for a centralized application placement algorithm which is run in all management nodes. Decoupling ensures management nodes can function independently. Subsequently, we compare the performance of hierarchical application placement method with that of a fully centralized algorithm. The results show that a solution, within 5{\%} of the optimum placement when using the centralized algorithm, can be achieved hierarchically in less than 25{\%} of the time needed for execution of the centralized algorithm.},
author = {Moens, Hendrik and Famaey, Jeroen and Latr??, Steven and Dhoedt, Bart and {De Turck}, Filip},
doi = {10.1109/INM.2011.5990684},
isbn = {9781424492213},
journal = {Proceedings of the 12th IFIP/IEEE International Symposium on Integrated Network Management, IM 2011},
keywords = {Clouds,Distributed computing,Hierarchical systems},
pages = {137--144},
title = {{Design and evaluation of a hierarchical application placement algorithm in large scale clouds}},
year = {2011}
}
@inproceedings{Energy_4,
abstract = {Power delivery, electricity consumption, and heat management are becoming key challenges in data center environments. Several past solutions have individually evaluated different techniques to address separate aspects of this problem, in hardware and software, and at local and global levels. Unfortunately, there has been no corresponding work on coordinating all these solutions. In the absence of such coordination, these solutions are likely to interfere with one another, in unpredictable (and potentially dangerous) ways. This paper seeks to address this problem. We make two key contributions. First, we propose and validate a power management solution that coordinates different individual approaches. Using simulations based on 180 server traces from nine different real-world enterprises, we demonstrate the correctness, stability, and efficiency advantages of our solution. Second, using our unified architecture as the base, we perform a detailed quantitative sensitivity analysis and draw conclusions about the impact of different architectures, implementations, workloads, and system design choices.},
author = {Raghavendra, Ramya and Ranganathan, Parthasarathy and Talwar, Vanish and Wang, Zhikui and Zhu, Xiaoyun},
booktitle = {Solutions},
doi = {http://doi.acm.org/10.1145/1346281.1346289},
isbn = {9781595939586},
issn = {01635980},
keywords = {capping,control theory,coordination,data center,efficiency,power management,virtualization},
organization = {ACM},
pages = {48--59},
title = {{No “ Power ” Struggles : Coordinated Multi-level Power Management for the Data Center}},
url = {http://portal.acm.org/citation.cfm?id=1346289},
volume = {36},
year = {2008}
}
@article{Ganesan:2012eb,
abstract = {Sharing of physical infrastructure using virtualization presents an opportunity to improve the overall resource utilization. It is extremely important for a Software as a Service (SaaS) provider to understand the characteristics of the business application workload in order to size and place the virtual machine (VM) containing the application. A typical business application has a multi-tier architecture and the application workload is often predictable. Using the knowledge of the application architecture and statistical analysis of the workload, one can obtain an appropriate capacity and a good placement strategy for the corresponding VM. In this paper we propose a tool iCirrus-WoP that determines VM capacity and VM collocation possibilities for a given set of application workloads. We perform an empirical analysis of the approach on a set of business application workloads obtained from geographically distributed data centers. The iCirrus-WoP tool determines the fixed reserved capacity and a shared capacity of a VM which it can share with another collocated VM. Based on the workload variation, the tool determines if the VM should be statically allocated or needs a dynamic placement. To determine the collocation possibility, iCirrus-WoP performs a peak utilization analysis of the workloads. The empirical analysis reveals the possibility of collocating applications running in different time-zones. The VM capacity that the tool recommends, show a possibility of improving the overall utilization of the infrastructure by more than 70{\%} if they are appropriately collocated. {\textcopyright} 2012 IEEE.},
author = {Ganesan, Rajeshwari and Sarkar, Santonu and Narayan, Akshay},
doi = {10.1109/CLOUD.2012.73},
isbn = {9780769547558},
issn = {2159-6182},
journal = {Proceedings - 2012 IEEE 5th International Conference on Cloud Computing, CLOUD 2012},
keywords = {IaaS,SaaS,Virtual machine,data correlation,peak to mean ratio,placement,reserved capacity,shared capacity,sizing,static allocation,workload},
pages = {868--875},
title = {{Analysis of SaaS business platform workloads for sizing and collocation}},
year = {2012}
}
@article{Hall:2014db,
abstract = {In the era of social media there are now many different ways that a scientist can build their public profile; the publication of high-quality scientific papers being just one. While social media is a valuable tool for outreach and the sharing of ideas, there is a danger that this form of communication is gaining too high a value and that we are losing sight of key metrics of scientific value, such as citation indices. To help quantify this, I propose the 'Kardashian Index', a measure of discrepancy between a scientist's social media profile and publication record based on the direct comparison of numbers of citations and Twitter followers.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hall, Neil},
doi = {10.1186/s13059-014-0424-0},
eprint = {arXiv:1011.1669v3},
isbn = {1465-6906},
issn = {1474-760X},
journal = {Genome Biology},
number = {7},
pages = {424},
pmid = {25315513},
title = {{The Kardashian index: a measure of discrepant social media profile for scientists}},
url = {http://genomebiology.biomedcentral.com/articles/10.1186/s13059-014-0424-0},
volume = {15},
year = {2014}
}
@article{Wang:2008kb,
author = {Wang, Guangmin and Wan, Zhongping and Wang, Xianjia and Lv, Yibing},
doi = {10.1016/j.camwa.2008.05.006},
journal = {Computers {\&} Mathematics with Applications},
keywords = {the linear-quadratic bilevel programming},
month = {Nov},
number = {10},
pages = {2550--2555},
title = {{Genetic algorithm based on simplex method for solving linear-quadratic bilevel programming problem {\$}}},
volume = {56},
year = {2008}
}
@article{Adhikari:2012uq,
abstract = {Netflix is the leading provider of on-demand Internet video streaming in the US and Canada, accounting for 29.7{\%} of the peak downstream traffic in US. Understanding the Netflix architecture and its performance can shed light on how to best optimize its design as well as on the design of similar on-demand streaming services. In this paper, we perform a measurement study of Netflix to uncover its architecture and service strategy. We find that Netflix employs a blend of data centers and Content Delivery Networks (CDNs) for content distribution. We also perform active measurements of the three CDNs employed by Netflix to quantify the video delivery bandwidth available to users across the US. Finally, as improvements to Netflix's current CDN assignment strategy, we propose a measurement-based adaptive CDN selection strategy and a multiple-CDN-based video delivery strategy, and demonstrate their potentials in significantly increasing user's average bandwidth.},
author = {Adhikari, Vijay Kumar and Guo, Yang and Hao, Fang and Varvello, Matteo and Hilt, Volker and Steiner, Moritz and Zhang, Zhi Li},
doi = {10.1109/INFCOM.2012.6195531},
isbn = {9781467307758},
issn = {0743166X},
journal = {Proceedings - IEEE INFOCOM},
pages = {1620--1628},
title = {{Unreeling netflix: Understanding and improving multi-CDN movie delivery}},
year = {2012}
}
@inproceedings{Energy_8,
author = {Ghribi, Chaima and Hadji, Makhlouf and Zeghlache, Djamal},
booktitle = {Cluster, Cloud and Grid Computing (CCGrid), 2013 13th IEEE/ACM International Symposium on},
doi = {10.1109/CCGrid.2013.89},
number = {May},
organization = {IEEE},
pages = {671--678},
title = {{Energy Efficient VM Scheduling for Cloud Data Centers : Exact Allocation and Migration Algorithms Energy Efficient VM Scheduling for Cloud Data Centers : Exact allocation and migration algorithms}},
year = {2013}
}
@article{Jennings:2015ht,
annote = {This survey helps me define my research scope and 
highlight some challenges in the field. For detail notes, placement read the attachments and the hard copy.

The major problem in this paper is that, it identifies the scalability problem, however, it does not defines the term "scalability" clearly.},
author = {Jennings, B and Stadler, R},
journal = {Journal of Network and Systems Management},
number = {3},
pages = {567--619},
title = {{Resource management in clouds: Survey and research challenges}},
url = {http://link.springer.com/article/10.1007/s10922-014-9307-7},
volume = {23},
year = {2014}
}
@inproceedings{Service_dataset,
abstract = {With the increasing popularity of cloud computing as a solution for building high-quality applications on distributed components, efficiently evaluating user-side quality of cloud components becomes an urgent and crucial research problem. However, invoking all the available cloud components from user-side for evaluation purpose is expensive and impractical. To address this critical challenge, we propose a neighborhood-based approach, called CloudPred, for collaborative and personalized quality prediction of cloud components. CloudPred is enhanced by feature modeling on both users and components. Our approach CloudPred requires no additional invocation of cloud components on behalf of the cloud application designers. The extensive experimental results show that CloudPred achieves higher QoS prediction accuracy than other competing methods. We also publicly release our large-scale QoS dataset for future related research in cloud computing.},
author = {Zhang, Yilei and Zheng, Zibin and Lyu, Michael R.},
booktitle = {Proceedings of the IEEE Symposium on Reliable Distributed Systems},
doi = {10.1109/SRDS.2011.10},
isbn = {9780769544502},
issn = {10609857},
keywords = {Cloud Computing,Prediction,QoS},
pages = {1--10},
title = {{Exploring latent features for memory-based QoS prediction in cloud computing}},
year = {2011}
}
@article{Mazumdar20174,
abstract = {Cloud computing has become an essential part of the global digital economy due to its extensibility, flexibility and reduced costs of operations. Nowadays, data centers (DCs) contain thousands of different machines running a huge number of diverse applications over an extended period. Resource management in Cloud is an open issue since an efficient resource allocation can reduce the infrastructure running cost. In this paper, we propose a snapshot-based solution for server consolidation problem from Cloud infrastructure provider (CIP) perspective. Our proposed mathematical formulation aims at reducing power cost by employing efficient server consolidation, and also considering the issues such as (i) mapping incoming and failing virtual machines (VMs), (ii) reducing a total number of VM migrations and (iii) consolidating running server workloads. We also compare the performance of our proposed model to the well-known Best Fit heuristics and its extension to include server consolidation via VM migration denoted as Best Fit with Consolidation (BFC). Our proposed mathematical formulation allows us to measure the solution quality in absolute terms, and it can also be applicable in practice. In our simulations, we show that relevant improvements (from 6{\%} to 15{\%}) over the widely adopted Best Fit algorithm achieved in a reasonable computing time.},
author = {Mazumdar, Somnath and Pranzo, Marco},
doi = {10.1016/j.future.2016.12.022},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Cloud,Integer linear programming,Optimisation,Server consolidation,Virtual Machine allocation},
pages = {4--16},
title = {{Power efficient server consolidation for Cloud data center}},
volume = {70},
year = {2017}
}
@article{Wang:2016eha,
abstract = {—In this paper, we investigate the issue of minimizing data center energy usage. In particular, we formulate a problem of virtual machine placement with the objective of minimizing the total power consumption of all the servers. To do this, we examine a CPU power consumption model and then incorporate the model into an mixed integer programming formulation. In order to find optimal or near-optimal solutions fast, we resolve two difficulties: non-linearity of the power model and integer decision variables. We first show how to linearize the problem, and then give a relaxation and iterative rounding algorithm. Computation experiments have shown that the algorithm can solve the problem much faster than the standard integer programming algorithms, and it consistently yields near-optimal solutions. We also provide a heuristic min-cost algorithm, which finds less optimal solutions but works even faster.},
author = {Wang, Yi and Xia, Ye},
doi = {10.1109/CLOUD.2016.19},
isbn = {9781509026197},
issn = {21596190},
journal = {IEEE International Conference on Cloud Computing, CLOUD},
keywords = {Cloud computing,Data center,Energy consumption,Mixed integer programming,Resource management},
pages = {84--91},
title = {{Energy optimal VM placement in the cloud}},
year = {2017}
}
@article{Murtazaev:2014eo,
abstract = {Virtualization technologies changed the way data centers of enterprises utilize their server resources. Instead of using dedicated servers for each type of application, virtualization allows viewing resources as a pool of unified resources, thereby reducing complexity and easing manageability. Server consolidation technique, which deals with reducing the number of servers used by consolidating applications, is one of the main applications of virtualization in data centers. The latter technique helps to use computing resources more effectively and has many benefits, such as reducing costs of power, cooling and, hence, contributes to the Green IT initiative. In a dynamic data center environment, where applications encapsulated as virtual machines are mapped to and released from the nodes frequently, reducing the number of server nodes used can be achieved by migrating applications without stopping their services, the technology known as live migration. However, live migration is a costly operation; hence, how to perform periodic server consolidation operation in a migration-aware way is a challenging task. We propose server consolidation algorithm – Sercon, which not only minimizes the overall number of used servers, but also minimizes the number of migrations. We verify the feasibility of our algorithm along with showing its scalability by conducting experiments with eight different test cases.},
author = {Murtazaev, Aziz and Oh, Sangyoon},
doi = {10.4103/0256-4602.81230},
isbn = {9781450312493},
issn = {0256-4602},
journal = {IETE Technical Review},
month = {Sep},
number = {3},
pages = {212},
title = {{Sercon: Server Consolidation Algorithm using Live Migration of Virtual Machines for Green Computing}},
url = {http://tr.ietejournals.org/text.asp?2011/28/3/212/81230},
volume = {28},
year = {2011}
}
@article{Constantin:1995hu,
abstract = {We consider the problem of optimizing the frequencies of transit lines in an urban transportation network. The problem is formulated first as a nonlinear nonconvex mixed integer programming problem and then it is converted into a bi-level Min-Min nonconvex optimization problem. This problem is solved by a projected (sub)gradient algorithm, where a (sub)gradient is obtained at each iteration by solving the lower level problem. Computational results obtained with this algorithm are presented for the transit networks of the cities of Stockholm, Sweden, Winnipeg, Man., Canada and Portland, OR, U.S.A. {\textcopyright} 1995.},
author = {Constantin, Isabelle and Florian, Michael},
doi = {10.1016/0969-6016(94)00023-M},
isbn = {0969-6016},
issn = {09696016},
journal = {International Transactions in Operational Research},
keywords = {bi-level programming,optimization of transit network,projected gradient,urban transportation},
month = {Apr},
number = {2},
pages = {149--164},
title = {{Optimizing frequencies in a transit network: a nonlinear bi-level programming approach}},
volume = {2},
year = {1995}
}
@article{Beloglazov:2012bw,
abstract = {The rapid growth in demand for computational power driven by modern service applications combined with the shift to the Cloud computing model have led to the establishment of large-scale virtualized data centers. Such data centers consume enormous amounts of electrical energy resulting in high operating costs and carbon dioxide emissions. Dynamic consolidation of virtual machines (VMs) using live migration and switching idle nodes to the sleep mode allows Cloud providers to optimize resource usage and reduce energy consumption. However, the obligation of providing high quality of service to customers leads to the necessity in dealing with the energy-performance trade-off, as aggressive consolidation may lead to performance degradation. Because of the variability of workloads experienced by modern applications, the VM placement should be optimized continuously in an online manner. To understand the implications of the online nature of the problem, we conduct a competitive analysis and prove competitive ratios of optimal online deterministic algorithms for the single VM migration and dynamic VM consolidation problems. Furthermore, we propose novel adaptive heuristics for dynamic consolidation of VMs based on an analysis of historical data from the resource usage by VMs. The proposed algorithms significantly reduce energy consumption, while ensuring a high level of adherence to the service level agreement. We validate the high efficiency of the proposed algorithms by extensive simulations using real-world workload traces from more than a thousand PlanetLab VMs. Copyright 2011 John Wiley {\&} Sons, Ltd.},
archivePrefix = {arXiv},
arxivId = {1006.0308},
author = {Beloglazov, Anton and Buyya, Rajkumar},
doi = {10.1002/cpe.1867},
eprint = {1006.0308},
isbn = {1532-0634},
issn = {15320626},
journal = {Concurrency Computation Practice and Experience},
keywords = {Cloud computing,Green IT,dynamic consolidation,resource management,virtualization},
number = {13},
pages = {1397--1420},
pmid = {23335858},
title = {{Optimal online deterministic algorithms and adaptive heuristics for energy and performance efficient dynamic consolidation of virtual machines in Cloud data centers}},
volume = {24},
year = {2012}
}
@article{Qin:2012wu,
abstract = {Live data migration is an important technique for key-value stores. However, due to the stateful feature, new virtualization technology, stringent low latency requirements and unexpected workload changes, key-value stores deployed in cloud environment have to face new challenges for data migration: effects of VM interference, and the need to trade off between the two ingredients of migration cost, say migration time and performance impact. To address these challenges, we focus on the data migration problem in a load rebalancing scenario and build a new framework that aims to rebalance load while minimizing migration costs. We build two interference-aware prediction models to predict the migration time and performance impact for each action using statistical machine learning and then create a cost model to strike a right balance between the two ingredients of cost. A cost-aware migration algorithm is designed to utilize the cost model and balance rate to guide the choice of possible migration actions. We demonstrate the effectiveness of the data migration approach as well as the cost model and two prediction models using YCSB.},
author = {Qin, Xiulei and Zhang, Wenbo and Wang, Wei and Wei, Jun and Zhao, Xin and Huang, Tao},
doi = {10.1109/CLUSTER.2012.14},
isbn = {9780769548074},
journal = {Proceedings - 2012 IEEE International Conference on Cluster Computing, CLUSTER 2012},
keywords = {cost,data migration,key-value store,rebalancing},
pages = {551--556},
title = {{Towards a cost-aware data migration approach for key-value stores}},
year = {2012}
}
@article{Klockgether:1970tw,
author = {Klockgether, J and Schwefel, H P},
journal = {Proc. 11th Symp. Engineering Aspects of Magnetohydrodynamics},
number = {DECEMBER 1969},
pages = {141--148},
title = {{Two-phase nozzle and hollow core jet experiments}},
year = {1970}
}
@article{Johnson:2016wp,
author = {Johnson, David S.},
doi = {10.1007/978-3-642-27848-8_495-1},
journal = {Encyclopedia of Algorithms},
pages = {1--6},
title = {{Vector Bin Packing}},
url = {http://link.springer.com/10.1007/978-3-642-27848-8{\_}495-1},
year = {2014}
}
@article{Brotcorne:2001je,
abstract = {We consider the problem of determining a set of optimal tolls on the arcs of a multicommodity transportation network. The problem is formulated as a bilevel mathematical program where the upper level consists in a firm that raises revenues from tolls set on arcs of the network, while the lower level is represented by a group of users travelling on shortest paths with respect to a generalized travel cost.},
author = {Brotcorne, Luce and Labb�, Martine and Marcotte, Patrice and Savard, Gilles},
doi = {10.1287/trsc.35.4.345.10433},
isbn = {00411655},
issn = {0041-1655},
journal = {Transportation Science},
month = {Nov},
number = {4},
pages = {345--358},
title = {{A Bilevel Model for Toll Optimization on a Multicommodity Transportation Network}},
volume = {35},
year = {2001}
}
@booklet{Belady:AcnDy0mJ,
abstract = {The Green Grid is an association of IT professionals seeking to dramatically raise the energy effi ciency of datacenters through a series of short-term and long-term proposals. This is an update to the very fi rst white paper published by the Green Grid in February 2007 called “Green Grid Metrics: Describing Data Center Power Effi ciency” to refi ne the nomenclature and intent of that paper. In that paper, The Green Grid proposed the use of Power Usage Effectiveness (PUE) and its reciprocal, Datacenter Effi ciency (DCE) metrics, which enable datacenter operators to quickly estimate the energy effi ciency of their datacenters, compare the results against other datacenters, and determine if any energy effi ciency improvements need to be made. Since then PUE has received broad adoption in the industry but DCE has had limited success due to the misconception of what data center effi ciency really means. As a result, this paper re-affi rms the use of PUE but redefi nes its reciprocal as datacenter infrastructure effi ciency (DCiE). This refi nement will avoid much of the confusion around DCE and will now be called DCiE. In the long term, The Green Grid is developing metrics to measure data center productivity as well as effi ciency metrics for all major power-consuming subsystems in the datacenter. To promote these metrics and drive greater datacenter energy effi ciency for businesses around the world, The Green Grid will publish future white papers that provide detailed guidance on using these metrics. We will also continue to collaborate with organizations such as the EPA, ECMA and Climate Savers that promote a similar goal and vision.},
author = {Belady, Christian (Microsoft) and Rawson, Andy (Amd) and Pfleuger, John (Dell) and Cader, Tahir (Spraycool)},
booktitle = {The Green Grid},
doi = {10.1016/S0927-0507(07)15006-4},
pages = {1--9},
title = {{Green Grid Data Center Power Efficiency Metrics : Pue and Dcie Editors :}},
year = {2008}
}
@inproceedings{Takouna:2014fa,
author = {Takoun, Ibrahim and Alzaghoul, Esra and Sachs, Kai and Meinel, Christoph},
booktitle = {2014 IEEE International Conference on{\~{}}Internet of Things(iThings), and IEEE{\~{}}Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing(CPSCom)},
keywords = {abstract,ation of their cpu,consolidation,consolidation environments host-,energy-aware,ing heterogeneous applications,of vms,resource management,the fluctu-,the hosted vms encounter,to,utilization,virtualization,virtualized data centers are,which allows to consolidate},
pages = {1--12},
publisher = {IEEE},
title = {{Robust Dynamic Virtual Machine Consolidation for Efficient Energy and Performance Management in Virtualized Data Centers}},
year = {2014}
}
@article{PriceModel_1,
abstract = {Cloud computing is emerging as a promising field offering a variety of computing services to end users. These services are offered at different prices using various pricing schemes and techniques. End users will favor the service provider offering the best QoS with the lowest price. Therefore, applying a fair pricing model will attract more customers and achieve higher revenues for service providers. This work focuses on comparing many employed and proposed pricing models techniques and highlights the pros and cons of each. The comparison is based on many aspects such as fairness, pricing approach, and utilization period. Such an approach provides a solid ground for designing better models in the future. We have found that most approaches are theoretical and not implemented in the real market, although their simulation results are very promising. Moreover, most of these approaches are biased toward the service provider.},
author = {Al-Roomi, May and Al-Ebrahim, Shaikha and Buqrais, Sabika and Ahmad, Imtiaz},
doi = {10.14257/ijgdc.2013.6.5.09},
issn = {20054262},
journal = {International Journal of Grid and Distributed Computing},
keywords = {charging models,cloud computing,fairness,pricing models,survey},
number = {5},
pages = {93--106},
title = {{Cloud Computing Pricing Models: A Survey}},
url = {http://dx.doi.org/10.14257/ijgdc.2013.6.5.09},
volume = {6},
year = {2013}
}
@book{Anonymous:2016vva,
address = {Cham},
author = {{De Paoli Stefan Schulte Einar Broch Johnsen}, Flavio},
doi = {10.1007/978-3-319-67262-5},
isbn = {978-3-319-67261-8},
month = {Aug},
publisher = {Springer International Publishing},
title = {{Service-Oriented and Cloud Computing}},
url = {https://link-springer-com.sire.ub.edu/content/pdf/10.1007{\%}2F978-3-319-67262-5.pdf},
year = {2016}
}
@article{Sarin:2011fu,
author = {Taylor, Publisher and Sarin, Subhash C and Varadarajan, Amrusha and Wang, Lixin},
doi = {10.1080/09537287.2010.490014},
journal = {Production Planning and {\ldots}},
keywords = {automated material handling systems,batch,dispatching rules,photolithography scheduling,processing,scheduling,sequencing,wafer fabrication},
month = {Jan},
number = {April 2013},
pages = {37--41},
title = {{Production Planning {\&} Control : The Management of Operations A survey of dispatching rules for operational control in wafer fabrication}},
volume = {22},
year = {2011}
}
@article{Dong:2014iz,
author = {Dong, Ziqian and Zhuang, Wenjie and Rojas-Cessa, Roberto},
doi = {10.1109/OnlineGreenCom.2014.7114422},
isbn = {9781479973842},
journal = {2014 IEEE Online Conference on Green Communications, OnlineGreenComm 2014},
keywords = {Energy,Google trace,data center,scheduling},
pages = {1--6},
title = {{Energy-aware scheduling schemes for cloud data centers on Google trace data}},
year = {2014}
}
@article{Deb:2010in,
abstract = {Bilevel optimization problems involve two optimization tasks (upper and lower level), in which every feasible upper level solution must correspond to an optimal solution to a lower level optimization problem. These problems commonly appear in many practical problem solving tasks including optimal control, process optimization, game-playing strategy developments, transportation problems, and others. However, they are commonly converted into a single level optimization problem by using an approximate solution procedure to replace the lower level optimization task. Although there exist a number of theoretical, numerical, and evolutionary optimization studies involving single-objective bilevel programming problems, not many studies look at the context of multiple conflicting objectives in each level of a bilevel programming problem. In this paper, we address certain intricate issues related to solving multi-objective bilevel programming problems, present challenging test problems, and propose a viable and hybrid evolutionary-cum-local-search based algorithm as a solution methodology. The hybrid approach performs better than a number of existing methodologies and scales well up to 40-variable difficult test problems used in this study. The population sizing and termination criteria are made self-adaptive, so that no additional parameters need to be supplied by the user. The study indicates a clear niche of evolutionary algorithms in solving such difficult problems of practical importance compared to their usual solution by a computationally expensive nested procedure. The study opens up many issues related to multi-objective bilevel programming and hopefully this study will motivate EMO and other researchers to pay more attention to this important and difficult problem solving activity.},
author = {Deb, Kalyanmoy and Sinha, Ankur},
doi = {10.1162/EVCO_a_00015},
isbn = {1063-6560},
issn = {1063-6560},
journal = {Evolutionary Computation},
month = {Aug},
number = {3},
pages = {403--449},
pmid = {20560758},
title = {{An Efficient and Accurate Solution Methodology for Bilevel Multi-Objective Programming Problems Using a Hybrid Evolutionary-Local-Search Algorithm}},
url = {http://www.mitpressjournals.org/doi/10.1162/EVCO{\_}a{\_}00015},
volume = {18},
year = {2010}
}
@article{ope,
author = {Kanagarajan, D and Karthikeyan, R and Palanikumar, K and Davim, J Paulo},
doi = {10.1007/s00170-006-0921-8},
isbn = {0017000609218},
issn = {02683768},
journal = {Journal of Materials},
keywords = {co composite,edm,electrical discharge,machining,modeling,non dominated sorting genetic algorithm,nsga ii,wc},
number = {11-12},
pages = {1124--1132},
title = {{Optimization of electrical discharge machining characteristics of WC / Co composites using non-dominated sorting genetic algorithm ( NSGA-II )}},
url = {http://www.springerlink.com/index/10.1007/s00170-006-0921-8},
volume = {36},
year = {2008}
}
@article{Wang:2005fa,
author = {Wang, Yuping and Jiao, Yong-Chang and Li, Hong},
journal = {IEEE Transactions on Systems, Man, and Cybernetics},
keywords = {Constraint handling,Convergence,Design optimization,Educational programs,Electromagnetic scattering,Evolutionary computation,Genetic programming,NP-hard problem,Programming profession,Resource management,constraint handling,constraint-handling scheme,evolutionary algorithm,evolutionary algorithm (EA),evolutionary computation,global optimization,linear programming,nonlinear bilevel programming,nonlinear bilevel programming problem,objective optimization problem,specific-design crossover operator},
number = {2},
pages = {221--232},
title = {{An evolutionary algorithm for solving nonlinear bilevel programming based on a new constraint-handling scheme}},
volume = {35},
year = {2005}
}
@article{Buyya:2009ix,
abstract = {With the significant advances in Information and Communications Technology (ICT) over the last half century, there is an increasingly perceived vision that computing will one day be the 5th utility (after water, electricity, gas, and telephony). This computing utility, like all other four existing utilities, will provide the basic level of computing service that is considered essential to meet the everyday needs of the general community. To deliver this vision, a number of computing paradigms have been proposed, of which the latest one is known as Cloud computing. Hence, in this paper, we define Cloud computing and provide the architecture for creating Clouds with market-oriented resource allocation by leveraging technologies such as Virtual Machines (VMs). We also provide insights on market-based resource management strategies that encompass both customer-driven service management and computational risk management to sustain Service Level Agreement (SLA)-oriented resource allocation. In addition, we reveal our early thoughts on interconnecting Clouds for dynamically creating global Cloud exchanges and markets. Then, we present some representative Cloud platforms, especially those developed in industries, along with our current work towards realizing market-oriented resource allocation of Clouds as realized in Aneka enterprise Cloud technology. Furthermore, we highlight the difference between High Performance Computing (HPC) workload and Internet-based services workload. We also describe a meta-negotiation infrastructure to establish global Cloud exchanges and markets, and illustrate a case study of harnessing 'Storage Clouds' for high performance content delivery. Finally, we conclude with the need for convergence of competing IT paradigms to deliver our 21st century vision.},
archivePrefix = {arXiv},
arxivId = {0808.3558},
author = {Buyya, Rajkumar and Buyya, Rajkumar and Yeo, Chee Shin and Yeo, Chee Shin and Venugopal, Srikumar and Venugopal, Srikumar and Broberg, James and Broberg, James and Brandic, Ivona and Brandic, Ivona},
doi = {10.1016/j.future.2008.12.001},
eprint = {0808.3558},
isbn = {0167-739X},
issn = {0167-739},
journal = {Future Generation Computer Systems},
keywords = {Cloud computing,Data Centers,Market-oriented resource allocation,Utility computing,Virtualization},
month = {Jun},
number = {June 2009},
pages = {17},
pmid = {21450758},
title = {{Cloud computing and emerging IT platforms: Vision, hype, and reality for delivering computing as the 5th utility}},
url = {http://portal.acm.org/citation.cfm?id=1528937.1529211},
volume = {25},
year = {2009}
}
@article{Nguyen:2014eu,
abstract = {A scheduling policy strongly influences the performance of a manufacturing system. However, the design of an effective scheduling policy is complicated and time-consuming due to the complexity of each scheduling decision as well as the interactions among these decisions. This paper develops four new multi-objective genetic programming based hyper-heuristic (MO-GPHH) methods for automatic design of scheduling policies including dispatching rules and due-date assignment rules in job shop environments. Besides using three existing search strategies NSGA-II, SPEA2 and HaD-MOEA to develop new MO-GPHH methods, a new approach called Diversified Multi-Objective Cooperative Coevolution (DMOCC) is also proposed. The novelty of these MO-GPHH methods is that they are able to handle multiple scheduling decisions simultaneously. The experimental results show that the evolved Pareto fronts represent effective scheduling policies that can dominate scheduling policies from combinations of existing dispatching rules with dynamic/regression-based duedate assignment rules. The evolved scheduling policies also show dominating performance on unseen simulation scenarios with different shop settings. In addition, the uniformity of the scheduling policies obtained from the proposed method of DMOCC is better than those evolved by other evolutionary approaches.},
author = {Nguyen, Su and Zhang, Mengjie and Johnston, Mark and Tan, Kay Chen},
doi = {10.1109/TEVC.2013.2248159},
isbn = {1089-778X},
issn = {1089778X},
journal = {IEEE Transactions on Evolutionary Computation},
keywords = {Dispatching rule (DR),genetic programming (GP),hyperheuristic,job shop scheduling (JSS)},
number = {2},
pages = {193--208},
title = {{Automatic design of scheduling policies for dynamic multi-objective job shop scheduling via cooperative coevolution genetic programming}},
volume = {18},
year = {2014}
}
@inproceedings{Farahnakian:2013gg,
abstract = {Virtualization is a vital technology of cloud computing which enables the partition of a physical host into several Virtual Machines (VMs). The number of active hosts can be reduced according to the resources requirements using live migration in order to minimize the power consumption in this technology. However, the Service Level Agreement (SLA) is essential for maintaining reliable quality of service between data centers and their users in the cloud environment. Therefore, reduction of the SLA violation level and power costs are considered as two objectives in this paper. We present a CPU usage prediction method based on the linear regression technique. The proposed approach approximates the short-time future CPU utilization based on the history of usage in each host. It is employed in the live migration process to predict over-loaded and under-loaded hosts. When a host becomes over-loaded, some VMs migrate to other hosts to avoid SLA violation. Moreover, first all VMs migrate from a host while it becomes under-loaded. Then, the host switches to the sleep mode for reducing power consumption. Experimental results on the real workload traces from more than a thousand Planet Lab VMs show that the proposed technique can significantly reduce the energy consumption and SLA violation rates.},
author = {Farahnakian, Fahimeh and Liljeberg, Pasi and Plosila, Juha},
booktitle = {Proceedings - 39th Euromicro Conference Series on Software Engineering and Advanced Applications, SEAA 2013},
doi = {10.1109/SEAA.2013.23},
isbn = {9780769550916},
keywords = {cloud computing,dynamic consolidation,green IT,live migration,regression,virtulization},
pages = {357--364},
publisher = {IEEE},
title = {{LiRCUP: Linear regression based CPU usage prediction algorithm for live migration of virtual machines in data centers}},
year = {2013}
}
@article{Erren:2007hl,
abstract = {Thomas C. Erren is with the Institute and Policlinic for Occupational and Social Medicine, School of Medicine and Dentistry, University of Cologne, K{\"{o}}ln, Lindenthal, Germany. Paul Cullen is with the Medizinisches Versorgungszentrum f{\"{u}}r Laboratoriumsmedizin Dr. L{\"{o}}er, Dr.Treder, ...},
annote = {--- 
- I think these rules may help me in setting my PhD proposal development into context. What is clearly of importance of having an important problem to work on, being open to interruptions during the research process to learn about the world, and be s},
author = {Erren, Thomas C and Cullen, Paul and Erren, Michael and Bourne, Philip E},
doi = {10.1371/journal.pcbi.0030213},
issn = {1553-734X},
journal = {PLoS computational biology},
keywords = {Research Methods,Writing and Research},
month = {Oct},
number = {10},
pages = {1839--1840},
title = {{Ten Simple Rules for Doing Your Best}},
url = {http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed{\&}id=17967054{\&}retmode=ref{\&}cmd=prlinks{\%}5Cnpapers2://publication/doi/10.1371/journal.pcbi.0030213},
volume = {3},
year = {2007}
}
@article{Kivity:2007wu,
abstract = {Virtualization is a hot topic in operating systems these days. It is useful in many scenarios: server consolida- tion, virtual test environments, and for Linux enthusiasts who still can not decide which distribution is best. Re- cently, hardware vendors of commodity x86 processors have added virtualization extensions to the instruction set that can be utilized to write relatively simple virtual machine monitors. The Kernel-based Virtual Machine, or kvm, is a new Linux subsystem which leverages these virtualization extensions to add a virtual machine monitor (or hyper- visor) capability to Linux. Using kvm, one can create and run multiple virtual machines. These virtual ma- chines appear as normal Linux processes and integrate seamlessly with the rest of the system.},
author = {Kivity, Avi and Lublin, Uri and Liguori, Anthony and Kamay, Yaniv and Laor, Dor},
doi = {10.1186/gb-2008-9-1-r8},
isbn = {1465-6914 (Linking)},
issn = {1465-6906},
journal = {Proceedings of the Linux Symposium},
pages = {225--230},
pmid = {18197987},
title = {{kvm: the Linux virtual machine monitor}},
url = {https://www.kernel.org/doc/mirror/ols2007v1.pdf{\#}page=225},
volume = {1},
year = {2007}
}
@article{Hofmeyr:2000ju,
abstract = {An artificial immune system (ARTIS) is described which incorporates many properties of natural immune systems, including diversity, distributed computation, error tolerance, dynamic learning and adaptation, and self-monitoring. ARTIS is a general framework for a distributed adaptive system and could, in principle, be applied to many domains. In this paper, ARTIS is applied to computer security in the form of a network intrusion detection system called LISYS. LISYS is described and shown to be effective at detecting intrusions, while maintaining low false positive rates. Finally, similarities and differences between ARTIS and Holland's classifier systems are discussed.},
author = {Hofmeyr, Steven A. and Forrest, Stephanie},
doi = {10.1162/106365600568257},
isbn = {1063-6560},
issn = {1063-6560},
journal = {Evolutionary Computation},
number = {4},
pages = {443--473},
pmid = {11130924},
title = {{Architecture for an Artificial Immune System}},
url = {http://www.mitpressjournals.org/doi/10.1162/106365600568257},
volume = {8},
year = {2000}
}
@article{DeJong:1992ws,
abstract = {Genetic Algorithms (GAs) have received a great deal of attention regarding their potential as optimization techniques for complex functions. The level of interest and success in this area has led to a number of improvements to GA-based function optimizers and a good deal of progress in characterizing the kinds of functions that are easy/hard for GAs to optimize. With all this activity, there has been a natural tendency to equate GAs with function optimization. However, the motivating context of Holland's initial GA work was the design and implementation of robust adaptive systems. In this paper we argue that a proper understanding of GAs in this broader adaptive systems context is a necessary prerequisite for understanding their potential application to any problem domain. We then use these insights to better understand the strengths and limitations of GAs as function optimizers.},
author = {De{\~{}}Jong, K a},
doi = {10.1016/B978-0-08-094832-4.50006-4},
isbn = {1558602631},
issn = {10816593},
journal = {Proceedings of the Second Workshop on Foundations of Genetic Algorithms},
pages = {5--18},
title = {{Genetic Algorithms are NOT Function Optimizers}},
year = {1992}
}
@incollection{Li:2006br,
address = {Berlin, Heidelberg},
author = {Li, Xiangyong and Tian, Peng and Min, Xiaoping},
booktitle = {Artificial Intelligence and Soft Computing -- ICAISC 2006: 8th International Conference, Zakopane, Poland, June 25-29, 2006. Proceedings},
pages = {1169--1178},
publisher = {Springer Berlin Heidelberg},
title = {{A Hierarchical Particle Swarm Optimization for Solving Bilevel Programming Problems}},
year = {2006}
}
@article{Bernstein:2014ur,
abstract = {This issue's "Cloud Tidbit" focuses on container technology and how it's emerging as an important part of the cloud computing infrastructure. It looks at Docker, an open source project that automates the faster deployment of Linux applications, and Kubernetes, an open source cluster manager for Docker containers.},
author = {Bernstein, David},
doi = {10.1109/MCC.2014.51},
isbn = {2325-6095},
issn = {23256095},
journal = {IEEE Cloud Computing},
keywords = {cloud,containers,dockers,virtual machines},
number = {3},
pages = {81--84},
title = {{Containers and cloud: From LXC to docker to kubernetes}},
volume = {1},
year = {2014}
}
@article{Sim:2013fe,
abstract = {Novel deterministic heuristics are generated using Single Node Genetic Programming for application to the One Dimensional Bin Packing Problem. First a single deterministic heuristic was evolved that minimised the total number of bins used when applied to a set of 685 training instances. Following this, a set of heuristics were evolved using a form of cooperative co-evolution that collectively minimise the number of bins used across the same set of problems. Results on an unseen test set comprising a further 685 problem instances show that the single evolved heuristic outperforms existing deterministic heuristics described in the literature. The collection of heuristics evolved by cooperative co-evolution outperforms any of the single heuristics, including the newly generated ones.},
author = {Sim, Kevin and Hart, Emma},
doi = {doi:10.1145/2463372.2463555},
isbn = {9781450319638},
journal = {GECCO 2013: Proceeding of the fifteenth annual conference on Genetic and evolutionary computation conference},
keywords = {algorithms,genetic algorithms,genetic programming,hyper-heuristics,one dimensional bin packing,single node genetic programming},
pages = {1549--1556},
title = {{Generating single and multiple cooperative heuristics for the one dimensional bin packing problem using a single node genetic programming island model}},
year = {2013}
}


@article{Babu:2013jf,
abstract = {Cloud computing has recently emerging technology getting popular day by day having wide scope in future.  Cloud computing is defined as a large scale distributed computing paradigm that is driven by economics of scale in which a pool of abstracted virtualized energetically. The number of users in cloud computing is growing exponentially. Large number of user requests tries to designate the resources for many applications which along with to high load not far afield off from cloud server.  Whenever certain VMs are overloaded then no more tasks should be send to overloaded virtual machine if under loaded virtual machines are available. For optimize solution and better response time the load has to be balanced among overloaded and under loaded virtual machines. In this paper, an algorithm is proposed named honey bee behavior based load balancing (HBB-LB), which targets to achieve well balanced load across virtual machine. The experimental results show that the algorithm has many advantages over existing algorithms. There is improvement in average execution time and reduction in waiting time of tasks. The paper also describes briefly about other existing load balancing approaches.},
author = {Gupta, Harshit and Sahu, Kalicharan},
doi = {http://dx.doi.org/10.1016/j.asoc.2013.01.025},
isbn = {1568-4946},
issn = {1568-4946},
journal = {Applied Soft Computing},
keywords = {Honey Bee, Load Balancing, Cloud Computing, Honey,cloud computing,honey bee,honey bee foraging behavior,load balancing,virtual machine scheduling},
mendeley-groups = {proposal},
number = {6},
pages = {842--846},
title = {{Honey Bee Behavior Based Load Balancing of Tasks in Cloud Computing}},
url = {http://www.sciencedirect.com/science/article/pii/S1568494613000446},
volume = {3},
year = {2014}
}

@incollection{Vose:1993iu,
author = {Vose, Michael D and Hall, Ayres},
booktitle = {Evolutionary Computation},
doi = {10.1162/evco.1995.3.4.453},
issn = {1063-6560},
keywords = {asymptotic behavior,markov chain,steady state distribution},
mendeley-groups = {proposal},
number = {4},
pages = {453--472},
publisher = {Elsevier},
title = {{Modeling Simple Genetic Algorithms}},
volume = {3},
year = {1996}
}

@article{Zomaya:2005bb,
author = {Zomaya, Albert Y and Crnomarkovic, Kris},
journal = {Handbook of Bioinspired Algorithms and Applications},
mendeley-groups = {proposal},
pages = {13--208},
title = {{Genetic Algorithms for Scheduling in Grid Computing Environments.}},
volume = {20053845},
year = {2005}
}

@article{Anderson:1994io,
abstract = {Genetic algorithms are one example of the use of a random element within an algorithm for combi-natorial optimization. We consider the application of the genetic algorithm to a particular problem, the Assembly Line Balancing Problem. A general description of genetic algorithms is given, and their specialized use on our test-bed problems is discussed. We carry out extensive computational testing to appropriate values for the various parameters associated with this genetic algorithm. These experiments underscore the importance of the correct choice of a scaling parameter and mutation rate to ensure the good performance of a genetic algorithm. We also describe a parallel implementation of the genetic algorithm and give some comparisons between the parallel and se-rial implementations. Both versions of the algorithm are shown to be eeective in producing good solutions for problems of this type (with appropriately chosen parameters).},
author = {Anderson, EJ and Ferris, MC},
journal = {ORSA Journal on Computing},
keywords = {assembly line,combinatorial optimization,genetic algorithms,parallel processing},
mendeley-groups = {proposal},
month = {may},
number = {2},
pages = {1--23},
title = {{Genetic algorithms for combinatorial optimization: The assemble line balancing problem}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/ijoc.6.2.161},
volume = {6},
year = {1994}
}
Now
